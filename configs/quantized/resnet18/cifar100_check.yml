dataset: torch/cifar100
num_classes: 100
img_size: 32
mean:
    - 0.5071
    - 0.4867
    - 0.4408
std:
    - 0.2675
    - 0.2565
    - 0.2761
crop_pct: 1.0
scale:
    - 0.8
    - 1.0
interpolation: bicubic
train_interpolation: random
aa: rand-m9-mstd0.5-inc1
mixup: 0.8
mixup_off_epoch: 175
mixup_prob: 1.0
mixup_mode: batch
mixup_switch_prob: 0.5
cutmix: 1.0
reprob: 0.25
remode: pixel
amp: True
model_ema: False
batch_size: 256
lr: 5e-5
min_lr: 1e-5
sched: cosine
weight_decay: 0.000025
epochs: 50
cooldown_epochs: 10
warmup_epochs: 0
warmup_lr: 0.00001
opt: adamw
smoothing: 0.1
workers: 4
seed: 1101
checkpoint_hist: 3
log_name: resnet18_c100_w4a4_check

use_kd: true
kd_alpha: 4
teacher: ResNet18
teacher_checkpoint: pretrained/resnet_c100_w4a4.pth.tar
initial_checkpoint: pretrained/resnet_c100_w4a4.pth.tar

gpu: 0

powerof2: True
# w3a3
# bw_list: 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 3, 4, 4, 2, 4
# ba_list: 6, 6, 6, 6, 3, 5, 5, 2, 6, 4, 4, 5, 6, 3, 5, 4
# w2a2
# bw_list: 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 4, 2, 4
# ba_list: 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 5, 4
# w4a4
# bw_list: 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4
# ba_list: 6, 6, 6, 6, 6, 5, 5, 5, 6, 6, 4, 5, 6, 6, 6, 4
wq_per_channel: True
wq_enable: True
wq_mode: "LSQ"
wq_bitw: 4
#wq_pos: 1
#wq_neg: -1
aq_enable: True
aq_asym: True
aq_mode: "LSQ"
aq_bitw: 4
#aq_pos: 1
#aq_neg: -1
resq_enable: True
resq_mode: "LSQ"
resq_bitw: 8
#resq_pos: 1
#resq_neg: -1
qmodules: 
  - "convbn_first;wq:bit:8;aq:bit:8"
  - "layer1.0.convbn1"
  - "layer1.0.convbn2"
  - "layer1.1.convbn1"
  - "layer1.1.convbn2"
  - "layer2.0.convbn1"
  - "layer2.0.convbn2"
  # - "layer2.0.shortcut.0"
  - "layer2.1.convbn1"
  - "layer2.1.convbn2"
  - "layer3.0.convbn1"
  - "layer3.0.convbn2"
  # - "layer3.0.shortcut.0"
  - "layer3.1.convbn1"
  - "layer3.1.convbn2"
  - "layer4.0.convbn1"
  - "layer4.0.convbn2"
  # - "layer4.0.shortcut.0"
  - "layer4.1.convbn1"
  - "layer4.1.convbn2"
  - fc;wq:bit:8;aq:bit:8
resq_modules:
    - relu
    - layer1.0.relu2
    - layer1.1.relu2
    - layer2.0.relu2
    - layer2.0.downsample
    - layer2.1.relu2
    - layer3.0.relu2
    - layer3.0.downsample
    - layer3.1.relu2
    - layer4.0.downsample
    - layer4.0.relu2
    - layer4.1.relu2
