2025-05-31 10:29:21,406 - train - INFO - Namespace(data_dir='/data/dataset/imagenet', dataset='image_folder', train_split='train', val_split='validation', model='ResNet18', pretrained=False, initial_checkpoint='output/train/20250427-184912-ResNet18-224/best.pth.tar.pth', resume='', no_resume_opt=False, num_classes=1000, gp=None, img_size=224, input_size=None, crop_pct=None, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], interpolation='', batch_size=1024, validation_batch_size_multiplier=1, gpu=0, opt='sgd', opt_eps=None, opt_betas=None, momentum=0.9, weight_decay=0.0001, clip_grad=None, clip_mode='norm', sched='cosine', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, lr_cycle_mul=1.0, lr_cycle_limit=1, warmup_lr=0.0001, min_lr=1e-05, epochs=100, epoch_repeats=0.0, start_epoch=None, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, no_aug=False, scale=[0, 1.0], ratio=[0.75, 1.3333333333333333], hflip=0, vflip=0.0, color_jitter=None, aa='rand-m9-mstd0.5-inc1', aug_splits=0, jsd=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.0, cutmix=0.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', mixup_off_epoch=0, smoothing=0.1, train_interpolation='random', bn_tf=False, bn_momentum=None, bn_eps=None, sync_bn=True, dist_bn='', split_bn=False, log_interval=50, recovery_interval=0, checkpoint_hist=1, save_images=False, amp=False, apex_amp=False, native_amp=False, channels_last=False, pin_mem=True, output='', experiment='', eval_metric='top1', tta=0, use_multi_epochs_loader=False, torchscript=False, log_wandb=False, wq_enable=True, wq_mode='LSQ', wq_bitw=32, wq_pos=None, wq_neg=None, wq_per_channel=True, wq_asym=True, aq_enable=True, aq_mode='LSQ', aq_bitw=32, aq_pos=None, aq_neg=None, aq_asym=True, qmodules=['convbn_first;wq:bit:8;aq:bit:8', 'layer1.0.convbn1', 'layer1.0.convbn2', 'layer1.1.convbn1', 'layer1.1.convbn2', 'layer2.0.convbn1', 'layer2.0.convbn2', 'layer2.1.convbn1', 'layer2.1.convbn2', 'layer3.0.convbn1', 'layer3.0.convbn2', 'layer3.1.convbn1', 'layer3.1.convbn2', 'layer4.0.convbn1', 'layer4.0.convbn2', 'layer4.1.convbn1', 'layer4.1.convbn2', 'fc;wq:bit:8;aq:bit:8'], resq_modules=['relu', 'layer1.0.relu2', 'layer1.1.relu2', 'layer2.0.relu2', 'layer3.0.downsample', 'layer2.1.relu2', 'layer3.0.relu2', 'layer3.0.downsample', 'layer3.1.relu2', 'layer4.0.downsample', 'layer4.0.relu2', 'layer4.1.relu2'], resq_enable=True, resq_mode='LSQ', resq_bitw=16, resq_pos=None, resq_neg=None, resq_asym=False, aq_per_channel=False, powerof2=True, world_size=8, local_rank=-1, dist_on_itp=False, dist_url='env://', device='cuda', seed=0, dist_eval=False, use_kd=False, kd_alpha=1.0, teacher='ResNet18', teacher_checkpoint='output/train/20250427-184912-ResNet18-224/best.pth.tar.pth', log_name='resnet18_imagenet_avgpool_acc15', budget=1, bw_list='5, 5, 5, 5, 5, 4, 4, 3, 4, 3, 3, 3, 3, 2, 2, 2', ba_list='4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3', workers=4, multiprocessing_distributed=True, rank=0, distributed=True, dist_backend='nccl', **{'weight-decay': '1e-4'})
2025-05-31 10:29:22,923 - train - INFO - Model ResNet18 created, param count:11689512
2025-05-31 10:29:24,321 - train - INFO - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2025-05-31 10:29:24,322 - train - INFO - Using native Torch AMP. Training in mixed precision.
2025-05-31 10:29:24,322 - train - INFO - Scheduled epochs: 110
2025-05-31 10:29:24,322 - train - INFO - Verifying initial model in test dataset
2025-05-31 10:29:24,322 - train - INFO - cuda:0
2025-05-31 10:29:34,149 - train - INFO - Test: [   0/48]  Time: 9.826 (9.826)  Loss:  6.5078 (6.5078)  Acc@1:  0.3906 ( 0.3906)  Acc@5:  2.5391 ( 2.5391)
2025-05-31 10:30:29,238 - train - INFO - Test: [  48/48]  Time: 1.574 (1.325)  Loss:  6.1562 (6.6466)  Acc@1:  4.0094 ( 1.2660)  Acc@5: 10.2594 ( 4.0520)
2025-05-31 10:30:29,468 - train - INFO - DistributedDataParallel(
  (module): ResNet(
    (relu): ReLU(
      (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
    )
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (maxpool): AvgPool2d(kernel_size=3, stride=2, padding=1)
    (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (convbn_first): QConvBn2d(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w_fn): LsqQuantizer(bit=8, pos=127, neg=-128, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
      (quan_a_fn): LsqQuantizer(bit=8, pos=127, neg=-127, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=True, per_channel=False, apot=False )
      (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=4, pos=7, neg=-8, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=4, pos=7, neg=-8, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=4, pos=7, neg=-8, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (quan_a_fn): LsqQuantizer(bit=16, pos=32767, neg=-32767, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=True, per_channel=False, apot=False )
        )
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=2, pos=1, neg=-2, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (quan_a_fn): LsqQuantizer(bit=16, pos=32767, neg=-32767, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=True, per_channel=False, apot=False )
        )
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=2, pos=1, neg=-2, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=2, pos=1, neg=-2, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): QLinear(
      in_features=512, out_features=1000, bias=True
      (quan_w_fn): LsqQuantizer(bit=8, pos=127, neg=-128, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
      (quan_a_fn): LsqQuantizer(bit=8, pos=255, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
    )
  )
)
2025-05-31 10:30:37,823 - train - INFO - Train: 0 [   0/156 (  0%)]  Loss:  5.896966 (5.8970)  Time: 8.340s,  982.21/s  (8.340s,  982.21/s)  LR: 1.000e-02  Data: 5.188 (5.188)
2025-05-31 10:31:35,634 - train - INFO - Train: 0 [  50/156 ( 32%)]  Loss:  4.588526 (5.2427)  Time: 0.532s, 15404.79/s  (1.291s, 6346.35/s)  LR: 1.000e-02  Data: 0.000 (0.185)
2025-05-31 10:32:36,301 - train - INFO - Train: 0 [ 100/156 ( 65%)]  Loss:  4.128466 (4.8713)  Time: 3.098s, 2644.69/s  (1.252s, 6540.75/s)  LR: 1.000e-02  Data: 0.000 (0.093)
2025-05-31 10:33:34,119 - train - INFO - Train: 0 [ 150/156 ( 97%)]  Loss:  3.860799 (4.6187)  Time: 0.526s, 15565.78/s  (1.221s, 6711.41/s)  LR: 1.000e-02  Data: 0.000 (0.063)
2025-05-31 10:33:39,230 - train - INFO - Train: 0 [ 155/156 (100%)]  Loss:  3.834692 (4.4619)  Time: 0.524s, 15636.08/s  (1.214s, 6746.55/s)  LR: 1.000e-02  Data: 0.000 (0.061)
2025-05-31 10:33:45,915 - train - INFO - Test: [   0/48]  Time: 6.428 (6.428)  Loss:  1.8105 (1.8105)  Acc@1: 61.2305 (61.2305)  Acc@5: 85.7422 (85.7422)
2025-05-31 10:34:45,338 - train - INFO - Test: [  48/48]  Time: 2.420 (1.344)  Loss:  1.5605 (2.4536)  Acc@1: 70.5189 (47.5600)  Acc@5: 87.2642 (73.3180)
2025-05-31 10:34:52,459 - train - INFO - Train: 1 [   0/156 (  0%)]  Loss:  3.808421 (3.8084)  Time: 6.585s, 1243.96/s  (6.585s, 1243.96/s)  LR: 9.998e-03  Data: 5.602 (5.602)
2025-05-31 10:35:51,231 - train - INFO - Train: 1 [  50/156 ( 32%)]  Loss:  4.511157 (4.1598)  Time: 0.531s, 15425.25/s  (1.281s, 6392.81/s)  LR: 9.998e-03  Data: 0.000 (0.319)
2025-05-31 10:36:51,932 - train - INFO - Train: 1 [ 100/156 ( 65%)]  Loss:  3.921058 (4.0802)  Time: 2.986s, 2743.23/s  (1.248s, 6563.85/s)  LR: 9.998e-03  Data: 1.035 (0.324)
2025-05-31 10:37:49,748 - train - INFO - Train: 1 [ 150/156 ( 97%)]  Loss:  4.499266 (4.1850)  Time: 0.530s, 15445.14/s  (1.218s, 6727.66/s)  LR: 9.998e-03  Data: 0.000 (0.272)
2025-05-31 10:37:54,706 - train - INFO - Train: 1 [ 155/156 (100%)]  Loss:  4.038651 (4.1557)  Time: 0.521s, 15723.39/s  (1.210s, 6767.97/s)  LR: 9.998e-03  Data: 0.000 (0.264)
2025-05-31 10:38:01,592 - train - INFO - Test: [   0/48]  Time: 6.635 (6.635)  Loss:  2.4375 (2.4375)  Acc@1: 45.9961 (45.9961)  Acc@5: 72.8516 (72.8516)
2025-05-31 10:38:59,339 - train - INFO - Test: [  48/48]  Time: 2.206 (1.314)  Loss:  2.5781 (3.0678)  Acc@1: 45.8726 (36.2960)  Acc@5: 71.2264 (62.4480)
2025-05-31 10:39:06,210 - train - INFO - Train: 2 [   0/156 (  0%)]  Loss:  4.016989 (4.0170)  Time: 6.513s, 1257.82/s  (6.513s, 1257.82/s)  LR: 9.990e-03  Data: 5.987 (5.987)
2025-05-31 10:40:04,892 - train - INFO - Train: 2 [  50/156 ( 32%)]  Loss:  3.667029 (3.8420)  Time: 0.529s, 15493.09/s  (1.278s, 6408.44/s)  LR: 9.990e-03  Data: 0.000 (0.687)
2025-05-31 10:41:05,271 - train - INFO - Train: 2 [ 100/156 ( 65%)]  Loss:  3.596947 (3.7603)  Time: 3.258s, 2514.45/s  (1.243s, 6589.00/s)  LR: 9.990e-03  Data: 0.799 (0.484)
2025-05-31 10:42:03,339 - train - INFO - Train: 2 [ 150/156 ( 97%)]  Loss:  3.558998 (3.7100)  Time: 0.526s, 15566.99/s  (1.216s, 6736.03/s)  LR: 9.990e-03  Data: 0.000 (0.347)
2025-05-31 10:42:08,427 - train - INFO - Train: 2 [ 155/156 (100%)]  Loss:  3.570013 (3.6820)  Time: 0.522s, 15687.67/s  (1.210s, 6771.48/s)  LR: 9.990e-03  Data: 0.000 (0.336)
2025-05-31 10:42:15,068 - train - INFO - Test: [   0/48]  Time: 6.316 (6.316)  Loss:  1.5068 (1.5068)  Acc@1: 68.1641 (68.1641)  Acc@5: 88.1836 (88.1836)
2025-05-31 10:43:13,499 - train - INFO - Test: [  48/48]  Time: 2.799 (1.321)  Loss:  1.3271 (2.1646)  Acc@1: 73.3491 (53.1200)  Acc@5: 89.1509 (77.9680)
2025-05-31 10:43:20,058 - train - INFO - Train: 3 [   0/156 (  0%)]  Loss:  3.544803 (3.5448)  Time: 6.019s, 1361.10/s  (6.019s, 1361.10/s)  LR: 9.978e-03  Data: 5.488 (5.488)
2025-05-31 10:44:18,662 - train - INFO - Train: 3 [  50/156 ( 32%)]  Loss:  3.538912 (3.5419)  Time: 0.529s, 15487.33/s  (1.267s, 6465.34/s)  LR: 9.978e-03  Data: 0.000 (0.683)
2025-05-31 10:45:18,713 - train - INFO - Train: 3 [ 100/156 ( 65%)]  Loss:  3.467232 (3.5170)  Time: 3.165s, 2588.66/s  (1.234s, 6636.64/s)  LR: 9.978e-03  Data: 0.737 (0.500)
2025-05-31 10:46:16,838 - train - INFO - Train: 3 [ 150/156 ( 97%)]  Loss:  3.451979 (3.5007)  Time: 0.526s, 15582.74/s  (1.211s, 6767.21/s)  LR: 9.978e-03  Data: 0.000 (0.347)
2025-05-31 10:46:22,136 - train - INFO - Train: 3 [ 155/156 (100%)]  Loss:  3.424266 (3.4854)  Time: 0.522s, 15679.50/s  (1.206s, 6794.40/s)  LR: 9.978e-03  Data: 0.000 (0.336)
2025-05-31 10:46:28,868 - train - INFO - Test: [   0/48]  Time: 6.428 (6.428)  Loss:  1.4990 (1.4990)  Acc@1: 72.0703 (72.0703)  Acc@5: 89.2578 (89.2578)
2025-05-31 10:47:27,048 - train - INFO - Test: [  48/48]  Time: 2.982 (1.319)  Loss:  1.3525 (2.0622)  Acc@1: 74.5283 (56.3040)  Acc@5: 89.6226 (80.3380)
2025-05-31 10:47:33,842 - train - INFO - Train: 4 [   0/156 (  0%)]  Loss:  3.402242 (3.4022)  Time: 6.253s, 1310.10/s  (6.253s, 1310.10/s)  LR: 9.961e-03  Data: 5.201 (5.201)
2025-05-31 10:48:32,623 - train - INFO - Train: 4 [  50/156 ( 32%)]  Loss:  3.379215 (3.3907)  Time: 0.527s, 15555.09/s  (1.275s, 6424.22/s)  LR: 9.961e-03  Data: 0.000 (0.648)
2025-05-31 10:49:33,223 - train - INFO - Train: 4 [ 100/156 ( 65%)]  Loss:  3.342668 (3.3747)  Time: 3.365s, 2434.21/s  (1.244s, 6585.83/s)  LR: 9.961e-03  Data: 0.781 (0.460)
2025-05-31 10:50:31,193 - train - INFO - Train: 4 [ 150/156 ( 97%)]  Loss:  3.339417 (3.3659)  Time: 0.529s, 15491.94/s  (1.216s, 6737.36/s)  LR: 9.961e-03  Data: 0.000 (0.334)
2025-05-31 10:50:36,252 - train - INFO - Train: 4 [ 155/156 (100%)]  Loss:  3.312668 (3.3552)  Time: 0.523s, 15671.54/s  (1.209s, 6773.81/s)  LR: 9.961e-03  Data: 0.000 (0.325)
2025-05-31 10:50:43,048 - train - INFO - Test: [   0/48]  Time: 6.508 (6.508)  Loss:  1.9209 (1.9209)  Acc@1: 70.5078 (70.5078)  Acc@5: 88.9648 (88.9648)
2025-05-31 10:51:41,389 - train - INFO - Test: [  48/48]  Time: 2.343 (1.323)  Loss:  1.6641 (2.3742)  Acc@1: 73.1132 (55.2640)  Acc@5: 91.2736 (79.7240)
2025-05-31 10:51:48,365 - train - INFO - Train: 5 [   0/156 (  0%)]  Loss:  3.366850 (3.3668)  Time: 6.600s, 1241.24/s  (6.600s, 1241.24/s)  LR: 9.939e-03  Data: 6.020 (6.020)
2025-05-31 10:52:45,978 - train - INFO - Train: 5 [  50/156 ( 32%)]  Loss:  3.329144 (3.3480)  Time: 0.529s, 15481.29/s  (1.259s, 6506.46/s)  LR: 9.939e-03  Data: 0.000 (0.672)
2025-05-31 10:53:47,592 - train - INFO - Train: 5 [ 100/156 ( 65%)]  Loss:  3.251656 (3.3159)  Time: 3.009s, 2722.70/s  (1.246s, 6575.85/s)  LR: 9.939e-03  Data: 0.193 (0.450)
2025-05-31 10:54:46,298 - train - INFO - Train: 5 [ 150/156 ( 97%)]  Loss:  3.298435 (3.3115)  Time: 0.529s, 15476.27/s  (1.222s, 6703.57/s)  LR: 9.939e-03  Data: 0.000 (0.303)
2025-05-31 10:54:51,431 - train - INFO - Train: 5 [ 155/156 (100%)]  Loss:  3.263565 (3.3019)  Time: 0.522s, 15681.22/s  (1.216s, 6738.16/s)  LR: 9.939e-03  Data: 0.000 (0.293)
2025-05-31 10:54:58,193 - train - INFO - Test: [   0/48]  Time: 6.469 (6.469)  Loss:  1.2178 (1.2178)  Acc@1: 75.0977 (75.0977)  Acc@5: 91.4062 (91.4062)
2025-05-31 10:55:56,292 - train - INFO - Test: [  48/48]  Time: 2.326 (1.318)  Loss:  1.0557 (1.8758)  Acc@1: 77.5943 (58.9240)  Acc@5: 91.3915 (82.1080)
2025-05-31 10:56:02,703 - train - INFO - Train: 6 [   0/156 (  0%)]  Loss:  3.261967 (3.2620)  Time: 5.851s, 1400.13/s  (5.851s, 1400.13/s)  LR: 9.912e-03  Data: 5.263 (5.263)
2025-05-31 10:56:20,058 - train - INFO - Namespace(data_dir='/data/dataset/imagenet', dataset='image_folder', train_split='train', val_split='validation', model='ResNet18', pretrained=False, initial_checkpoint='output/train/20250427-184912-ResNet18-224/best.pth.tar.pth', resume='', no_resume_opt=False, num_classes=1000, gp=None, img_size=224, input_size=None, crop_pct=None, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], interpolation='', batch_size=1024, validation_batch_size_multiplier=1, gpu=0, opt='sgd', opt_eps=None, opt_betas=None, momentum=0.9, weight_decay=0.0001, clip_grad=None, clip_mode='norm', sched='cosine', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, lr_cycle_mul=1.0, lr_cycle_limit=1, warmup_lr=0.0001, min_lr=1e-05, epochs=100, epoch_repeats=0.0, start_epoch=None, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, no_aug=False, scale=[0, 1.0], ratio=[0.75, 1.3333333333333333], hflip=0, vflip=0.0, color_jitter=None, aa='rand-m9-mstd0.5-inc1', aug_splits=0, jsd=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.0, cutmix=0.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', mixup_off_epoch=0, smoothing=0.1, train_interpolation='random', bn_tf=False, bn_momentum=None, bn_eps=None, sync_bn=True, dist_bn='', split_bn=False, log_interval=50, recovery_interval=0, checkpoint_hist=1, save_images=False, amp=False, apex_amp=False, native_amp=False, channels_last=False, pin_mem=True, output='', experiment='', eval_metric='top1', tta=0, use_multi_epochs_loader=False, torchscript=False, log_wandb=False, wq_enable=True, wq_mode='LSQ', wq_bitw=32, wq_pos=None, wq_neg=None, wq_per_channel=True, wq_asym=True, aq_enable=True, aq_mode='LSQ', aq_bitw=32, aq_pos=None, aq_neg=None, aq_asym=True, qmodules=['convbn_first;wq:bit:8;aq:bit:8', 'layer1.0.convbn1', 'layer1.0.convbn2', 'layer1.1.convbn1', 'layer1.1.convbn2', 'layer2.0.convbn1', 'layer2.0.convbn2', 'layer2.1.convbn1', 'layer2.1.convbn2', 'layer3.0.convbn1', 'layer3.0.convbn2', 'layer3.1.convbn1', 'layer3.1.convbn2', 'layer4.0.convbn1', 'layer4.0.convbn2', 'layer4.1.convbn1', 'layer4.1.convbn2', 'fc;wq:bit:8;aq:bit:8'], resq_modules=['relu', 'layer1.0.relu2', 'layer1.1.relu2', 'layer2.0.relu2', 'layer3.0.downsample', 'layer2.1.relu2', 'layer3.0.relu2', 'layer3.0.downsample', 'layer3.1.relu2', 'layer4.0.downsample', 'layer4.0.relu2', 'layer4.1.relu2'], resq_enable=True, resq_mode='LSQ', resq_bitw=16, resq_pos=None, resq_neg=None, resq_asym=False, aq_per_channel=False, powerof2=True, world_size=8, local_rank=-1, dist_on_itp=False, dist_url='env://', device='cuda', seed=0, dist_eval=False, use_kd=False, kd_alpha=1.0, teacher='ResNet18', teacher_checkpoint='output/train/20250427-184912-ResNet18-224/best.pth.tar.pth', log_name='resnet18_imagenet_avgpool_acc15', budget=1, bw_list='5, 5, 5, 5, 5, 4, 4, 3, 4, 3, 3, 3, 3, 2, 2, 2', ba_list='4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3', workers=4, multiprocessing_distributed=True, rank=0, distributed=True, dist_backend='nccl', **{'weight-decay': '1e-4'})
2025-05-31 10:56:21,548 - train - INFO - Model ResNet18 created, param count:11689512
2025-05-31 10:56:22,949 - train - INFO - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2025-05-31 10:56:22,950 - train - INFO - Using native Torch AMP. Training in mixed precision.
2025-05-31 10:56:22,950 - train - INFO - Scheduled epochs: 110
2025-05-31 10:56:22,951 - train - INFO - Verifying initial model in test dataset
2025-05-31 10:56:22,951 - train - INFO - cuda:0
2025-05-31 10:56:32,512 - train - INFO - Test: [   0/48]  Time: 9.561 (9.561)  Loss:  6.5078 (6.5078)  Acc@1:  0.3906 ( 0.3906)  Acc@5:  2.5391 ( 2.5391)
2025-05-31 10:57:28,723 - train - INFO - Test: [  48/48]  Time: 1.944 (1.342)  Loss:  6.1562 (6.6466)  Acc@1:  4.0094 ( 1.2660)  Acc@5: 10.2594 ( 4.0520)
2025-05-31 10:57:28,936 - train - INFO - DistributedDataParallel(
  (module): ResNet(
    (relu): ReLU(
      (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
    )
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (maxpool): AvgPool2d(kernel_size=3, stride=2, padding=1)
    (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (convbn_first): QConvBn2d(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w_fn): LsqQuantizer(bit=8, pos=127, neg=-128, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
      (quan_a_fn): LsqQuantizer(bit=8, pos=127, neg=-127, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=True, per_channel=False, apot=False )
      (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=4, pos=7, neg=-8, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=4, pos=7, neg=-8, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=4, pos=7, neg=-8, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (quan_a_fn): LsqQuantizer(bit=16, pos=32767, neg=-32767, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=True, per_channel=False, apot=False )
        )
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=2, pos=1, neg=-2, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (quan_a_fn): LsqQuantizer(bit=16, pos=32767, neg=-32767, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=True, per_channel=False, apot=False )
        )
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=2, pos=1, neg=-2, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=2, pos=1, neg=-2, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): QLinear(
      in_features=512, out_features=1000, bias=True
      (quan_w_fn): LsqQuantizer(bit=8, pos=127, neg=-128, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
      (quan_a_fn): LsqQuantizer(bit=8, pos=255, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
    )
  )
)
2025-05-31 10:57:37,043 - train - INFO - Train: 0 [   0/156 (  0%)]  Loss:  5.896966 (5.8970)  Time: 8.104s, 1010.85/s  (8.104s, 1010.85/s)  LR: 1.000e-02  Data: 5.777 (5.777)
2025-05-31 10:58:32,585 - train - INFO - Train: 0 [  50/156 ( 32%)]  Loss:  4.470128 (5.1835)  Time: 0.526s, 15566.79/s  (1.248s, 6564.47/s)  LR: 1.000e-02  Data: 0.000 (0.516)
2025-05-31 10:59:32,070 - train - INFO - Train: 0 [ 100/156 ( 65%)]  Loss:  4.065176 (4.8108)  Time: 2.703s, 3030.35/s  (1.219s, 6719.97/s)  LR: 1.000e-02  Data: 1.522 (0.439)
2025-05-31 11:00:29,926 - train - INFO - Train: 0 [ 150/156 ( 97%)]  Loss:  3.843566 (4.5690)  Time: 0.751s, 10903.28/s  (1.199s, 6835.06/s)  LR: 1.000e-02  Data: 0.000 (0.399)
2025-05-31 11:00:35,141 - train - INFO - Train: 0 [ 155/156 (100%)]  Loss:  4.064226 (4.4680)  Time: 0.760s, 10772.43/s  (1.194s, 6863.80/s)  LR: 1.000e-02  Data: 0.000 (0.393)
2025-05-31 11:00:41,899 - train - INFO - Test: [   0/48]  Time: 6.464 (6.464)  Loss:  1.8574 (1.8574)  Acc@1: 58.6914 (58.6914)  Acc@5: 84.6680 (84.6680)
2025-05-31 11:01:42,312 - train - INFO - Test: [  48/48]  Time: 3.718 (1.365)  Loss:  1.5820 (2.5224)  Acc@1: 67.8066 (45.6080)  Acc@5: 85.2594 (71.4760)
2025-05-31 11:01:49,078 - train - INFO - Train: 1 [   0/156 (  0%)]  Loss:  3.898241 (3.8982)  Time: 6.287s, 1303.10/s  (6.287s, 1303.10/s)  LR: 9.998e-03  Data: 5.347 (5.347)
2025-05-31 11:02:48,199 - train - INFO - Train: 1 [  50/156 ( 32%)]  Loss:  3.736769 (3.8175)  Time: 0.529s, 15472.71/s  (1.282s, 6387.95/s)  LR: 9.998e-03  Data: 0.000 (0.414)
2025-05-31 11:03:48,656 - train - INFO - Train: 1 [ 100/156 ( 65%)]  Loss:  3.728924 (3.7880)  Time: 2.170s, 3775.74/s  (1.246s, 6574.10/s)  LR: 9.998e-03  Data: 0.000 (0.220)
2025-05-31 11:04:47,633 - train - INFO - Train: 1 [ 150/156 ( 97%)]  Loss:  3.732229 (3.7740)  Time: 0.525s, 15613.41/s  (1.224s, 6692.62/s)  LR: 9.998e-03  Data: 0.000 (0.148)
2025-05-31 11:04:52,580 - train - INFO - Train: 1 [ 155/156 (100%)]  Loss:  3.691475 (3.7575)  Time: 0.522s, 15705.54/s  (1.216s, 6734.13/s)  LR: 9.998e-03  Data: 0.000 (0.143)
2025-05-31 11:04:59,535 - train - INFO - Test: [   0/48]  Time: 6.653 (6.653)  Loss:  1.7197 (1.7197)  Acc@1: 63.3789 (63.3789)  Acc@5: 87.8906 (87.8906)
2025-05-31 11:05:57,959 - train - INFO - Test: [  48/48]  Time: 2.124 (1.328)  Loss:  1.6104 (2.3831)  Acc@1: 68.7500 (49.8080)  Acc@5: 85.0236 (75.2200)
2025-05-31 11:06:04,723 - train - INFO - Train: 2 [   0/156 (  0%)]  Loss:  3.689186 (3.6892)  Time: 6.221s, 1316.86/s  (6.221s, 1316.86/s)  LR: 9.990e-03  Data: 5.455 (5.455)
2025-05-31 11:07:03,437 - train - INFO - Train: 2 [  50/156 ( 32%)]  Loss:  3.588188 (3.6387)  Time: 0.527s, 15541.88/s  (1.273s, 6434.38/s)  LR: 9.990e-03  Data: 0.000 (0.482)
2025-05-31 11:08:04,284 - train - INFO - Train: 2 [ 100/156 ( 65%)]  Loss:  3.557158 (3.6115)  Time: 2.730s, 3001.21/s  (1.245s, 6578.32/s)  LR: 9.990e-03  Data: 0.000 (0.244)
2025-05-31 11:09:02,524 - train - INFO - Train: 2 [ 150/156 ( 97%)]  Loss:  3.524970 (3.5899)  Time: 0.527s, 15535.29/s  (1.219s, 6722.30/s)  LR: 9.990e-03  Data: 0.000 (0.164)
2025-05-31 11:09:07,352 - train - INFO - Train: 2 [ 155/156 (100%)]  Loss:  3.709320 (3.6138)  Time: 0.521s, 15718.38/s  (1.211s, 6767.39/s)  LR: 9.990e-03  Data: 0.000 (0.158)
2025-05-31 11:09:14,553 - train - INFO - Test: [   0/48]  Time: 6.902 (6.902)  Loss:  1.5244 (1.5244)  Acc@1: 67.2852 (67.2852)  Acc@5: 88.7695 (88.7695)
2025-05-31 11:10:12,675 - train - INFO - Test: [  48/48]  Time: 1.732 (1.327)  Loss:  1.5244 (2.2539)  Acc@1: 70.8726 (51.4120)  Acc@5: 86.5566 (76.5840)
2025-05-31 11:10:19,817 - train - INFO - Train: 3 [   0/156 (  0%)]  Loss:  3.634082 (3.6341)  Time: 6.599s, 1241.33/s  (6.599s, 1241.33/s)  LR: 9.978e-03  Data: 6.075 (6.075)
2025-05-31 11:11:18,526 - train - INFO - Train: 3 [  50/156 ( 32%)]  Loss:  3.511294 (3.5727)  Time: 0.530s, 15449.70/s  (1.281s, 6397.46/s)  LR: 9.978e-03  Data: 0.000 (0.755)
2025-05-31 11:12:18,724 - train - INFO - Train: 3 [ 100/156 ( 65%)]  Loss:  3.475687 (3.5404)  Time: 3.002s, 2728.53/s  (1.243s, 6592.78/s)  LR: 9.978e-03  Data: 2.488 (0.717)
2025-05-31 11:13:16,061 - train - INFO - Train: 3 [ 150/156 ( 97%)]  Loss:  3.448584 (3.5174)  Time: 0.528s, 15525.85/s  (1.211s, 6765.71/s)  LR: 9.978e-03  Data: 0.000 (0.685)
2025-05-31 11:13:21,073 - train - INFO - Train: 3 [ 155/156 (100%)]  Loss:  3.434904 (3.5009)  Time: 0.523s, 15656.33/s  (1.204s, 6803.37/s)  LR: 9.978e-03  Data: 0.000 (0.678)
2025-05-31 11:13:27,994 - train - INFO - Test: [   0/48]  Time: 6.624 (6.624)  Loss:  1.3652 (1.3652)  Acc@1: 71.5820 (71.5820)  Acc@5: 89.8438 (89.8438)
2025-05-31 11:14:25,778 - train - INFO - Test: [  48/48]  Time: 2.350 (1.314)  Loss:  1.2412 (2.0541)  Acc@1: 74.7642 (55.9160)  Acc@5: 89.3868 (79.7780)
2025-05-31 11:14:32,855 - train - INFO - Train: 4 [   0/156 (  0%)]  Loss:  3.416214 (3.4162)  Time: 6.540s, 1252.56/s  (6.540s, 1252.56/s)  LR: 9.961e-03  Data: 6.024 (6.024)
2025-05-31 11:15:32,721 - train - INFO - Train: 4 [  50/156 ( 32%)]  Loss:  3.381505 (3.3989)  Time: 0.530s, 15449.54/s  (1.302s, 6291.77/s)  LR: 9.961e-03  Data: 0.000 (0.777)
2025-05-31 11:16:32,339 - train - INFO - Train: 4 [ 100/156 ( 65%)]  Loss:  3.331373 (3.3764)  Time: 2.865s, 2859.74/s  (1.248s, 6565.66/s)  LR: 9.961e-03  Data: 2.339 (0.719)
2025-05-31 11:17:30,216 - train - INFO - Train: 4 [ 150/156 ( 97%)]  Loss:  3.348646 (3.3694)  Time: 0.761s, 10771.39/s  (1.218s, 6726.65/s)  LR: 9.961e-03  Data: 0.000 (0.681)
2025-05-31 11:17:34,957 - train - INFO - Train: 4 [ 155/156 (100%)]  Loss:  3.304898 (3.3565)  Time: 0.523s, 15670.15/s  (1.209s, 6774.77/s)  LR: 9.961e-03  Data: 0.000 (0.672)
2025-05-31 11:17:41,701 - train - INFO - Test: [   0/48]  Time: 6.463 (6.463)  Loss:  1.4229 (1.4229)  Acc@1: 71.0938 (71.0938)  Acc@5: 89.7461 (89.7461)
2025-05-31 11:18:40,061 - train - INFO - Test: [  48/48]  Time: 2.457 (1.323)  Loss:  1.3594 (1.9664)  Acc@1: 74.8821 (57.5240)  Acc@5: 90.2123 (81.3000)
2025-05-31 11:18:46,868 - train - INFO - Train: 5 [   0/156 (  0%)]  Loss:  3.365951 (3.3660)  Time: 6.244s, 1311.99/s  (6.244s, 1311.99/s)  LR: 9.939e-03  Data: 5.725 (5.725)
2025-05-31 11:19:46,138 - train - INFO - Train: 5 [  50/156 ( 32%)]  Loss:  3.351651 (3.3588)  Time: 0.530s, 15453.59/s  (1.285s, 6377.47/s)  LR: 9.939e-03  Data: 0.000 (0.600)
2025-05-31 11:20:46,350 - train - INFO - Train: 5 [ 100/156 ( 65%)]  Loss:  3.290990 (3.3362)  Time: 2.456s, 3335.00/s  (1.245s, 6581.13/s)  LR: 9.939e-03  Data: 1.940 (0.495)
2025-05-31 11:21:45,406 - train - INFO - Train: 5 [ 150/156 ( 97%)]  Loss:  3.426074 (3.3587)  Time: 0.525s, 15594.47/s  (1.224s, 6694.52/s)  LR: 9.939e-03  Data: 0.000 (0.432)
2025-05-31 11:21:50,466 - train - INFO - Train: 5 [ 155/156 (100%)]  Loss:  3.368483 (3.3606)  Time: 0.524s, 15635.11/s  (1.217s, 6731.86/s)  LR: 9.939e-03  Data: 0.000 (0.422)
2025-05-31 11:21:57,003 - train - INFO - Test: [   0/48]  Time: 6.247 (6.247)  Loss:  1.3057 (1.3057)  Acc@1: 72.2656 (72.2656)  Acc@5: 90.6250 (90.6250)
2025-05-31 11:22:55,006 - train - INFO - Test: [  48/48]  Time: 2.430 (1.311)  Loss:  1.2334 (1.9506)  Acc@1: 76.1793 (57.9000)  Acc@5: 90.2123 (81.4720)
2025-05-31 11:23:01,485 - train - INFO - Train: 6 [   0/156 (  0%)]  Loss:  3.380082 (3.3801)  Time: 5.945s, 1377.89/s  (5.945s, 1377.89/s)  LR: 9.912e-03  Data: 5.419 (5.419)
2025-05-31 11:24:00,250 - train - INFO - Train: 6 [  50/156 ( 32%)]  Loss:  3.288269 (3.3342)  Time: 0.537s, 15264.75/s  (1.269s, 6456.44/s)  LR: 9.912e-03  Data: 0.000 (0.545)
2025-05-31 11:25:00,960 - train - INFO - Train: 6 [ 100/156 ( 65%)]  Loss:  3.331568 (3.3333)  Time: 1.851s, 4425.09/s  (1.242s, 6597.09/s)  LR: 9.912e-03  Data: 0.000 (0.284)
2025-05-31 11:26:01,359 - train - INFO - Train: 6 [ 150/156 ( 97%)]  Loss:  3.322881 (3.3307)  Time: 0.530s, 15465.33/s  (1.231s, 6657.11/s)  LR: 9.912e-03  Data: 0.000 (0.190)
2025-05-31 11:26:06,345 - train - INFO - Train: 6 [ 155/156 (100%)]  Loss:  3.301069 (3.3248)  Time: 0.523s, 15663.57/s  (1.223s, 6697.86/s)  LR: 9.912e-03  Data: 0.000 (0.184)
2025-05-31 11:26:13,005 - train - INFO - Test: [   0/48]  Time: 6.365 (6.365)  Loss:  1.2275 (1.2275)  Acc@1: 73.8281 (73.8281)  Acc@5: 91.2109 (91.2109)
2025-05-31 11:27:11,879 - train - INFO - Test: [  48/48]  Time: 2.793 (1.331)  Loss:  1.1436 (1.8762)  Acc@1: 78.1840 (58.9420)  Acc@5: 89.8585 (82.1800)
2025-05-31 11:27:18,795 - train - INFO - Train: 7 [   0/156 (  0%)]  Loss:  3.323471 (3.3235)  Time: 6.364s, 1287.32/s  (6.364s, 1287.32/s)  LR: 9.880e-03  Data: 5.258 (5.258)
2025-05-31 11:28:17,264 - train - INFO - Train: 7 [  50/156 ( 32%)]  Loss:  3.226249 (3.2749)  Time: 0.528s, 15517.90/s  (1.271s, 6444.46/s)  LR: 9.880e-03  Data: 0.000 (0.486)
2025-05-31 11:29:16,784 - train - INFO - Train: 7 [ 100/156 ( 65%)]  Loss:  3.273046 (3.2743)  Time: 3.078s, 2661.88/s  (1.231s, 6653.76/s)  LR: 9.880e-03  Data: 0.826 (0.374)
2025-05-31 11:30:15,496 - train - INFO - Train: 7 [ 150/156 ( 97%)]  Loss:  3.226846 (3.2624)  Time: 0.527s, 15544.23/s  (1.212s, 6757.33/s)  LR: 9.880e-03  Data: 0.000 (0.282)
2025-05-31 11:30:20,365 - train - INFO - Train: 7 [ 155/156 (100%)]  Loss:  3.247619 (3.2594)  Time: 0.525s, 15610.09/s  (1.205s, 6800.24/s)  LR: 9.880e-03  Data: 0.000 (0.273)
2025-05-31 11:30:26,998 - train - INFO - Test: [   0/48]  Time: 6.337 (6.337)  Loss:  1.2959 (1.2959)  Acc@1: 73.6328 (73.6328)  Acc@5: 91.5039 (91.5039)
2025-05-31 11:31:25,436 - train - INFO - Test: [  48/48]  Time: 2.431 (1.322)  Loss:  1.2617 (1.9132)  Acc@1: 72.7594 (57.8920)  Acc@5: 89.6226 (81.5400)
2025-05-31 11:31:32,085 - train - INFO - Train: 8 [   0/156 (  0%)]  Loss:  3.241719 (3.2417)  Time: 6.276s, 1305.26/s  (6.276s, 1305.26/s)  LR: 9.843e-03  Data: 5.193 (5.193)
2025-05-31 11:32:31,718 - train - INFO - Train: 8 [  50/156 ( 32%)]  Loss:  3.308999 (3.2754)  Time: 0.529s, 15493.30/s  (1.292s, 6338.99/s)  LR: 9.843e-03  Data: 0.001 (0.359)
2025-05-31 11:33:31,593 - train - INFO - Train: 8 [ 100/156 ( 65%)]  Loss:  3.203338 (3.2514)  Time: 1.653s, 4954.57/s  (1.245s, 6578.03/s)  LR: 9.843e-03  Data: 0.000 (0.196)
2025-05-31 11:34:32,117 - train - INFO - Train: 8 [ 150/156 ( 97%)]  Loss:  3.178137 (3.2330)  Time: 0.526s, 15580.51/s  (1.234s, 6639.66/s)  LR: 9.843e-03  Data: 0.000 (0.131)
2025-05-31 11:34:36,958 - train - INFO - Train: 8 [ 155/156 (100%)]  Loss:  3.214624 (3.2294)  Time: 0.524s, 15624.57/s  (1.225s, 6685.82/s)  LR: 9.843e-03  Data: 0.000 (0.127)
2025-05-31 11:34:43,701 - train - INFO - Test: [   0/48]  Time: 6.453 (6.453)  Loss:  1.7979 (1.7979)  Acc@1: 63.2812 (63.2812)  Acc@5: 83.1055 (83.1055)
2025-05-31 11:35:42,441 - train - INFO - Test: [  48/48]  Time: 2.928 (1.330)  Loss:  1.4590 (2.2732)  Acc@1: 70.8726 (50.9300)  Acc@5: 87.8538 (75.8620)
2025-05-31 11:35:49,239 - train - INFO - Train: 9 [   0/156 (  0%)]  Loss:  3.200822 (3.2008)  Time: 6.415s, 1277.02/s  (6.415s, 1277.02/s)  LR: 9.802e-03  Data: 5.405 (5.405)
2025-05-31 11:36:47,219 - train - INFO - Train: 9 [  50/156 ( 32%)]  Loss:  3.305404 (3.2531)  Time: 0.529s, 15472.83/s  (1.263s, 6488.23/s)  LR: 9.802e-03  Data: 0.000 (0.595)
2025-05-31 11:37:47,374 - train - INFO - Train: 9 [ 100/156 ( 65%)]  Loss:  3.243825 (3.2500)  Time: 2.445s, 3351.06/s  (1.233s, 6643.20/s)  LR: 9.802e-03  Data: 0.990 (0.459)
2025-05-31 11:38:46,447 - train - INFO - Train: 9 [ 150/156 ( 97%)]  Loss:  3.170556 (3.2302)  Time: 0.526s, 15561.68/s  (1.216s, 6736.75/s)  LR: 9.802e-03  Data: 0.000 (0.355)
2025-05-31 11:38:51,359 - train - INFO - Train: 9 [ 155/156 (100%)]  Loss:  3.188182 (3.2218)  Time: 0.523s, 15651.84/s  (1.209s, 6778.47/s)  LR: 9.802e-03  Data: 0.000 (0.344)
2025-05-31 11:38:58,229 - train - INFO - Test: [   0/48]  Time: 6.567 (6.567)  Loss:  1.1602 (1.1602)  Acc@1: 75.4883 (75.4883)  Acc@5: 91.4062 (91.4062)
2025-05-31 11:39:57,001 - train - INFO - Test: [  48/48]  Time: 2.606 (1.333)  Loss:  1.1201 (1.7927)  Acc@1: 75.5896 (60.3940)  Acc@5: 91.3915 (83.0920)
2025-05-31 11:40:03,771 - train - INFO - Train: 10 [   0/156 (  0%)]  Loss:  3.186213 (3.1862)  Time: 6.217s, 1317.64/s  (6.217s, 1317.64/s)  LR: 9.756e-03  Data: 5.449 (5.449)
2025-05-31 11:41:02,082 - train - INFO - Train: 10 [  50/156 ( 32%)]  Loss:  3.167852 (3.1770)  Time: 0.528s, 15510.76/s  (1.265s, 6474.86/s)  LR: 9.756e-03  Data: 0.000 (0.518)
2025-05-31 11:42:02,054 - train - INFO - Train: 10 [ 100/156 ( 65%)]  Loss:  3.190739 (3.1816)  Time: 2.639s, 3104.50/s  (1.233s, 6645.91/s)  LR: 9.756e-03  Data: 0.001 (0.313)
2025-05-31 11:42:59,873 - train - INFO - Train: 10 [ 150/156 ( 97%)]  Loss:  3.173435 (3.1796)  Time: 0.530s, 15467.97/s  (1.207s, 6784.93/s)  LR: 9.756e-03  Data: 0.000 (0.219)
2025-05-31 11:43:05,215 - train - INFO - Train: 10 [ 155/156 (100%)]  Loss:  3.162021 (3.1761)  Time: 0.524s, 15627.08/s  (1.203s, 6810.08/s)  LR: 9.756e-03  Data: 0.000 (0.212)
2025-05-31 11:43:11,672 - train - INFO - Test: [   0/48]  Time: 6.164 (6.164)  Loss:  1.1016 (1.1016)  Acc@1: 77.5391 (77.5391)  Acc@5: 92.3828 (92.3828)
2025-05-31 11:44:10,140 - train - INFO - Test: [  48/48]  Time: 2.683 (1.319)  Loss:  1.0322 (1.7605)  Acc@1: 78.4198 (61.3300)  Acc@5: 92.9245 (83.5160)
2025-05-31 11:44:16,930 - train - INFO - Train: 11 [   0/156 (  0%)]  Loss:  3.116356 (3.1164)  Time: 6.253s, 1310.01/s  (6.253s, 1310.01/s)  LR: 9.705e-03  Data: 5.085 (5.085)
2025-05-31 11:45:15,122 - train - INFO - Train: 11 [  50/156 ( 32%)]  Loss:  3.146795 (3.1316)  Time: 0.533s, 15369.63/s  (1.264s, 6483.13/s)  LR: 9.705e-03  Data: 0.000 (0.498)
2025-05-31 11:46:15,155 - train - INFO - Train: 11 [ 100/156 ( 65%)]  Loss:  3.144416 (3.1359)  Time: 0.851s, 9630.37/s  (1.232s, 6647.02/s)  LR: 9.705e-03  Data: 0.000 (0.286)
2025-05-31 11:47:14,477 - train - INFO - Train: 11 [ 150/156 ( 97%)]  Loss:  3.090657 (3.1246)  Time: 0.528s, 15517.54/s  (1.217s, 6730.23/s)  LR: 9.705e-03  Data: 0.000 (0.192)
2025-05-31 11:47:21,867 - train - INFO - Train: 11 [ 155/156 (100%)]  Loss:  3.122342 (3.1241)  Time: 2.693s, 3041.45/s  (1.226s, 6684.36/s)  LR: 9.705e-03  Data: 0.000 (0.186)
2025-05-31 11:47:28,647 - train - INFO - Test: [   0/48]  Time: 6.483 (6.483)  Loss:  1.1084 (1.1084)  Acc@1: 76.2695 (76.2695)  Acc@5: 91.9922 (91.9922)
2025-05-31 11:48:26,684 - train - INFO - Test: [  48/48]  Time: 2.277 (1.317)  Loss:  1.0381 (1.7331)  Acc@1: 76.8868 (61.5940)  Acc@5: 91.8632 (83.8280)
2025-05-31 11:48:33,557 - train - INFO - Train: 12 [   0/156 (  0%)]  Loss:  3.150132 (3.1501)  Time: 6.342s, 1291.76/s  (6.342s, 1291.76/s)  LR: 9.649e-03  Data: 5.820 (5.820)
2025-05-31 11:49:30,875 - train - INFO - Train: 12 [  50/156 ( 32%)]  Loss:  3.245821 (3.1980)  Time: 0.527s, 15538.74/s  (1.248s, 6563.28/s)  LR: 9.649e-03  Data: 0.000 (0.694)
2025-05-31 11:50:30,293 - train - INFO - Train: 12 [ 100/156 ( 65%)]  Loss:  3.190198 (3.1954)  Time: 1.576s, 5196.55/s  (1.219s, 6722.78/s)  LR: 9.649e-03  Data: 0.000 (0.463)
2025-05-31 11:51:30,054 - train - INFO - Train: 12 [ 150/156 ( 97%)]  Loss:  3.166564 (3.1882)  Time: 0.529s, 15493.09/s  (1.211s, 6765.71/s)  LR: 9.649e-03  Data: 0.000 (0.310)
2025-05-31 11:51:35,075 - train - INFO - Train: 12 [ 155/156 (100%)]  Loss:  3.124282 (3.1754)  Time: 0.523s, 15661.51/s  (1.204s, 6802.97/s)  LR: 9.649e-03  Data: 0.000 (0.300)
2025-05-31 11:51:41,882 - train - INFO - Test: [   0/48]  Time: 6.525 (6.525)  Loss:  1.1328 (1.1328)  Acc@1: 75.7812 (75.7812)  Acc@5: 92.1875 (92.1875)
2025-05-31 11:52:39,770 - train - INFO - Test: [  48/48]  Time: 2.235 (1.315)  Loss:  1.1367 (1.7353)  Acc@1: 75.9434 (61.1240)  Acc@5: 91.3915 (83.6420)
2025-05-31 11:52:46,628 - train - INFO - Train: 13 [   0/156 (  0%)]  Loss:  3.152873 (3.1529)  Time: 6.475s, 1265.20/s  (6.475s, 1265.20/s)  LR: 9.589e-03  Data: 5.776 (5.776)
2025-05-31 11:53:45,288 - train - INFO - Train: 13 [  50/156 ( 32%)]  Loss:  3.193776 (3.1733)  Time: 0.527s, 15535.87/s  (1.277s, 6414.53/s)  LR: 9.589e-03  Data: 0.000 (0.455)
2025-05-31 11:54:47,149 - train - INFO - Train: 13 [ 100/156 ( 65%)]  Loss:  3.191527 (3.1794)  Time: 3.138s, 2610.40/s  (1.257s, 6515.26/s)  LR: 9.589e-03  Data: 0.000 (0.233)
2025-05-31 11:55:44,627 - train - INFO - Train: 13 [ 150/156 ( 97%)]  Loss:  3.149803 (3.1720)  Time: 0.543s, 15099.82/s  (1.222s, 6705.72/s)  LR: 9.589e-03  Data: 0.000 (0.156)
2025-05-31 11:55:49,492 - train - INFO - Train: 13 [ 155/156 (100%)]  Loss:  3.131892 (3.1640)  Time: 0.524s, 15622.50/s  (1.214s, 6749.77/s)  LR: 9.589e-03  Data: 0.000 (0.151)
2025-05-31 11:55:56,086 - train - INFO - Test: [   0/48]  Time: 6.305 (6.305)  Loss:  1.4854 (1.4854)  Acc@1: 69.9219 (69.9219)  Acc@5: 88.1836 (88.1836)
2025-05-31 11:56:52,733 - train - INFO - Test: [  48/48]  Time: 2.019 (1.285)  Loss:  1.6631 (2.1125)  Acc@1: 67.4528 (54.5980)  Acc@5: 83.3726 (78.3200)
2025-05-31 11:56:59,744 - train - INFO - Train: 14 [   0/156 (  0%)]  Loss:  3.160080 (3.1601)  Time: 6.625s, 1236.49/s  (6.625s, 1236.49/s)  LR: 9.525e-03  Data: 5.625 (5.625)
2025-05-31 11:57:57,508 - train - INFO - Train: 14 [  50/156 ( 32%)]  Loss:  3.150828 (3.1555)  Time: 0.527s, 15558.81/s  (1.262s, 6489.00/s)  LR: 9.525e-03  Data: 0.000 (0.661)
2025-05-31 11:58:57,818 - train - INFO - Train: 14 [ 100/156 ( 65%)]  Loss:  3.145715 (3.1522)  Time: 2.965s, 2763.08/s  (1.235s, 6635.53/s)  LR: 9.525e-03  Data: 1.039 (0.526)
2025-05-31 11:59:55,704 - train - INFO - Train: 14 [ 150/156 ( 97%)]  Loss:  3.148420 (3.1513)  Time: 0.527s, 15531.60/s  (1.209s, 6775.32/s)  LR: 9.525e-03  Data: 0.000 (0.400)
2025-05-31 12:00:00,540 - train - INFO - Train: 14 [ 155/156 (100%)]  Loss:  3.165432 (3.1541)  Time: 0.524s, 15624.74/s  (1.201s, 6819.16/s)  LR: 9.525e-03  Data: 0.000 (0.388)
2025-05-31 12:00:07,221 - train - INFO - Test: [   0/48]  Time: 6.396 (6.396)  Loss:  1.1572 (1.1572)  Acc@1: 76.5625 (76.5625)  Acc@5: 91.5039 (91.5039)
2025-05-31 12:01:04,927 - train - INFO - Test: [  48/48]  Time: 2.437 (1.308)  Loss:  1.1641 (1.7797)  Acc@1: 77.2406 (61.1640)  Acc@5: 91.9811 (83.7620)
2025-05-31 12:01:11,986 - train - INFO - Train: 15 [   0/156 (  0%)]  Loss:  3.091301 (3.0913)  Time: 6.672s, 1227.89/s  (6.672s, 1227.89/s)  LR: 9.456e-03  Data: 6.147 (6.147)
2025-05-31 12:02:09,537 - train - INFO - Train: 15 [  50/156 ( 32%)]  Loss:  3.142752 (3.1170)  Time: 0.527s, 15536.48/s  (1.259s, 6505.97/s)  LR: 9.456e-03  Data: 0.000 (0.628)
2025-05-31 12:03:10,211 - train - INFO - Train: 15 [ 100/156 ( 65%)]  Loss:  3.108985 (3.1143)  Time: 3.309s, 2476.00/s  (1.237s, 6625.13/s)  LR: 9.456e-03  Data: 0.746 (0.458)
2025-05-31 12:04:07,305 - train - INFO - Train: 15 [ 150/156 ( 97%)]  Loss:  3.132413 (3.1189)  Time: 0.528s, 15528.23/s  (1.205s, 6797.48/s)  LR: 9.456e-03  Data: 0.000 (0.362)
2025-05-31 12:04:12,500 - train - INFO - Train: 15 [ 155/156 (100%)]  Loss:  3.099768 (3.1150)  Time: 0.522s, 15699.97/s  (1.200s, 6827.81/s)  LR: 9.456e-03  Data: 0.000 (0.354)
2025-05-31 12:04:19,317 - train - INFO - Test: [   0/48]  Time: 6.527 (6.527)  Loss:  1.1377 (1.1377)  Acc@1: 76.5625 (76.5625)  Acc@5: 91.2109 (91.2109)
2025-05-31 12:05:16,836 - train - INFO - Test: [  48/48]  Time: 2.572 (1.307)  Loss:  1.0459 (1.7478)  Acc@1: 77.2406 (61.1880)  Acc@5: 92.2170 (83.6720)
2025-05-31 12:05:23,795 - train - INFO - Train: 16 [   0/156 (  0%)]  Loss:  3.102403 (3.1024)  Time: 6.586s, 1243.86/s  (6.586s, 1243.86/s)  LR: 9.382e-03  Data: 6.068 (6.068)
2025-05-31 12:06:22,730 - train - INFO - Train: 16 [  50/156 ( 32%)]  Loss:  3.116017 (3.1092)  Time: 0.534s, 15347.05/s  (1.285s, 6376.66/s)  LR: 9.382e-03  Data: 0.000 (0.404)
2025-05-31 12:07:23,296 - train - INFO - Train: 16 [ 100/156 ( 65%)]  Loss:  3.123780 (3.1141)  Time: 2.977s, 2751.66/s  (1.248s, 6562.22/s)  LR: 9.382e-03  Data: 0.000 (0.204)
2025-05-31 12:08:21,092 - train - INFO - Train: 16 [ 150/156 ( 97%)]  Loss:  3.130865 (3.1183)  Time: 0.530s, 15458.80/s  (1.218s, 6727.18/s)  LR: 9.382e-03  Data: 0.000 (0.137)
2025-05-31 12:08:26,286 - train - INFO - Train: 16 [ 155/156 (100%)]  Loss:  3.119290 (3.1185)  Time: 0.522s, 15692.15/s  (1.212s, 6759.07/s)  LR: 9.382e-03  Data: 0.000 (0.132)
2025-05-31 12:08:32,835 - train - INFO - Test: [   0/48]  Time: 6.256 (6.256)  Loss:  1.2197 (1.2197)  Acc@1: 75.4883 (75.4883)  Acc@5: 91.6016 (91.6016)
2025-05-31 12:09:30,583 - train - INFO - Test: [  48/48]  Time: 1.105 (1.306)  Loss:  1.2178 (1.8127)  Acc@1: 75.2358 (60.0900)  Acc@5: 89.9764 (82.9780)
2025-05-31 12:09:37,273 - train - INFO - Train: 17 [   0/156 (  0%)]  Loss:  3.106407 (3.1064)  Time: 6.264s, 1307.81/s  (6.264s, 1307.81/s)  LR: 9.304e-03  Data: 5.729 (5.729)
2025-05-31 12:10:35,075 - train - INFO - Train: 17 [  50/156 ( 32%)]  Loss:  3.104241 (3.1053)  Time: 0.535s, 15311.53/s  (1.256s, 6521.62/s)  LR: 9.304e-03  Data: 0.000 (0.681)
2025-05-31 12:11:35,579 - train - INFO - Train: 17 [ 100/156 ( 65%)]  Loss:  3.094310 (3.1017)  Time: 3.143s, 2606.60/s  (1.233s, 6642.20/s)  LR: 9.304e-03  Data: 0.570 (0.491)
2025-05-31 12:12:33,503 - train - INFO - Train: 17 [ 150/156 ( 97%)]  Loss:  3.102798 (3.1019)  Time: 0.529s, 15485.12/s  (1.209s, 6778.45/s)  LR: 9.304e-03  Data: 0.000 (0.334)
2025-05-31 12:12:38,538 - train - INFO - Train: 17 [ 155/156 (100%)]  Loss:  3.082574 (3.0981)  Time: 0.523s, 15670.10/s  (1.202s, 6814.91/s)  LR: 9.304e-03  Data: 0.000 (0.323)
2025-05-31 12:12:45,003 - train - INFO - Test: [   0/48]  Time: 6.163 (6.163)  Loss:  1.1084 (1.1084)  Acc@1: 76.3672 (76.3672)  Acc@5: 92.8711 (92.8711)
2025-05-31 12:13:43,150 - train - INFO - Test: [  48/48]  Time: 2.102 (1.312)  Loss:  1.0693 (1.7399)  Acc@1: 76.7689 (60.9960)  Acc@5: 91.9811 (83.7060)
2025-05-31 12:13:49,568 - train - INFO - Train: 18 [   0/156 (  0%)]  Loss:  3.105415 (3.1054)  Time: 6.046s, 1355.01/s  (6.046s, 1355.01/s)  LR: 9.222e-03  Data: 5.458 (5.458)
2025-05-31 12:14:48,272 - train - INFO - Train: 18 [  50/156 ( 32%)]  Loss:  3.091122 (3.0983)  Time: 0.528s, 15525.05/s  (1.270s, 6452.49/s)  LR: 9.222e-03  Data: 0.000 (0.355)
2025-05-31 12:15:50,082 - train - INFO - Train: 18 [ 100/156 ( 65%)]  Loss:  3.121815 (3.1061)  Time: 3.436s, 2383.88/s  (1.253s, 6537.72/s)  LR: 9.222e-03  Data: 0.000 (0.191)
2025-05-31 12:16:48,545 - train - INFO - Train: 18 [ 150/156 ( 97%)]  Loss:  3.075702 (3.0985)  Time: 0.526s, 15566.53/s  (1.225s, 6685.80/s)  LR: 9.222e-03  Data: 0.000 (0.128)
2025-05-31 12:16:53,534 - train - INFO - Train: 18 [ 155/156 (100%)]  Loss:  3.065009 (3.0918)  Time: 0.524s, 15638.47/s  (1.218s, 6725.88/s)  LR: 9.222e-03  Data: 0.000 (0.124)
2025-05-31 12:17:00,298 - train - INFO - Test: [   0/48]  Time: 6.491 (6.491)  Loss:  1.1123 (1.1123)  Acc@1: 75.9766 (75.9766)  Acc@5: 92.1875 (92.1875)
2025-05-31 12:17:58,580 - train - INFO - Test: [  48/48]  Time: 1.946 (1.322)  Loss:  1.1650 (1.7595)  Acc@1: 76.0613 (60.8080)  Acc@5: 90.3302 (83.4140)
2025-05-31 12:18:05,246 - train - INFO - Train: 19 [   0/156 (  0%)]  Loss:  3.062732 (3.0627)  Time: 6.296s, 1301.10/s  (6.296s, 1301.10/s)  LR: 9.136e-03  Data: 5.681 (5.681)
2025-05-31 12:19:05,028 - train - INFO - Train: 19 [  50/156 ( 32%)]  Loss:  3.109785 (3.0863)  Time: 0.529s, 15489.95/s  (1.296s, 6322.76/s)  LR: 9.136e-03  Data: 0.001 (0.240)
2025-05-31 12:20:04,936 - train - INFO - Train: 19 [ 100/156 ( 65%)]  Loss:  3.126108 (3.0995)  Time: 1.081s, 7579.72/s  (1.247s, 6567.40/s)  LR: 9.136e-03  Data: 0.000 (0.121)
2025-05-31 12:21:06,124 - train - INFO - Train: 19 [ 150/156 ( 97%)]  Loss:  3.126239 (3.1062)  Time: 0.526s, 15586.54/s  (1.240s, 6608.91/s)  LR: 9.136e-03  Data: 0.000 (0.081)
2025-05-31 12:21:10,908 - train - INFO - Train: 19 [ 155/156 (100%)]  Loss:  3.173929 (3.1198)  Time: 0.523s, 15658.88/s  (1.230s, 6657.62/s)  LR: 9.136e-03  Data: 0.000 (0.079)
2025-05-31 12:21:17,562 - train - INFO - Test: [   0/48]  Time: 6.391 (6.391)  Loss:  1.2158 (1.2158)  Acc@1: 76.4648 (76.4648)  Acc@5: 89.8438 (89.8438)
2025-05-31 12:22:15,798 - train - INFO - Test: [  48/48]  Time: 2.741 (1.319)  Loss:  1.2773 (1.8546)  Acc@1: 73.8207 (59.2780)  Acc@5: 89.5047 (82.2280)
2025-05-31 12:22:22,543 - train - INFO - Train: 20 [   0/156 (  0%)]  Loss:  3.186389 (3.1864)  Time: 6.363s, 1287.45/s  (6.363s, 1287.45/s)  LR: 9.046e-03  Data: 5.678 (5.678)
2025-05-31 12:23:21,226 - train - INFO - Train: 20 [  50/156 ( 32%)]  Loss:  3.063288 (3.1248)  Time: 0.529s, 15486.39/s  (1.275s, 6423.23/s)  LR: 9.046e-03  Data: 0.000 (0.595)
2025-05-31 12:24:20,696 - train - INFO - Train: 20 [ 100/156 ( 65%)]  Loss:  3.093436 (3.1144)  Time: 2.536s, 3230.87/s  (1.233s, 6645.05/s)  LR: 9.046e-03  Data: 0.000 (0.449)
2025-05-31 12:25:18,880 - train - INFO - Train: 20 [ 150/156 ( 97%)]  Loss:  3.111965 (3.1138)  Time: 0.533s, 15380.95/s  (1.210s, 6770.76/s)  LR: 9.046e-03  Data: 0.000 (0.300)
2025-05-31 12:25:23,689 - train - INFO - Train: 20 [ 155/156 (100%)]  Loss:  3.082118 (3.1074)  Time: 0.524s, 15642.44/s  (1.202s, 6815.63/s)  LR: 9.046e-03  Data: 0.000 (0.291)
2025-05-31 12:25:30,408 - train - INFO - Test: [   0/48]  Time: 6.414 (6.414)  Loss:  1.1250 (1.1250)  Acc@1: 77.5391 (77.5391)  Acc@5: 92.6758 (92.6758)
2025-05-31 12:26:28,154 - train - INFO - Test: [  48/48]  Time: 2.913 (1.309)  Loss:  1.1982 (1.7814)  Acc@1: 76.1793 (61.0700)  Acc@5: 90.2123 (83.4660)
2025-05-31 12:26:34,756 - train - INFO - Train: 21 [   0/156 (  0%)]  Loss:  3.091164 (3.0912)  Time: 6.231s, 1314.62/s  (6.231s, 1314.62/s)  LR: 8.952e-03  Data: 5.079 (5.079)
2025-05-31 12:27:34,463 - train - INFO - Train: 21 [  50/156 ( 32%)]  Loss:  3.084975 (3.0881)  Time: 0.528s, 15511.04/s  (1.293s, 6336.20/s)  LR: 8.952e-03  Data: 0.000 (0.503)
2025-05-31 12:28:35,151 - train - INFO - Train: 21 [ 100/156 ( 65%)]  Loss:  3.108571 (3.0949)  Time: 3.332s, 2458.89/s  (1.254s, 6534.27/s)  LR: 8.952e-03  Data: 0.756 (0.420)
2025-05-31 12:29:32,975 - train - INFO - Train: 21 [ 150/156 ( 97%)]  Loss:  3.091967 (3.0942)  Time: 0.528s, 15526.01/s  (1.221s, 6706.54/s)  LR: 8.952e-03  Data: 0.000 (0.342)
2025-05-31 12:29:38,130 - train - INFO - Train: 21 [ 155/156 (100%)]  Loss:  3.103726 (3.0961)  Time: 0.525s, 15614.73/s  (1.215s, 6740.26/s)  LR: 8.952e-03  Data: 0.000 (0.334)
2025-05-31 12:29:44,473 - train - INFO - Test: [   0/48]  Time: 6.057 (6.057)  Loss:  1.2031 (1.2031)  Acc@1: 75.0977 (75.0977)  Acc@5: 90.7227 (90.7227)
2025-05-31 12:30:42,271 - train - INFO - Test: [  48/48]  Time: 2.166 (1.303)  Loss:  1.0439 (1.7406)  Acc@1: 77.9481 (61.4120)  Acc@5: 92.4528 (83.8920)
2025-05-31 12:30:49,077 - train - INFO - Train: 22 [   0/156 (  0%)]  Loss:  3.089694 (3.0897)  Time: 6.419s, 1276.16/s  (6.419s, 1276.16/s)  LR: 8.854e-03  Data: 5.119 (5.119)
2025-05-31 12:31:47,527 - train - INFO - Train: 22 [  50/156 ( 32%)]  Loss:  3.078144 (3.0839)  Time: 0.528s, 15524.91/s  (1.272s, 6440.61/s)  LR: 8.854e-03  Data: 0.000 (0.456)
2025-05-31 12:32:48,948 - train - INFO - Train: 22 [ 100/156 ( 65%)]  Loss:  3.088257 (3.0854)  Time: 3.224s, 2541.01/s  (1.250s, 6551.61/s)  LR: 8.854e-03  Data: 0.000 (0.270)
2025-05-31 12:33:47,383 - train - INFO - Train: 22 [ 150/156 ( 97%)]  Loss:  3.070695 (3.0817)  Time: 0.527s, 15542.19/s  (1.223s, 6696.48/s)  LR: 8.854e-03  Data: 0.000 (0.181)
2025-05-31 12:33:52,416 - train - INFO - Train: 22 [ 155/156 (100%)]  Loss:  3.070885 (3.0795)  Time: 0.524s, 15631.54/s  (1.216s, 6734.78/s)  LR: 8.854e-03  Data: 0.000 (0.175)
2025-05-31 12:33:58,968 - train - INFO - Test: [   0/48]  Time: 6.250 (6.250)  Loss:  1.0928 (1.0928)  Acc@1: 76.8555 (76.8555)  Acc@5: 93.2617 (93.2617)
2025-05-31 12:34:57,249 - train - INFO - Test: [  48/48]  Time: 1.396 (1.317)  Loss:  1.2061 (1.7561)  Acc@1: 75.8255 (61.7420)  Acc@5: 91.6274 (84.1360)
2025-05-31 12:35:04,172 - train - INFO - Train: 23 [   0/156 (  0%)]  Loss:  3.059628 (3.0596)  Time: 6.381s, 1283.85/s  (6.381s, 1283.85/s)  LR: 8.752e-03  Data: 5.634 (5.634)
2025-05-31 12:36:03,258 - train - INFO - Train: 23 [  50/156 ( 32%)]  Loss:  3.101143 (3.0804)  Time: 0.525s, 15592.19/s  (1.284s, 6381.91/s)  LR: 8.752e-03  Data: 0.000 (0.457)
2025-05-31 12:37:03,933 - train - INFO - Train: 23 [ 100/156 ( 65%)]  Loss:  3.089677 (3.0835)  Time: 3.220s, 2543.90/s  (1.249s, 6559.45/s)  LR: 8.752e-03  Data: 0.000 (0.249)
2025-05-31 12:38:02,066 - train - INFO - Train: 23 [ 150/156 ( 97%)]  Loss:  3.079914 (3.0826)  Time: 0.527s, 15541.65/s  (1.220s, 6713.02/s)  LR: 8.752e-03  Data: 0.000 (0.167)
2025-05-31 12:38:06,900 - train - INFO - Train: 23 [ 155/156 (100%)]  Loss:  3.099155 (3.0859)  Time: 0.522s, 15706.97/s  (1.212s, 6758.04/s)  LR: 8.752e-03  Data: 0.000 (0.162)
2025-05-31 12:38:13,690 - train - INFO - Test: [   0/48]  Time: 6.499 (6.499)  Loss:  1.2236 (1.2236)  Acc@1: 76.0742 (76.0742)  Acc@5: 91.3086 (91.3086)
2025-05-31 12:39:11,359 - train - INFO - Test: [  48/48]  Time: 2.314 (1.310)  Loss:  1.1943 (1.8086)  Acc@1: 76.7689 (61.0760)  Acc@5: 91.6274 (83.4040)
2025-05-31 12:39:18,160 - train - INFO - Train: 24 [   0/156 (  0%)]  Loss:  3.068486 (3.0685)  Time: 6.421s, 1275.81/s  (6.421s, 1275.81/s)  LR: 8.646e-03  Data: 5.803 (5.803)
2025-05-31 12:40:16,324 - train - INFO - Train: 24 [  50/156 ( 32%)]  Loss:  3.108375 (3.0884)  Time: 0.527s, 15532.54/s  (1.266s, 6468.98/s)  LR: 8.646e-03  Data: 0.000 (0.399)
2025-05-31 12:41:19,011 - train - INFO - Train: 24 [ 100/156 ( 65%)]  Loss:  3.096740 (3.0912)  Time: 3.528s, 2321.68/s  (1.260s, 6501.17/s)  LR: 8.646e-03  Data: 0.000 (0.201)
2025-05-31 12:42:18,478 - train - INFO - Train: 24 [ 150/156 ( 97%)]  Loss:  3.072303 (3.0865)  Time: 0.533s, 15378.93/s  (1.237s, 6624.37/s)  LR: 8.646e-03  Data: 0.000 (0.135)
2025-05-31 12:42:23,315 - train - INFO - Train: 24 [ 155/156 (100%)]  Loss:  3.087714 (3.0867)  Time: 0.531s, 15415.95/s  (1.228s, 6670.95/s)  LR: 8.646e-03  Data: 0.000 (0.131)
2025-05-31 12:42:30,099 - train - INFO - Test: [   0/48]  Time: 6.503 (6.503)  Loss:  1.1992 (1.1992)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.7969 (91.7969)
2025-05-31 12:43:28,606 - train - INFO - Test: [  48/48]  Time: 2.028 (1.327)  Loss:  1.0273 (1.7511)  Acc@1: 80.1887 (61.4220)  Acc@5: 93.5142 (83.8380)
2025-05-31 12:43:35,238 - train - INFO - Train: 25 [   0/156 (  0%)]  Loss:  3.025249 (3.0252)  Time: 6.252s, 1310.27/s  (6.252s, 1310.27/s)  LR: 8.537e-03  Data: 5.539 (5.539)
2025-05-31 12:44:34,134 - train - INFO - Train: 25 [  50/156 ( 32%)]  Loss:  3.076508 (3.0509)  Time: 0.527s, 15536.23/s  (1.277s, 6413.15/s)  LR: 8.537e-03  Data: 0.000 (0.498)
2025-05-31 12:45:35,344 - train - INFO - Train: 25 [ 100/156 ( 65%)]  Loss:  3.082802 (3.0615)  Time: 3.255s, 2516.38/s  (1.251s, 6548.22/s)  LR: 8.537e-03  Data: 0.000 (0.251)
2025-05-31 12:46:33,027 - train - INFO - Train: 25 [ 150/156 ( 97%)]  Loss:  3.072733 (3.0643)  Time: 0.526s, 15571.17/s  (1.219s, 6721.49/s)  LR: 8.537e-03  Data: 0.000 (0.168)
2025-05-31 12:46:37,870 - train - INFO - Train: 25 [ 155/156 (100%)]  Loss:  3.037026 (3.0589)  Time: 0.521s, 15734.08/s  (1.211s, 6766.15/s)  LR: 8.537e-03  Data: 0.000 (0.163)
2025-05-31 12:46:44,729 - train - INFO - Test: [   0/48]  Time: 6.550 (6.550)  Loss:  1.0947 (1.0947)  Acc@1: 75.6836 (75.6836)  Acc@5: 92.9688 (92.9688)
2025-05-31 12:47:42,862 - train - INFO - Test: [  48/48]  Time: 2.646 (1.320)  Loss:  1.0967 (1.7001)  Acc@1: 78.0660 (62.1660)  Acc@5: 92.0991 (84.5080)
2025-05-31 12:47:50,540 - train - INFO - Train: 26 [   0/156 (  0%)]  Loss:  3.061708 (3.0617)  Time: 7.126s, 1149.52/s  (7.126s, 1149.52/s)  LR: 8.424e-03  Data: 5.783 (5.783)
2025-05-31 12:48:49,951 - train - INFO - Train: 26 [  50/156 ( 32%)]  Loss:  3.036740 (3.0492)  Time: 0.537s, 15244.23/s  (1.305s, 6279.15/s)  LR: 8.424e-03  Data: 0.000 (0.369)
2025-05-31 12:49:50,407 - train - INFO - Train: 26 [ 100/156 ( 65%)]  Loss:  3.058913 (3.0525)  Time: 3.165s, 2588.45/s  (1.257s, 6515.38/s)  LR: 8.424e-03  Data: 1.527 (0.343)
2025-05-31 12:50:48,425 - train - INFO - Train: 26 [ 150/156 ( 97%)]  Loss:  3.070064 (3.0569)  Time: 0.527s, 15549.49/s  (1.225s, 6686.26/s)  LR: 8.424e-03  Data: 0.000 (0.337)
2025-05-31 12:50:53,329 - train - INFO - Train: 26 [ 155/156 (100%)]  Loss:  3.059511 (3.0574)  Time: 0.523s, 15675.16/s  (1.217s, 6729.30/s)  LR: 8.424e-03  Data: 0.000 (0.331)
2025-05-31 12:51:00,008 - train - INFO - Test: [   0/48]  Time: 6.375 (6.375)  Loss:  1.0850 (1.0850)  Acc@1: 76.5625 (76.5625)  Acc@5: 91.7969 (91.7969)
2025-05-31 12:51:57,760 - train - INFO - Test: [  48/48]  Time: 1.944 (1.309)  Loss:  1.0156 (1.7155)  Acc@1: 78.4198 (61.9820)  Acc@5: 92.4528 (83.9900)
2025-05-31 12:52:04,891 - train - INFO - Train: 27 [   0/156 (  0%)]  Loss:  3.074226 (3.0742)  Time: 6.754s, 1212.98/s  (6.754s, 1212.98/s)  LR: 8.308e-03  Data: 5.616 (5.616)
2025-05-31 12:53:04,731 - train - INFO - Train: 27 [  50/156 ( 32%)]  Loss:  3.064712 (3.0695)  Time: 0.528s, 15515.65/s  (1.306s, 6273.80/s)  LR: 8.308e-03  Data: 0.000 (0.154)
2025-05-31 12:54:06,379 - train - INFO - Train: 27 [ 100/156 ( 65%)]  Loss:  3.039467 (3.0595)  Time: 3.246s, 2523.35/s  (1.270s, 6451.96/s)  LR: 8.308e-03  Data: 0.000 (0.078)
2025-05-31 12:55:05,078 - train - INFO - Train: 27 [ 150/156 ( 97%)]  Loss:  3.049737 (3.0570)  Time: 0.527s, 15544.44/s  (1.238s, 6617.15/s)  LR: 8.308e-03  Data: 0.000 (0.052)
2025-05-31 12:55:10,387 - train - INFO - Train: 27 [ 155/156 (100%)]  Loss:  3.062020 (3.0580)  Time: 0.524s, 15636.19/s  (1.232s, 6647.51/s)  LR: 8.308e-03  Data: 0.000 (0.051)
2025-05-31 12:55:17,482 - train - INFO - Test: [   0/48]  Time: 6.791 (6.791)  Loss:  1.1553 (1.1553)  Acc@1: 75.6836 (75.6836)  Acc@5: 91.6992 (91.6992)
2025-05-31 12:56:15,914 - train - INFO - Test: [  48/48]  Time: 2.699 (1.331)  Loss:  1.2070 (1.7564)  Acc@1: 76.0613 (61.1680)  Acc@5: 91.5094 (83.6280)
2025-05-31 12:56:22,406 - train - INFO - Train: 28 [   0/156 (  0%)]  Loss:  3.057495 (3.0575)  Time: 6.100s, 1342.98/s  (6.100s, 1342.98/s)  LR: 8.189e-03  Data: 5.317 (5.317)
2025-05-31 12:57:20,686 - train - INFO - Train: 28 [  50/156 ( 32%)]  Loss:  3.040477 (3.0490)  Time: 0.546s, 14998.55/s  (1.262s, 6489.55/s)  LR: 8.189e-03  Data: 0.000 (0.313)
2025-05-31 12:58:20,753 - train - INFO - Train: 28 [ 100/156 ( 65%)]  Loss:  3.068637 (3.0555)  Time: 2.618s, 3129.01/s  (1.232s, 6648.64/s)  LR: 8.189e-03  Data: 0.000 (0.158)
2025-05-31 12:59:18,146 - train - INFO - Train: 28 [ 150/156 ( 97%)]  Loss:  3.081050 (3.0619)  Time: 0.578s, 14169.96/s  (1.204s, 6802.74/s)  LR: 8.189e-03  Data: 0.000 (0.106)
2025-05-31 12:59:23,186 - train - INFO - Train: 28 [ 155/156 (100%)]  Loss:  3.086054 (3.0667)  Time: 0.524s, 15646.48/s  (1.198s, 6838.47/s)  LR: 8.189e-03  Data: 0.000 (0.102)
2025-05-31 12:59:29,841 - train - INFO - Test: [   0/48]  Time: 6.371 (6.371)  Loss:  0.9570 (0.9570)  Acc@1: 80.3711 (80.3711)  Acc@5: 93.7500 (93.7500)
2025-05-31 13:00:28,377 - train - INFO - Test: [  48/48]  Time: 2.267 (1.325)  Loss:  1.0537 (1.7019)  Acc@1: 78.0660 (62.5800)  Acc@5: 91.8632 (84.3340)
2025-05-31 13:00:34,939 - train - INFO - Train: 29 [   0/156 (  0%)]  Loss:  3.013058 (3.0131)  Time: 6.020s, 1360.77/s  (6.020s, 1360.77/s)  LR: 8.066e-03  Data: 5.214 (5.214)
2025-05-31 13:01:33,717 - train - INFO - Train: 29 [  50/156 ( 32%)]  Loss:  3.027742 (3.0204)  Time: 0.527s, 15546.69/s  (1.271s, 6447.72/s)  LR: 8.066e-03  Data: 0.000 (0.419)
2025-05-31 13:02:34,742 - train - INFO - Train: 29 [ 100/156 ( 65%)]  Loss:  3.026356 (3.0224)  Time: 3.216s, 2547.07/s  (1.246s, 6575.94/s)  LR: 8.066e-03  Data: 0.000 (0.212)
2025-05-31 13:03:32,720 - train - INFO - Train: 29 [ 150/156 ( 97%)]  Loss:  3.031139 (3.0246)  Time: 0.525s, 15608.88/s  (1.217s, 6730.20/s)  LR: 8.066e-03  Data: 0.000 (0.142)
2025-05-31 13:03:37,769 - train - INFO - Train: 29 [ 155/156 (100%)]  Loss:  3.078236 (3.0353)  Time: 0.524s, 15647.59/s  (1.211s, 6767.22/s)  LR: 8.066e-03  Data: 0.000 (0.137)
2025-05-31 13:03:44,304 - train - INFO - Test: [   0/48]  Time: 6.224 (6.224)  Loss:  1.0898 (1.0898)  Acc@1: 77.2461 (77.2461)  Acc@5: 92.2852 (92.2852)
2025-05-31 13:04:42,384 - train - INFO - Test: [  48/48]  Time: 2.417 (1.312)  Loss:  1.0713 (1.6985)  Acc@1: 77.5943 (62.3760)  Acc@5: 91.9811 (84.5620)
2025-05-31 13:04:49,108 - train - INFO - Train: 30 [   0/156 (  0%)]  Loss:  3.035753 (3.0358)  Time: 6.341s, 1292.01/s  (6.341s, 1292.01/s)  LR: 7.941e-03  Data: 5.820 (5.820)
2025-05-31 13:05:47,597 - train - INFO - Train: 30 [  50/156 ( 32%)]  Loss:  3.062341 (3.0490)  Time: 0.534s, 15351.79/s  (1.271s, 6444.49/s)  LR: 7.941e-03  Data: 0.000 (0.720)
2025-05-31 13:06:48,925 - train - INFO - Train: 30 [ 100/156 ( 65%)]  Loss:  3.065269 (3.0545)  Time: 3.245s, 2524.85/s  (1.249s, 6558.53/s)  LR: 7.941e-03  Data: 0.949 (0.573)
2025-05-31 13:07:47,552 - train - INFO - Train: 30 [ 150/156 ( 97%)]  Loss:  3.030126 (3.0484)  Time: 0.531s, 15431.23/s  (1.224s, 6694.35/s)  LR: 7.941e-03  Data: 0.000 (0.417)
2025-05-31 13:07:52,390 - train - INFO - Train: 30 [ 155/156 (100%)]  Loss:  3.054394 (3.0496)  Time: 0.521s, 15718.13/s  (1.215s, 6739.62/s)  LR: 7.941e-03  Data: 0.000 (0.404)
2025-05-31 13:07:58,917 - train - INFO - Test: [   0/48]  Time: 6.217 (6.217)  Loss:  1.1475 (1.1475)  Acc@1: 76.2695 (76.2695)  Acc@5: 92.2852 (92.2852)
2025-05-31 13:08:56,828 - train - INFO - Test: [  48/48]  Time: 2.037 (1.309)  Loss:  1.1182 (1.7210)  Acc@1: 77.0047 (61.8260)  Acc@5: 91.8632 (84.4640)
2025-05-31 13:09:03,498 - train - INFO - Train: 31 [   0/156 (  0%)]  Loss:  3.045608 (3.0456)  Time: 6.285s, 1303.39/s  (6.285s, 1303.39/s)  LR: 7.813e-03  Data: 5.127 (5.127)
2025-05-31 13:10:01,997 - train - INFO - Train: 31 [  50/156 ( 32%)]  Loss:  3.047099 (3.0464)  Time: 0.532s, 15385.54/s  (1.270s, 6449.14/s)  LR: 7.813e-03  Data: 0.000 (0.566)
2025-05-31 13:11:02,674 - train - INFO - Train: 31 [ 100/156 ( 65%)]  Loss:  2.985720 (3.0261)  Time: 2.997s, 2733.15/s  (1.242s, 6594.99/s)  LR: 7.813e-03  Data: 0.000 (0.317)
2025-05-31 13:12:00,890 - train - INFO - Train: 31 [ 150/156 ( 97%)]  Loss:  3.028141 (3.0266)  Time: 0.527s, 15530.87/s  (1.216s, 6734.79/s)  LR: 7.813e-03  Data: 0.000 (0.213)
2025-05-31 13:12:05,801 - train - INFO - Train: 31 [ 155/156 (100%)]  Loss:  3.040385 (3.0294)  Time: 0.524s, 15647.68/s  (1.209s, 6776.63/s)  LR: 7.813e-03  Data: 0.000 (0.206)
2025-05-31 13:12:12,523 - train - INFO - Test: [   0/48]  Time: 6.402 (6.402)  Loss:  1.0225 (1.0225)  Acc@1: 78.4180 (78.4180)  Acc@5: 92.5781 (92.5781)
2025-05-31 13:13:10,196 - train - INFO - Test: [  48/48]  Time: 2.342 (1.308)  Loss:  0.9756 (1.7159)  Acc@1: 78.4198 (61.0420)  Acc@5: 93.5142 (83.9540)
2025-05-31 13:13:17,256 - train - INFO - Train: 32 [   0/156 (  0%)]  Loss:  3.034113 (3.0341)  Time: 6.692s, 1224.13/s  (6.692s, 1224.13/s)  LR: 7.681e-03  Data: 6.172 (6.172)
2025-05-31 13:14:15,355 - train - INFO - Train: 32 [  50/156 ( 32%)]  Loss:  3.010309 (3.0222)  Time: 0.528s, 15519.67/s  (1.270s, 6448.39/s)  LR: 7.681e-03  Data: 0.000 (0.581)
2025-05-31 13:15:16,423 - train - INFO - Train: 32 [ 100/156 ( 65%)]  Loss:  3.020104 (3.0215)  Time: 2.842s, 2882.37/s  (1.246s, 6574.08/s)  LR: 7.681e-03  Data: 0.000 (0.308)
2025-05-31 13:16:15,364 - train - INFO - Train: 32 [ 150/156 ( 97%)]  Loss:  3.040863 (3.0263)  Time: 0.528s, 15522.94/s  (1.224s, 6693.84/s)  LR: 7.681e-03  Data: 0.000 (0.206)
2025-05-31 13:16:20,399 - train - INFO - Train: 32 [ 155/156 (100%)]  Loss:  3.025832 (3.0262)  Time: 0.523s, 15676.62/s  (1.217s, 6732.16/s)  LR: 7.681e-03  Data: 0.000 (0.199)
2025-05-31 13:16:27,048 - train - INFO - Test: [   0/48]  Time: 6.332 (6.332)  Loss:  1.0908 (1.0908)  Acc@1: 76.6602 (76.6602)  Acc@5: 92.3828 (92.3828)
2025-05-31 13:17:25,424 - train - INFO - Test: [  48/48]  Time: 2.700 (1.321)  Loss:  1.0303 (1.7125)  Acc@1: 78.0660 (62.1520)  Acc@5: 92.0991 (84.4080)
2025-05-31 13:17:32,126 - train - INFO - Train: 33 [   0/156 (  0%)]  Loss:  3.034981 (3.0350)  Time: 6.323s, 1295.61/s  (6.323s, 1295.61/s)  LR: 7.548e-03  Data: 5.507 (5.507)
2025-05-31 13:18:30,715 - train - INFO - Train: 33 [  50/156 ( 32%)]  Loss:  3.009056 (3.0220)  Time: 0.527s, 15544.57/s  (1.273s, 6436.40/s)  LR: 7.548e-03  Data: 0.000 (0.289)
2025-05-31 13:19:32,108 - train - INFO - Train: 33 [ 100/156 ( 65%)]  Loss:  3.031702 (3.0252)  Time: 3.312s, 2473.66/s  (1.251s, 6550.89/s)  LR: 7.548e-03  Data: 0.000 (0.146)
2025-05-31 13:20:30,330 - train - INFO - Train: 33 [ 150/156 ( 97%)]  Loss:  3.037132 (3.0282)  Time: 0.527s, 15533.89/s  (1.222s, 6703.73/s)  LR: 7.548e-03  Data: 0.000 (0.098)
2025-05-31 13:20:35,263 - train - INFO - Train: 33 [ 155/156 (100%)]  Loss:  3.042444 (3.0311)  Time: 0.523s, 15671.72/s  (1.214s, 6745.42/s)  LR: 7.548e-03  Data: 0.000 (0.095)
2025-05-31 13:20:41,837 - train - INFO - Test: [   0/48]  Time: 6.298 (6.298)  Loss:  0.9224 (0.9224)  Acc@1: 79.6875 (79.6875)  Acc@5: 93.4570 (93.4570)
2025-05-31 13:21:40,427 - train - INFO - Test: [  48/48]  Time: 2.958 (1.324)  Loss:  1.0537 (1.7275)  Acc@1: 77.0047 (61.3800)  Acc@5: 92.2170 (83.7140)
2025-05-31 13:21:46,989 - train - INFO - Train: 34 [   0/156 (  0%)]  Loss:  3.024368 (3.0244)  Time: 6.180s, 1325.57/s  (6.180s, 1325.57/s)  LR: 7.411e-03  Data: 5.031 (5.031)
2025-05-31 13:22:46,021 - train - INFO - Train: 34 [  50/156 ( 32%)]  Loss:  3.031413 (3.0279)  Time: 0.527s, 15546.31/s  (1.279s, 6406.82/s)  LR: 7.411e-03  Data: 0.000 (0.285)
2025-05-31 13:23:47,373 - train - INFO - Train: 34 [ 100/156 ( 65%)]  Loss:  3.052379 (3.0361)  Time: 3.088s, 2653.16/s  (1.253s, 6537.48/s)  LR: 7.411e-03  Data: 0.000 (0.146)
2025-05-31 13:24:46,029 - train - INFO - Train: 34 [ 150/156 ( 97%)]  Loss:  3.040904 (3.0373)  Time: 0.527s, 15557.13/s  (1.227s, 6678.66/s)  LR: 7.411e-03  Data: 0.000 (0.098)
2025-05-31 13:24:51,058 - train - INFO - Train: 34 [ 155/156 (100%)]  Loss:  3.027577 (3.0353)  Time: 0.526s, 15571.99/s  (1.220s, 6717.47/s)  LR: 7.411e-03  Data: 0.000 (0.095)
2025-05-31 13:24:57,913 - train - INFO - Test: [   0/48]  Time: 6.564 (6.564)  Loss:  1.1797 (1.1797)  Acc@1: 75.6836 (75.6836)  Acc@5: 92.3828 (92.3828)
2025-05-31 13:25:56,260 - train - INFO - Test: [  48/48]  Time: 1.480 (1.325)  Loss:  1.1885 (1.8035)  Acc@1: 76.8868 (61.4680)  Acc@5: 93.0425 (84.0920)
2025-05-31 13:26:03,191 - train - INFO - Train: 35 [   0/156 (  0%)]  Loss:  3.016701 (3.0167)  Time: 6.556s, 1249.49/s  (6.556s, 1249.49/s)  LR: 7.273e-03  Data: 6.029 (6.029)
2025-05-31 13:27:02,528 - train - INFO - Train: 35 [  50/156 ( 32%)]  Loss:  3.038394 (3.0275)  Time: 0.529s, 15479.44/s  (1.292s, 6340.52/s)  LR: 7.273e-03  Data: 0.000 (0.284)
2025-05-31 13:28:02,681 - train - INFO - Train: 35 [ 100/156 ( 65%)]  Loss:  3.018407 (3.0245)  Time: 2.788s, 2938.16/s  (1.248s, 6564.33/s)  LR: 7.273e-03  Data: 0.000 (0.143)
2025-05-31 13:29:02,226 - train - INFO - Train: 35 [ 150/156 ( 97%)]  Loss:  3.064865 (3.0346)  Time: 0.528s, 15525.54/s  (1.229s, 6665.27/s)  LR: 7.273e-03  Data: 0.000 (0.096)
2025-05-31 13:29:07,322 - train - INFO - Train: 35 [ 155/156 (100%)]  Loss:  3.017906 (3.0313)  Time: 0.525s, 15609.57/s  (1.222s, 6701.98/s)  LR: 7.273e-03  Data: 0.000 (0.093)
2025-05-31 13:29:14,159 - train - INFO - Test: [   0/48]  Time: 6.537 (6.537)  Loss:  1.0820 (1.0820)  Acc@1: 77.9297 (77.9297)  Acc@5: 92.6758 (92.6758)
2025-05-31 13:30:14,274 - train - INFO - Test: [  48/48]  Time: 2.925 (1.360)  Loss:  1.1240 (1.7401)  Acc@1: 77.5943 (62.1740)  Acc@5: 91.6274 (84.3360)
2025-05-31 13:30:21,269 - train - INFO - Train: 36 [   0/156 (  0%)]  Loss:  3.012464 (3.0125)  Time: 6.629s, 1235.78/s  (6.629s, 1235.78/s)  LR: 7.132e-03  Data: 5.267 (5.267)
2025-05-31 13:31:20,359 - train - INFO - Train: 36 [  50/156 ( 32%)]  Loss:  3.015289 (3.0139)  Time: 0.528s, 15511.05/s  (1.289s, 6357.43/s)  LR: 7.132e-03  Data: 0.000 (0.244)
2025-05-31 13:32:21,770 - train - INFO - Train: 36 [ 100/156 ( 65%)]  Loss:  3.013326 (3.0137)  Time: 3.121s, 2625.00/s  (1.259s, 6508.39/s)  LR: 7.132e-03  Data: 0.000 (0.123)
2025-05-31 13:33:20,557 - train - INFO - Train: 36 [ 150/156 ( 97%)]  Loss:  3.011344 (3.0131)  Time: 0.525s, 15599.76/s  (1.231s, 6653.65/s)  LR: 7.132e-03  Data: 0.000 (0.082)
2025-05-31 13:33:25,464 - train - INFO - Train: 36 [ 155/156 (100%)]  Loss:  3.011783 (3.0128)  Time: 0.522s, 15692.32/s  (1.223s, 6697.23/s)  LR: 7.132e-03  Data: 0.000 (0.080)
2025-05-31 13:33:32,486 - train - INFO - Test: [   0/48]  Time: 6.733 (6.733)  Loss:  1.0244 (1.0244)  Acc@1: 79.1016 (79.1016)  Acc@5: 92.7734 (92.7734)
2025-05-31 13:34:29,822 - train - INFO - Test: [  48/48]  Time: 2.203 (1.308)  Loss:  1.2344 (1.7717)  Acc@1: 75.9434 (62.2520)  Acc@5: 90.9198 (84.3560)
2025-05-31 13:34:36,345 - train - INFO - Train: 37 [   0/156 (  0%)]  Loss:  2.990183 (2.9902)  Time: 6.139s, 1334.44/s  (6.139s, 1334.44/s)  LR: 6.989e-03  Data: 5.606 (5.606)
2025-05-31 13:35:35,181 - train - INFO - Train: 37 [  50/156 ( 32%)]  Loss:  3.034739 (3.0125)  Time: 0.547s, 14983.03/s  (1.274s, 6430.23/s)  LR: 6.989e-03  Data: 0.000 (0.330)
2025-05-31 13:36:36,796 - train - INFO - Train: 37 [ 100/156 ( 65%)]  Loss:  3.005470 (3.0101)  Time: 3.206s, 2554.94/s  (1.253s, 6536.13/s)  LR: 6.989e-03  Data: 0.000 (0.167)
2025-05-31 13:37:36,069 - train - INFO - Train: 37 [ 150/156 ( 97%)]  Loss:  3.025206 (3.0139)  Time: 1.076s, 7611.63/s  (1.231s, 6655.51/s)  LR: 6.989e-03  Data: 0.000 (0.112)
2025-05-31 13:37:41,568 - train - INFO - Train: 37 [ 155/156 (100%)]  Loss:  3.024034 (3.0159)  Time: 0.524s, 15643.41/s  (1.227s, 6678.33/s)  LR: 6.989e-03  Data: 0.000 (0.108)
2025-05-31 13:37:48,414 - train - INFO - Test: [   0/48]  Time: 6.547 (6.547)  Loss:  1.0811 (1.0811)  Acc@1: 76.6602 (76.6602)  Acc@5: 92.0898 (92.0898)
2025-05-31 13:38:47,505 - train - INFO - Test: [  48/48]  Time: 2.330 (1.340)  Loss:  1.2617 (1.7709)  Acc@1: 74.1745 (60.4320)  Acc@5: 90.2123 (83.0740)
2025-05-31 13:38:54,387 - train - INFO - Train: 38 [   0/156 (  0%)]  Loss:  3.008621 (3.0086)  Time: 6.509s, 1258.54/s  (6.509s, 1258.54/s)  LR: 6.844e-03  Data: 5.078 (5.078)
2025-05-31 13:39:53,470 - train - INFO - Train: 38 [  50/156 ( 32%)]  Loss:  3.029195 (3.0189)  Time: 0.538s, 15219.69/s  (1.286s, 6369.70/s)  LR: 6.844e-03  Data: 0.000 (0.146)
2025-05-31 13:40:54,344 - train - INFO - Train: 38 [ 100/156 ( 65%)]  Loss:  3.019328 (3.0190)  Time: 3.112s, 2632.48/s  (1.252s, 6542.54/s)  LR: 6.844e-03  Data: 0.000 (0.074)
2025-05-31 13:41:53,730 - train - INFO - Train: 38 [ 150/156 ( 97%)]  Loss:  3.025266 (3.0206)  Time: 1.099s, 7453.21/s  (1.231s, 6655.91/s)  LR: 6.844e-03  Data: 0.000 (0.049)
2025-05-31 13:41:58,507 - train - INFO - Train: 38 [ 155/156 (100%)]  Loss:  3.004444 (3.0174)  Time: 0.522s, 15680.68/s  (1.222s, 6704.06/s)  LR: 6.844e-03  Data: 0.000 (0.048)
2025-05-31 13:42:05,689 - train - INFO - Test: [   0/48]  Time: 6.895 (6.895)  Loss:  1.0898 (1.0898)  Acc@1: 76.7578 (76.7578)  Acc@5: 92.6758 (92.6758)
2025-05-31 13:43:04,493 - train - INFO - Test: [  48/48]  Time: 2.770 (1.341)  Loss:  1.1406 (1.7076)  Acc@1: 76.5330 (62.2160)  Acc@5: 90.9198 (84.1900)
2025-05-31 13:43:11,411 - train - INFO - Train: 39 [   0/156 (  0%)]  Loss:  3.014343 (3.0143)  Time: 6.537s, 1253.22/s  (6.537s, 1253.22/s)  LR: 6.697e-03  Data: 6.022 (6.022)
2025-05-31 13:44:10,095 - train - INFO - Train: 39 [  50/156 ( 32%)]  Loss:  2.971971 (2.9932)  Time: 0.527s, 15537.05/s  (1.279s, 6406.08/s)  LR: 6.697e-03  Data: 0.000 (0.420)
2025-05-31 13:45:11,639 - train - INFO - Train: 39 [ 100/156 ( 65%)]  Loss:  3.039590 (3.0086)  Time: 3.171s, 2583.24/s  (1.255s, 6527.19/s)  LR: 6.697e-03  Data: 0.000 (0.212)
2025-05-31 13:46:09,947 - train - INFO - Train: 39 [ 150/156 ( 97%)]  Loss:  3.024005 (3.0125)  Time: 0.536s, 15291.77/s  (1.226s, 6684.00/s)  LR: 6.697e-03  Data: 0.000 (0.142)
2025-05-31 13:46:15,055 - train - INFO - Train: 39 [ 155/156 (100%)]  Loss:  2.976461 (3.0053)  Time: 0.523s, 15656.89/s  (1.219s, 6719.92/s)  LR: 6.697e-03  Data: 0.000 (0.138)
2025-05-31 13:46:21,993 - train - INFO - Test: [   0/48]  Time: 6.662 (6.662)  Loss:  1.2803 (1.2803)  Acc@1: 77.2461 (77.2461)  Acc@5: 91.7969 (91.7969)
2025-05-31 13:47:20,825 - train - INFO - Test: [  48/48]  Time: 2.467 (1.337)  Loss:  1.2412 (1.9237)  Acc@1: 78.8915 (61.4920)  Acc@5: 92.2170 (83.7820)
2025-05-31 13:47:27,653 - train - INFO - Train: 40 [   0/156 (  0%)]  Loss:  2.959591 (2.9596)  Time: 6.448s, 1270.41/s  (6.448s, 1270.41/s)  LR: 6.549e-03  Data: 5.798 (5.798)
2025-05-31 13:48:28,342 - train - INFO - Train: 40 [  50/156 ( 32%)]  Loss:  2.984000 (2.9718)  Time: 0.530s, 15463.91/s  (1.316s, 6223.41/s)  LR: 6.549e-03  Data: 0.000 (0.346)
2025-05-31 13:49:31,452 - train - INFO - Train: 40 [ 100/156 ( 65%)]  Loss:  3.033804 (2.9925)  Time: 3.203s, 2557.59/s  (1.289s, 6352.95/s)  LR: 6.549e-03  Data: 0.000 (0.175)
2025-05-31 13:50:31,361 - train - INFO - Train: 40 [ 150/156 ( 97%)]  Loss:  3.032835 (3.0026)  Time: 0.533s, 15368.53/s  (1.259s, 6505.60/s)  LR: 6.549e-03  Data: 0.000 (0.117)
2025-05-31 13:50:36,242 - train - INFO - Train: 40 [ 155/156 (100%)]  Loss:  3.079287 (3.0179)  Time: 0.524s, 15637.02/s  (1.250s, 6552.92/s)  LR: 6.549e-03  Data: 0.000 (0.113)
2025-05-31 13:50:43,493 - train - INFO - Test: [   0/48]  Time: 6.959 (6.959)  Loss:  1.1416 (1.1416)  Acc@1: 76.2695 (76.2695)  Acc@5: 91.5039 (91.5039)
2025-05-31 13:51:41,946 - train - INFO - Test: [  48/48]  Time: 2.375 (1.335)  Loss:  1.2295 (1.8166)  Acc@1: 74.0566 (60.1300)  Acc@5: 90.0943 (82.9060)
2025-05-31 13:51:48,747 - train - INFO - Train: 41 [   0/156 (  0%)]  Loss:  3.021883 (3.0219)  Time: 6.420s, 1276.08/s  (6.420s, 1276.08/s)  LR: 6.399e-03  Data: 5.835 (5.835)
2025-05-31 13:52:47,470 - train - INFO - Train: 41 [  50/156 ( 32%)]  Loss:  3.024156 (3.0230)  Time: 0.527s, 15539.00/s  (1.277s, 6413.60/s)  LR: 6.399e-03  Data: 0.000 (0.366)
2025-05-31 13:53:48,236 - train - INFO - Train: 41 [ 100/156 ( 65%)]  Loss:  3.010458 (3.0188)  Time: 2.987s, 2742.44/s  (1.247s, 6571.46/s)  LR: 6.399e-03  Data: 0.000 (0.185)
2025-05-31 13:54:46,710 - train - INFO - Train: 41 [ 150/156 ( 97%)]  Loss:  3.019902 (3.0191)  Time: 0.526s, 15576.27/s  (1.221s, 6708.98/s)  LR: 6.399e-03  Data: 0.000 (0.124)
2025-05-31 13:54:52,389 - train - INFO - Train: 41 [ 155/156 (100%)]  Loss:  3.024039 (3.0201)  Time: 0.523s, 15672.97/s  (1.218s, 6724.06/s)  LR: 6.399e-03  Data: 0.000 (0.120)
2025-05-31 13:54:58,759 - train - INFO - Test: [   0/48]  Time: 6.085 (6.085)  Loss:  1.1104 (1.1104)  Acc@1: 77.2461 (77.2461)  Acc@5: 92.0898 (92.0898)
2025-05-31 13:55:57,549 - train - INFO - Test: [  48/48]  Time: 1.927 (1.324)  Loss:  1.1016 (1.7143)  Acc@1: 78.6557 (62.1280)  Acc@5: 91.9811 (84.1960)
2025-05-31 13:56:04,206 - train - INFO - Train: 42 [   0/156 (  0%)]  Loss:  3.013211 (3.0132)  Time: 6.265s, 1307.56/s  (6.265s, 1307.56/s)  LR: 6.247e-03  Data: 5.048 (5.048)
2025-05-31 13:57:04,362 - train - INFO - Train: 42 [  50/156 ( 32%)]  Loss:  3.009157 (3.0112)  Time: 0.536s, 15288.39/s  (1.302s, 6290.37/s)  LR: 6.247e-03  Data: 0.000 (0.295)
2025-05-31 13:58:06,003 - train - INFO - Train: 42 [ 100/156 ( 65%)]  Loss:  3.008488 (3.0103)  Time: 3.280s, 2497.69/s  (1.268s, 6461.05/s)  LR: 6.247e-03  Data: 0.000 (0.149)
2025-05-31 13:59:03,678 - train - INFO - Train: 42 [ 150/156 ( 97%)]  Loss:  2.997609 (3.0071)  Time: 0.528s, 15528.34/s  (1.230s, 6660.15/s)  LR: 6.247e-03  Data: 0.000 (0.100)
2025-05-31 13:59:08,856 - train - INFO - Train: 42 [ 155/156 (100%)]  Loss:  3.014874 (3.0087)  Time: 0.528s, 15522.44/s  (1.224s, 6694.07/s)  LR: 6.247e-03  Data: 0.000 (0.097)
2025-05-31 13:59:15,716 - train - INFO - Test: [   0/48]  Time: 6.567 (6.567)  Loss:  1.1963 (1.1963)  Acc@1: 77.7344 (77.7344)  Acc@5: 91.1133 (91.1133)
2025-05-31 14:00:13,681 - train - INFO - Test: [  48/48]  Time: 1.827 (1.317)  Loss:  1.0977 (1.7737)  Acc@1: 77.1226 (61.0820)  Acc@5: 92.2170 (83.6600)
2025-05-31 14:00:20,409 - train - INFO - Train: 43 [   0/156 (  0%)]  Loss:  3.016378 (3.0164)  Time: 6.316s, 1296.95/s  (6.316s, 1296.95/s)  LR: 6.095e-03  Data: 5.210 (5.210)
2025-05-31 14:01:18,821 - train - INFO - Train: 43 [  50/156 ( 32%)]  Loss:  3.028416 (3.0224)  Time: 0.529s, 15479.81/s  (1.269s, 6454.68/s)  LR: 6.095e-03  Data: 0.000 (0.496)
2025-05-31 14:02:19,510 - train - INFO - Train: 43 [ 100/156 ( 65%)]  Loss:  3.001742 (3.0155)  Time: 3.111s, 2632.95/s  (1.242s, 6597.26/s)  LR: 6.095e-03  Data: 0.000 (0.260)
2025-05-31 14:03:17,525 - train - INFO - Train: 43 [ 150/156 ( 97%)]  Loss:  2.992308 (3.0097)  Time: 0.528s, 15525.02/s  (1.215s, 6743.75/s)  LR: 6.095e-03  Data: 0.000 (0.174)
2025-05-31 14:03:22,459 - train - INFO - Train: 43 [ 155/156 (100%)]  Loss:  3.020571 (3.0119)  Time: 0.523s, 15648.67/s  (1.207s, 6784.56/s)  LR: 6.095e-03  Data: 0.000 (0.168)
2025-05-31 14:03:28,984 - train - INFO - Test: [   0/48]  Time: 6.241 (6.241)  Loss:  1.0107 (1.0107)  Acc@1: 78.1250 (78.1250)  Acc@5: 93.2617 (93.2617)
2025-05-31 14:04:27,611 - train - INFO - Test: [  48/48]  Time: 2.246 (1.324)  Loss:  1.1162 (1.7043)  Acc@1: 76.6509 (62.2820)  Acc@5: 91.9811 (84.4840)
2025-05-31 14:04:34,098 - train - INFO - Train: 44 [   0/156 (  0%)]  Loss:  2.960926 (2.9609)  Time: 6.110s, 1340.81/s  (6.110s, 1340.81/s)  LR: 5.941e-03  Data: 5.581 (5.581)
2025-05-31 14:05:34,925 - train - INFO - Train: 44 [  50/156 ( 32%)]  Loss:  3.024707 (2.9928)  Time: 1.462s, 5604.56/s  (1.312s, 6241.71/s)  LR: 5.941e-03  Data: 0.945 (0.472)
2025-05-31 14:06:35,322 - train - INFO - Train: 44 [ 100/156 ( 65%)]  Loss:  2.991119 (2.9923)  Time: 1.586s, 5165.64/s  (1.261s, 6497.94/s)  LR: 5.941e-03  Data: 0.000 (0.395)
2025-05-31 14:07:35,650 - train - INFO - Train: 44 [ 150/156 ( 97%)]  Loss:  2.979388 (2.9890)  Time: 1.409s, 5812.95/s  (1.243s, 6591.72/s)  LR: 5.941e-03  Data: 0.893 (0.376)
2025-05-31 14:07:41,314 - train - INFO - Train: 44 [ 155/156 (100%)]  Loss:  2.984889 (2.9882)  Time: 0.523s, 15649.22/s  (1.239s, 6610.49/s)  LR: 5.941e-03  Data: 0.000 (0.373)
2025-05-31 14:07:48,245 - train - INFO - Test: [   0/48]  Time: 6.629 (6.629)  Loss:  1.0967 (1.0967)  Acc@1: 79.1992 (79.1992)  Acc@5: 92.7734 (92.7734)
2025-05-31 14:08:46,241 - train - INFO - Test: [  48/48]  Time: 2.770 (1.319)  Loss:  1.1396 (1.7186)  Acc@1: 78.6557 (63.0620)  Acc@5: 92.4528 (85.1340)
2025-05-31 14:08:53,324 - train - INFO - Train: 45 [   0/156 (  0%)]  Loss:  2.990884 (2.9909)  Time: 6.544s, 1251.82/s  (6.544s, 1251.82/s)  LR: 5.786e-03  Data: 6.012 (6.012)
2025-05-31 14:09:53,484 - train - INFO - Train: 45 [  50/156 ( 32%)]  Loss:  2.997770 (2.9943)  Time: 0.565s, 14500.21/s  (1.308s, 6263.65/s)  LR: 5.786e-03  Data: 0.047 (0.768)
2025-05-31 14:10:54,015 - train - INFO - Train: 45 [ 100/156 ( 65%)]  Loss:  2.984581 (2.9911)  Time: 1.290s, 6350.56/s  (1.260s, 6503.09/s)  LR: 5.786e-03  Data: 0.773 (0.567)
2025-05-31 14:11:54,616 - train - INFO - Train: 45 [ 150/156 ( 97%)]  Loss:  2.978734 (2.9880)  Time: 0.525s, 15604.66/s  (1.244s, 6585.73/s)  LR: 5.786e-03  Data: 0.000 (0.411)
2025-05-31 14:11:59,705 - train - INFO - Train: 45 [ 155/156 (100%)]  Loss:  3.033166 (2.9970)  Time: 0.522s, 15683.31/s  (1.237s, 6624.40/s)  LR: 5.786e-03  Data: 0.000 (0.398)
2025-05-31 14:12:06,415 - train - INFO - Test: [   0/48]  Time: 6.370 (6.370)  Loss:  1.0693 (1.0693)  Acc@1: 79.0039 (79.0039)  Acc@5: 92.0898 (92.0898)
2025-05-31 14:13:05,129 - train - INFO - Test: [  48/48]  Time: 1.633 (1.328)  Loss:  1.0322 (1.6820)  Acc@1: 79.3632 (62.9900)  Acc@5: 92.2170 (84.7720)
2025-05-31 14:13:11,854 - train - INFO - Train: 46 [   0/156 (  0%)]  Loss:  3.010153 (3.0102)  Time: 6.302s, 1300.01/s  (6.302s, 1300.01/s)  LR: 5.631e-03  Data: 5.422 (5.422)
2025-05-31 14:14:11,029 - train - INFO - Train: 46 [  50/156 ( 32%)]  Loss:  2.957772 (2.9840)  Time: 0.527s, 15545.51/s  (1.284s, 6380.91/s)  LR: 5.631e-03  Data: 0.000 (0.318)
2025-05-31 14:15:12,313 - train - INFO - Train: 46 [ 100/156 ( 65%)]  Loss:  3.002714 (2.9902)  Time: 3.106s, 2637.32/s  (1.255s, 6527.30/s)  LR: 5.631e-03  Data: 0.000 (0.164)
2025-05-31 14:16:10,974 - train - INFO - Train: 46 [ 150/156 ( 97%)]  Loss:  2.964352 (2.9837)  Time: 0.525s, 15609.02/s  (1.228s, 6671.39/s)  LR: 5.631e-03  Data: 0.000 (0.110)
2025-05-31 14:16:15,808 - train - INFO - Train: 46 [ 155/156 (100%)]  Loss:  2.991398 (2.9853)  Time: 0.524s, 15635.94/s  (1.220s, 6717.20/s)  LR: 5.631e-03  Data: 0.000 (0.107)
2025-05-31 14:16:22,848 - train - INFO - Test: [   0/48]  Time: 6.746 (6.746)  Loss:  1.1514 (1.1514)  Acc@1: 76.9531 (76.9531)  Acc@5: 91.5039 (91.5039)
2025-05-31 14:17:20,831 - train - INFO - Test: [  48/48]  Time: 2.080 (1.321)  Loss:  1.0479 (1.7005)  Acc@1: 77.8302 (62.6160)  Acc@5: 93.0425 (84.5400)
2025-05-31 14:17:27,380 - train - INFO - Train: 47 [   0/156 (  0%)]  Loss:  2.985836 (2.9858)  Time: 6.171s, 1327.61/s  (6.171s, 1327.61/s)  LR: 5.475e-03  Data: 5.130 (5.130)
2025-05-31 14:18:25,661 - train - INFO - Train: 47 [  50/156 ( 32%)]  Loss:  2.984982 (2.9854)  Time: 0.526s, 15575.31/s  (1.264s, 6482.41/s)  LR: 5.475e-03  Data: 0.000 (0.499)
2025-05-31 14:19:26,524 - train - INFO - Train: 47 [ 100/156 ( 65%)]  Loss:  3.019684 (2.9968)  Time: 3.235s, 2532.24/s  (1.241s, 6602.69/s)  LR: 5.475e-03  Data: 0.000 (0.332)
2025-05-31 14:20:24,646 - train - INFO - Train: 47 [ 150/156 ( 97%)]  Loss:  2.986320 (2.9942)  Time: 0.526s, 15579.04/s  (1.215s, 6743.56/s)  LR: 5.475e-03  Data: 0.000 (0.222)
2025-05-31 14:20:29,362 - train - INFO - Train: 47 [ 155/156 (100%)]  Loss:  2.976447 (2.9907)  Time: 0.522s, 15689.18/s  (1.206s, 6792.32/s)  LR: 5.475e-03  Data: 0.000 (0.215)
2025-05-31 14:20:36,122 - train - INFO - Test: [   0/48]  Time: 6.473 (6.473)  Loss:  1.1172 (1.1172)  Acc@1: 76.8555 (76.8555)  Acc@5: 91.2109 (91.2109)
2025-05-31 14:21:34,354 - train - INFO - Test: [  48/48]  Time: 2.393 (1.321)  Loss:  1.2002 (1.7480)  Acc@1: 76.5330 (61.7640)  Acc@5: 91.9811 (84.1180)
2025-05-31 14:21:41,064 - train - INFO - Train: 48 [   0/156 (  0%)]  Loss:  2.970720 (2.9707)  Time: 6.349s, 1290.18/s  (6.349s, 1290.18/s)  LR: 5.319e-03  Data: 5.496 (5.496)
2025-05-31 14:22:40,170 - train - INFO - Train: 48 [  50/156 ( 32%)]  Loss:  3.002467 (2.9866)  Time: 0.533s, 15381.11/s  (1.283s, 6383.04/s)  LR: 5.319e-03  Data: 0.000 (0.629)
2025-05-31 14:23:41,052 - train - INFO - Train: 48 [ 100/156 ( 65%)]  Loss:  2.974051 (2.9824)  Time: 2.094s, 3911.75/s  (1.251s, 6549.28/s)  LR: 5.319e-03  Data: 0.000 (0.425)
2025-05-31 14:24:41,501 - train - INFO - Train: 48 [ 150/156 ( 97%)]  Loss:  2.975784 (2.9808)  Time: 0.525s, 15613.13/s  (1.237s, 6622.67/s)  LR: 5.319e-03  Data: 0.000 (0.284)
2025-05-31 14:24:46,479 - train - INFO - Train: 48 [ 155/156 (100%)]  Loss:  2.961852 (2.9770)  Time: 0.525s, 15617.77/s  (1.229s, 6664.37/s)  LR: 5.319e-03  Data: 0.000 (0.275)
2025-05-31 14:24:53,238 - train - INFO - Test: [   0/48]  Time: 6.482 (6.482)  Loss:  1.2451 (1.2451)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.1133 (91.1133)
2025-05-31 14:25:51,786 - train - INFO - Test: [  48/48]  Time: 2.511 (1.327)  Loss:  1.1299 (1.7855)  Acc@1: 78.4198 (61.2280)  Acc@5: 92.0991 (83.8280)
2025-05-31 14:25:58,458 - train - INFO - Train: 49 [   0/156 (  0%)]  Loss:  2.991853 (2.9919)  Time: 6.303s, 1299.79/s  (6.303s, 1299.79/s)  LR: 5.162e-03  Data: 5.474 (5.474)
2025-05-31 14:26:57,020 - train - INFO - Train: 49 [  50/156 ( 32%)]  Loss:  2.958790 (2.9753)  Time: 0.528s, 15507.63/s  (1.272s, 6441.05/s)  LR: 5.162e-03  Data: 0.000 (0.333)
2025-05-31 14:27:58,315 - train - INFO - Train: 49 [ 100/156 ( 65%)]  Loss:  2.968596 (2.9731)  Time: 3.374s, 2427.91/s  (1.249s, 6558.37/s)  LR: 5.162e-03  Data: 0.000 (0.168)
2025-05-31 14:28:55,689 - train - INFO - Train: 49 [ 150/156 ( 97%)]  Loss:  2.995509 (2.9787)  Time: 0.526s, 15566.83/s  (1.215s, 6739.97/s)  LR: 5.162e-03  Data: 0.000 (0.113)
2025-05-31 14:29:00,888 - train - INFO - Train: 49 [ 155/156 (100%)]  Loss:  2.993371 (2.9816)  Time: 0.521s, 15727.43/s  (1.210s, 6771.41/s)  LR: 5.162e-03  Data: 0.000 (0.109)
2025-05-31 14:29:07,559 - train - INFO - Test: [   0/48]  Time: 6.387 (6.387)  Loss:  0.9810 (0.9810)  Acc@1: 79.8828 (79.8828)  Acc@5: 93.4570 (93.4570)
2025-05-31 14:30:05,830 - train - INFO - Test: [  48/48]  Time: 1.753 (1.320)  Loss:  0.9839 (1.6606)  Acc@1: 79.0094 (63.3420)  Acc@5: 93.5142 (85.1200)
2025-05-31 14:30:12,564 - train - INFO - Train: 50 [   0/156 (  0%)]  Loss:  2.985708 (2.9857)  Time: 6.141s, 1334.00/s  (6.141s, 1334.00/s)  LR: 5.005e-03  Data: 5.616 (5.616)
2025-05-31 14:31:11,023 - train - INFO - Train: 50 [  50/156 ( 32%)]  Loss:  2.979636 (2.9827)  Time: 0.529s, 15472.53/s  (1.267s, 6467.50/s)  LR: 5.005e-03  Data: 0.000 (0.622)
2025-05-31 14:32:11,182 - train - INFO - Train: 50 [ 100/156 ( 65%)]  Loss:  2.986621 (2.9840)  Time: 2.421s, 3383.48/s  (1.235s, 6632.09/s)  LR: 5.005e-03  Data: 1.641 (0.540)
2025-05-31 14:33:11,146 - train - INFO - Train: 50 [ 150/156 ( 97%)]  Loss:  2.958865 (2.9777)  Time: 0.525s, 15597.79/s  (1.223s, 6696.65/s)  LR: 5.005e-03  Data: 0.000 (0.573)
2025-05-31 14:33:15,959 - train - INFO - Train: 50 [ 155/156 (100%)]  Loss:  2.996875 (2.9815)  Time: 0.522s, 15681.27/s  (1.215s, 6742.83/s)  LR: 5.005e-03  Data: 0.000 (0.566)
2025-05-31 14:33:22,445 - train - INFO - Test: [   0/48]  Time: 6.208 (6.208)  Loss:  0.9761 (0.9761)  Acc@1: 79.9805 (79.9805)  Acc@5: 93.7500 (93.7500)
2025-05-31 14:34:20,549 - train - INFO - Test: [  48/48]  Time: 2.232 (1.312)  Loss:  1.1572 (1.7209)  Acc@1: 76.8868 (62.7720)  Acc@5: 92.2170 (84.9880)
2025-05-31 14:34:27,360 - train - INFO - Train: 51 [   0/156 (  0%)]  Loss:  2.968017 (2.9680)  Time: 6.427s, 1274.61/s  (6.427s, 1274.61/s)  LR: 4.848e-03  Data: 5.380 (5.380)
2025-05-31 14:35:25,999 - train - INFO - Train: 51 [  50/156 ( 32%)]  Loss:  3.008534 (2.9883)  Time: 0.839s, 9762.71/s  (1.276s, 6421.22/s)  LR: 4.848e-03  Data: 0.000 (0.629)
2025-05-31 14:36:26,191 - train - INFO - Train: 51 [ 100/156 ( 65%)]  Loss:  2.961867 (2.9795)  Time: 2.058s, 3980.70/s  (1.240s, 6605.64/s)  LR: 4.848e-03  Data: 0.165 (0.407)
2025-05-31 14:37:25,664 - train - INFO - Train: 51 [ 150/156 ( 97%)]  Loss:  2.942063 (2.9701)  Time: 1.178s, 6951.63/s  (1.223s, 6696.30/s)  LR: 4.848e-03  Data: 0.000 (0.272)
2025-05-31 14:37:30,442 - train - INFO - Train: 51 [ 155/156 (100%)]  Loss:  3.019259 (2.9799)  Time: 0.526s, 15582.57/s  (1.215s, 6743.63/s)  LR: 4.848e-03  Data: 0.000 (0.263)
2025-05-31 14:37:37,406 - train - INFO - Test: [   0/48]  Time: 6.668 (6.668)  Loss:  1.0146 (1.0146)  Acc@1: 77.9297 (77.9297)  Acc@5: 92.2852 (92.2852)
2025-05-31 14:38:36,193 - train - INFO - Test: [  48/48]  Time: 3.215 (1.336)  Loss:  1.1436 (1.7124)  Acc@1: 75.4717 (62.1800)  Acc@5: 90.2123 (84.3700)
2025-05-31 14:38:43,238 - train - INFO - Train: 52 [   0/156 (  0%)]  Loss:  2.954617 (2.9546)  Time: 6.673s, 1227.69/s  (6.673s, 1227.69/s)  LR: 4.691e-03  Data: 5.784 (5.784)
2025-05-31 14:39:42,413 - train - INFO - Train: 52 [  50/156 ( 32%)]  Loss:  2.954275 (2.9544)  Time: 0.534s, 15348.34/s  (1.291s, 6344.98/s)  LR: 4.691e-03  Data: 0.000 (0.721)
2025-05-31 14:40:43,626 - train - INFO - Train: 52 [ 100/156 ( 65%)]  Loss:  2.959234 (2.9560)  Time: 3.229s, 2537.18/s  (1.258s, 6511.93/s)  LR: 4.691e-03  Data: 2.479 (0.700)
2025-05-31 14:41:42,387 - train - INFO - Train: 52 [ 150/156 ( 97%)]  Loss:  2.983933 (2.9630)  Time: 0.924s, 8864.44/s  (1.231s, 6657.01/s)  LR: 4.691e-03  Data: 0.000 (0.643)
2025-05-31 14:41:47,232 - train - INFO - Train: 52 [ 155/156 (100%)]  Loss:  2.958279 (2.9621)  Time: 0.522s, 15703.25/s  (1.222s, 6702.76/s)  LR: 4.691e-03  Data: 0.000 (0.631)
2025-05-31 14:41:53,670 - train - INFO - Test: [   0/48]  Time: 6.145 (6.145)  Loss:  1.1318 (1.1318)  Acc@1: 78.0273 (78.0273)  Acc@5: 92.1875 (92.1875)
2025-05-31 14:42:51,062 - train - INFO - Test: [  48/48]  Time: 2.493 (1.297)  Loss:  1.1016 (1.8066)  Acc@1: 79.3632 (60.4820)  Acc@5: 91.9811 (83.0080)
2025-05-31 14:42:57,754 - train - INFO - Train: 53 [   0/156 (  0%)]  Loss:  2.953466 (2.9535)  Time: 6.309s, 1298.44/s  (6.309s, 1298.44/s)  LR: 4.535e-03  Data: 5.422 (5.422)
2025-05-31 14:43:56,060 - train - INFO - Train: 53 [  50/156 ( 32%)]  Loss:  2.945776 (2.9496)  Time: 0.528s, 15511.61/s  (1.267s, 6466.03/s)  LR: 4.535e-03  Data: 0.000 (0.597)
2025-05-31 14:44:56,926 - train - INFO - Train: 53 [ 100/156 ( 65%)]  Loss:  2.960659 (2.9533)  Time: 2.677s, 3060.59/s  (1.242s, 6593.89/s)  LR: 4.535e-03  Data: 0.349 (0.448)
2025-05-31 14:45:55,624 - train - INFO - Train: 53 [ 150/156 ( 97%)]  Loss:  2.996211 (2.9640)  Time: 0.526s, 15561.49/s  (1.220s, 6716.37/s)  LR: 4.535e-03  Data: 0.000 (0.320)
2025-05-31 14:46:00,732 - train - INFO - Train: 53 [ 155/156 (100%)]  Loss:  2.995984 (2.9704)  Time: 0.523s, 15652.64/s  (1.213s, 6751.56/s)  LR: 4.535e-03  Data: 0.000 (0.310)
2025-05-31 14:46:07,537 - train - INFO - Test: [   0/48]  Time: 6.517 (6.517)  Loss:  0.9927 (0.9927)  Acc@1: 81.7383 (81.7383)  Acc@5: 93.3594 (93.3594)
2025-05-31 14:47:05,532 - train - INFO - Test: [  48/48]  Time: 1.810 (1.317)  Loss:  0.9727 (1.6890)  Acc@1: 80.0707 (63.1540)  Acc@5: 94.3396 (85.0900)
2025-05-31 14:47:12,189 - train - INFO - Train: 54 [   0/156 (  0%)]  Loss:  2.981307 (2.9813)  Time: 6.286s, 1303.20/s  (6.286s, 1303.20/s)  LR: 4.379e-03  Data: 5.374 (5.374)
2025-05-31 14:48:11,752 - train - INFO - Train: 54 [  50/156 ( 32%)]  Loss:  2.920823 (2.9511)  Time: 0.529s, 15494.63/s  (1.291s, 6344.88/s)  LR: 4.379e-03  Data: 0.000 (0.729)
2025-05-31 14:49:12,437 - train - INFO - Train: 54 [ 100/156 ( 65%)]  Loss:  2.982329 (2.9615)  Time: 3.084s, 2656.24/s  (1.253s, 6539.10/s)  LR: 4.379e-03  Data: 2.199 (0.700)
2025-05-31 14:50:10,810 - train - INFO - Train: 54 [ 150/156 ( 97%)]  Loss:  2.979891 (2.9661)  Time: 0.526s, 15560.98/s  (1.225s, 6689.97/s)  LR: 4.379e-03  Data: 0.000 (0.681)
2025-05-31 14:50:16,202 - train - INFO - Train: 54 [ 155/156 (100%)]  Loss:  2.971987 (2.9673)  Time: 0.523s, 15669.65/s  (1.220s, 6715.70/s)  LR: 4.379e-03  Data: 0.000 (0.677)
2025-05-31 14:50:22,883 - train - INFO - Test: [   0/48]  Time: 6.381 (6.381)  Loss:  1.1055 (1.1055)  Acc@1: 78.0273 (78.0273)  Acc@5: 93.2617 (93.2617)
2025-05-31 14:51:21,083 - train - INFO - Test: [  48/48]  Time: 2.815 (1.318)  Loss:  1.1289 (1.7509)  Acc@1: 78.7736 (62.4760)  Acc@5: 92.9245 (84.5680)
2025-05-31 14:51:28,193 - train - INFO - Train: 55 [   0/156 (  0%)]  Loss:  2.963139 (2.9631)  Time: 6.744s, 1214.69/s  (6.744s, 1214.69/s)  LR: 4.224e-03  Data: 6.207 (6.207)
2025-05-31 14:52:27,030 - train - INFO - Train: 55 [  50/156 ( 32%)]  Loss:  2.975019 (2.9691)  Time: 0.527s, 15537.61/s  (1.286s, 6370.75/s)  LR: 4.224e-03  Data: 0.000 (0.760)
2025-05-31 14:53:27,659 - train - INFO - Train: 55 [ 100/156 ( 65%)]  Loss:  2.937927 (2.9587)  Time: 2.558s, 3202.35/s  (1.250s, 6555.81/s)  LR: 4.224e-03  Data: 2.042 (0.725)
2025-05-31 14:54:25,857 - train - INFO - Train: 55 [ 150/156 ( 97%)]  Loss:  2.940637 (2.9542)  Time: 0.527s, 15557.81/s  (1.221s, 6708.12/s)  LR: 4.224e-03  Data: 0.000 (0.697)
2025-05-31 14:54:32,197 - train - INFO - Train: 55 [ 155/156 (100%)]  Loss:  2.958833 (2.9551)  Time: 1.648s, 4970.88/s  (1.223s, 6699.88/s)  LR: 4.224e-03  Data: 1.134 (0.699)
2025-05-31 14:54:38,945 - train - INFO - Test: [   0/48]  Time: 6.455 (6.455)  Loss:  1.0234 (1.0234)  Acc@1: 79.2969 (79.2969)  Acc@5: 93.2617 (93.2617)
2025-05-31 14:55:36,709 - train - INFO - Test: [  48/48]  Time: 2.259 (1.311)  Loss:  1.1484 (1.6921)  Acc@1: 77.8302 (63.2620)  Acc@5: 93.5142 (85.1180)
2025-05-31 14:55:43,384 - train - INFO - Train: 56 [   0/156 (  0%)]  Loss:  2.978276 (2.9783)  Time: 6.296s, 1301.08/s  (6.296s, 1301.08/s)  LR: 4.069e-03  Data: 5.328 (5.328)
2025-05-31 14:56:42,417 - train - INFO - Train: 56 [  50/156 ( 32%)]  Loss:  2.947552 (2.9629)  Time: 0.529s, 15494.77/s  (1.281s, 6395.25/s)  LR: 4.069e-03  Data: 0.000 (0.623)
2025-05-31 14:57:42,254 - train - INFO - Train: 56 [ 100/156 ( 65%)]  Loss:  3.000288 (2.9754)  Time: 1.753s, 4672.33/s  (1.239s, 6610.44/s)  LR: 4.069e-03  Data: 1.235 (0.633)
2025-05-31 14:58:42,410 - train - INFO - Train: 56 [ 150/156 ( 97%)]  Loss:  2.963673 (2.9724)  Time: 1.462s, 5604.50/s  (1.227s, 6674.93/s)  LR: 4.069e-03  Data: 0.947 (0.624)
2025-05-31 14:58:47,534 - train - INFO - Train: 56 [ 155/156 (100%)]  Loss:  2.965287 (2.9710)  Time: 0.523s, 15674.93/s  (1.221s, 6710.47/s)  LR: 4.069e-03  Data: 0.000 (0.619)
2025-05-31 14:58:54,299 - train - INFO - Test: [   0/48]  Time: 6.473 (6.473)  Loss:  1.0303 (1.0303)  Acc@1: 77.6367 (77.6367)  Acc@5: 92.8711 (92.8711)
2025-05-31 14:59:52,093 - train - INFO - Test: [  48/48]  Time: 2.160 (1.312)  Loss:  1.0850 (1.6878)  Acc@1: 78.1840 (61.9900)  Acc@5: 92.5707 (84.5200)
2025-05-31 14:59:58,805 - train - INFO - Train: 57 [   0/156 (  0%)]  Loss:  2.943650 (2.9437)  Time: 6.338s, 1292.56/s  (6.338s, 1292.56/s)  LR: 3.915e-03  Data: 5.811 (5.811)
2025-05-31 15:00:57,538 - train - INFO - Train: 57 [  50/156 ( 32%)]  Loss:  2.957654 (2.9507)  Time: 0.533s, 15364.71/s  (1.276s, 6420.75/s)  LR: 3.915e-03  Data: 0.000 (0.471)
2025-05-31 15:01:58,313 - train - INFO - Train: 57 [ 100/156 ( 65%)]  Loss:  2.995737 (2.9657)  Time: 3.192s, 2566.42/s  (1.246s, 6574.82/s)  LR: 3.915e-03  Data: 0.000 (0.264)
2025-05-31 15:02:57,166 - train - INFO - Train: 57 [ 150/156 ( 97%)]  Loss:  2.966693 (2.9659)  Time: 1.358s, 6030.33/s  (1.223s, 6697.49/s)  LR: 3.915e-03  Data: 0.000 (0.177)
2025-05-31 15:03:02,118 - train - INFO - Train: 57 [ 155/156 (100%)]  Loss:  3.017543 (2.9763)  Time: 0.524s, 15641.42/s  (1.216s, 6738.62/s)  LR: 3.915e-03  Data: 0.000 (0.171)
2025-05-31 15:03:08,982 - train - INFO - Test: [   0/48]  Time: 6.557 (6.557)  Loss:  1.2998 (1.2998)  Acc@1: 75.7812 (75.7812)  Acc@5: 91.4062 (91.4062)
2025-05-31 15:04:07,073 - train - INFO - Test: [  48/48]  Time: 2.092 (1.319)  Loss:  1.1357 (1.8072)  Acc@1: 78.0660 (61.4140)  Acc@5: 92.8066 (84.0720)
2025-05-31 15:04:14,125 - train - INFO - Train: 58 [   0/156 (  0%)]  Loss:  2.967376 (2.9674)  Time: 6.680s, 1226.38/s  (6.680s, 1226.38/s)  LR: 3.763e-03  Data: 6.149 (6.149)
2025-05-31 15:05:11,801 - train - INFO - Train: 58 [  50/156 ( 32%)]  Loss:  2.961991 (2.9647)  Time: 0.531s, 15428.16/s  (1.262s, 6492.00/s)  LR: 3.763e-03  Data: 0.000 (0.658)
2025-05-31 15:06:13,274 - train - INFO - Train: 58 [ 100/156 ( 65%)]  Loss:  2.950629 (2.9600)  Time: 2.957s, 2770.09/s  (1.246s, 6575.66/s)  LR: 3.763e-03  Data: 0.000 (0.441)
2025-05-31 15:07:12,469 - train - INFO - Train: 58 [ 150/156 ( 97%)]  Loss:  2.980506 (2.9651)  Time: 1.441s, 5686.09/s  (1.225s, 6685.70/s)  LR: 3.763e-03  Data: 0.000 (0.295)
2025-05-31 15:07:17,401 - train - INFO - Train: 58 [ 155/156 (100%)]  Loss:  2.964752 (2.9651)  Time: 0.523s, 15669.79/s  (1.218s, 6727.81/s)  LR: 3.763e-03  Data: 0.000 (0.286)
2025-05-31 15:07:23,972 - train - INFO - Test: [   0/48]  Time: 6.293 (6.293)  Loss:  1.0264 (1.0264)  Acc@1: 79.2969 (79.2969)  Acc@5: 92.3828 (92.3828)
2025-05-31 15:08:22,269 - train - INFO - Test: [  48/48]  Time: 2.254 (1.318)  Loss:  0.9912 (1.6740)  Acc@1: 78.8915 (62.9720)  Acc@5: 93.1604 (84.9220)
2025-05-31 15:08:29,163 - train - INFO - Train: 59 [   0/156 (  0%)]  Loss:  2.958172 (2.9582)  Time: 6.525s, 1255.46/s  (6.525s, 1255.46/s)  LR: 3.611e-03  Data: 6.006 (6.006)
2025-05-31 15:09:28,000 - train - INFO - Train: 59 [  50/156 ( 32%)]  Loss:  2.934737 (2.9465)  Time: 0.527s, 15553.33/s  (1.282s, 6392.08/s)  LR: 3.611e-03  Data: 0.000 (0.611)
2025-05-31 15:10:28,484 - train - INFO - Train: 59 [ 100/156 ( 65%)]  Loss:  2.923193 (2.9387)  Time: 1.623s, 5047.61/s  (1.246s, 6574.75/s)  LR: 3.611e-03  Data: 1.106 (0.493)
2025-05-31 15:11:28,371 - train - INFO - Train: 59 [ 150/156 ( 97%)]  Loss:  2.936330 (2.9381)  Time: 0.527s, 15559.20/s  (1.230s, 6660.16/s)  LR: 3.611e-03  Data: 0.000 (0.393)
2025-05-31 15:11:33,366 - train - INFO - Train: 59 [ 155/156 (100%)]  Loss:  2.926267 (2.9357)  Time: 0.523s, 15661.77/s  (1.223s, 6700.52/s)  LR: 3.611e-03  Data: 0.000 (0.382)
2025-05-31 15:11:40,162 - train - INFO - Test: [   0/48]  Time: 6.503 (6.503)  Loss:  1.0479 (1.0479)  Acc@1: 78.0273 (78.0273)  Acc@5: 92.7734 (92.7734)
2025-05-31 15:12:38,382 - train - INFO - Test: [  48/48]  Time: 1.977 (1.321)  Loss:  0.9834 (1.6692)  Acc@1: 78.7736 (63.1420)  Acc@5: 93.7500 (84.8740)
2025-05-31 15:12:45,063 - train - INFO - Train: 60 [   0/156 (  0%)]  Loss:  2.918875 (2.9189)  Time: 6.298s, 1300.77/s  (6.298s, 1300.77/s)  LR: 3.461e-03  Data: 5.276 (5.276)
2025-05-31 15:13:43,909 - train - INFO - Train: 60 [  50/156 ( 32%)]  Loss:  2.969001 (2.9439)  Time: 0.527s, 15542.92/s  (1.277s, 6413.63/s)  LR: 3.461e-03  Data: 0.000 (0.529)
2025-05-31 15:14:45,583 - train - INFO - Train: 60 [ 100/156 ( 65%)]  Loss:  2.927166 (2.9383)  Time: 3.344s, 2449.49/s  (1.256s, 6524.49/s)  LR: 3.461e-03  Data: 0.567 (0.429)
2025-05-31 15:15:44,190 - train - INFO - Train: 60 [ 150/156 ( 97%)]  Loss:  2.946337 (2.9403)  Time: 0.528s, 15523.98/s  (1.228s, 6671.33/s)  LR: 3.461e-03  Data: 0.000 (0.292)
2025-05-31 15:15:49,081 - train - INFO - Train: 60 [ 155/156 (100%)]  Loss:  2.930585 (2.9384)  Time: 0.523s, 15672.02/s  (1.220s, 6715.17/s)  LR: 3.461e-03  Data: 0.000 (0.282)
2025-05-31 15:15:55,625 - train - INFO - Test: [   0/48]  Time: 6.253 (6.253)  Loss:  1.2021 (1.2021)  Acc@1: 78.1250 (78.1250)  Acc@5: 91.7969 (91.7969)
2025-05-31 15:16:54,767 - train - INFO - Test: [  48/48]  Time: 3.501 (1.335)  Loss:  1.2188 (1.7521)  Acc@1: 77.5943 (62.8300)  Acc@5: 92.4528 (84.9200)
2025-05-31 15:17:01,507 - train - INFO - Train: 61 [   0/156 (  0%)]  Loss:  2.971700 (2.9717)  Time: 6.366s, 1286.89/s  (6.366s, 1286.89/s)  LR: 3.313e-03  Data: 5.072 (5.072)
2025-05-31 15:17:59,260 - train - INFO - Train: 61 [  50/156 ( 32%)]  Loss:  2.926810 (2.9493)  Time: 0.528s, 15511.11/s  (1.257s, 6516.11/s)  LR: 3.313e-03  Data: 0.000 (0.418)
2025-05-31 15:18:58,942 - train - INFO - Train: 61 [ 100/156 ( 65%)]  Loss:  2.940654 (2.9464)  Time: 1.779s, 4605.06/s  (1.226s, 6683.44/s)  LR: 3.313e-03  Data: 0.000 (0.289)
2025-05-31 15:19:57,357 - train - INFO - Train: 61 [ 150/156 ( 97%)]  Loss:  2.963575 (2.9507)  Time: 0.790s, 10374.85/s  (1.207s, 6788.82/s)  LR: 3.313e-03  Data: 0.000 (0.194)
2025-05-31 15:20:04,792 - train - INFO - Train: 61 [ 155/156 (100%)]  Loss:  2.961780 (2.9529)  Time: 2.919s, 2806.70/s  (1.216s, 6738.66/s)  LR: 3.313e-03  Data: 0.000 (0.187)
2025-05-31 15:20:11,677 - train - INFO - Test: [   0/48]  Time: 6.586 (6.586)  Loss:  1.0830 (1.0830)  Acc@1: 77.8320 (77.8320)  Acc@5: 93.4570 (93.4570)
2025-05-31 15:21:09,770 - train - INFO - Test: [  48/48]  Time: 2.759 (1.320)  Loss:  1.1475 (1.8128)  Acc@1: 77.1226 (60.7280)  Acc@5: 91.7453 (83.4780)
2025-05-31 15:21:16,244 - train - INFO - Train: 62 [   0/156 (  0%)]  Loss:  2.958886 (2.9589)  Time: 6.102s, 1342.47/s  (6.102s, 1342.47/s)  LR: 3.166e-03  Data: 5.567 (5.567)
2025-05-31 15:22:14,179 - train - INFO - Train: 62 [  50/156 ( 32%)]  Loss:  2.969736 (2.9643)  Time: 0.528s, 15509.59/s  (1.256s, 6524.33/s)  LR: 3.166e-03  Data: 0.000 (0.565)
2025-05-31 15:23:14,370 - train - INFO - Train: 62 [ 100/156 ( 65%)]  Loss:  2.925463 (2.9514)  Time: 2.353s, 3481.27/s  (1.230s, 6660.45/s)  LR: 3.166e-03  Data: 0.048 (0.358)
2025-05-31 15:24:12,896 - train - INFO - Train: 62 [ 150/156 ( 97%)]  Loss:  2.946728 (2.9502)  Time: 0.527s, 15546.15/s  (1.210s, 6768.77/s)  LR: 3.166e-03  Data: 0.000 (0.243)
2025-05-31 15:24:17,904 - train - INFO - Train: 62 [ 155/156 (100%)]  Loss:  2.958923 (2.9519)  Time: 0.524s, 15635.89/s  (1.204s, 6806.41/s)  LR: 3.166e-03  Data: 0.000 (0.235)
2025-05-31 15:24:24,679 - train - INFO - Test: [   0/48]  Time: 6.479 (6.479)  Loss:  1.0205 (1.0205)  Acc@1: 77.3438 (77.3438)  Acc@5: 93.9453 (93.9453)
2025-05-31 15:25:22,756 - train - INFO - Test: [  48/48]  Time: 1.218 (1.317)  Loss:  1.0928 (1.6840)  Acc@1: 77.5943 (62.9800)  Acc@5: 91.7453 (84.8900)
2025-05-31 15:25:29,396 - train - INFO - Train: 63 [   0/156 (  0%)]  Loss:  2.938498 (2.9385)  Time: 6.275s, 1305.43/s  (6.275s, 1305.43/s)  LR: 3.021e-03  Data: 5.754 (5.754)
2025-05-31 15:26:27,546 - train - INFO - Train: 63 [  50/156 ( 32%)]  Loss:  2.983047 (2.9608)  Time: 0.526s, 15561.28/s  (1.263s, 6485.21/s)  LR: 3.021e-03  Data: 0.000 (0.735)
2025-05-31 15:27:27,739 - train - INFO - Train: 63 [ 100/156 ( 65%)]  Loss:  2.931333 (2.9510)  Time: 2.920s, 2805.66/s  (1.234s, 6639.59/s)  LR: 3.021e-03  Data: 1.891 (0.678)
2025-05-31 15:28:25,874 - train - INFO - Train: 63 [ 150/156 ( 97%)]  Loss:  2.920974 (2.9435)  Time: 0.780s, 10499.75/s  (1.210s, 6768.84/s)  LR: 3.021e-03  Data: 0.000 (0.645)
2025-05-31 15:28:30,684 - train - INFO - Train: 63 [ 155/156 (100%)]  Loss:  2.931642 (2.9411)  Time: 0.523s, 15669.89/s  (1.202s, 6813.66/s)  LR: 3.021e-03  Data: 0.000 (0.636)
2025-05-31 15:28:37,512 - train - INFO - Test: [   0/48]  Time: 6.540 (6.540)  Loss:  1.1025 (1.1025)  Acc@1: 77.4414 (77.4414)  Acc@5: 92.7734 (92.7734)
2025-05-31 15:29:35,065 - train - INFO - Test: [  48/48]  Time: 2.363 (1.308)  Loss:  1.0742 (1.7108)  Acc@1: 78.3019 (62.7800)  Acc@5: 92.0991 (84.3120)
2025-05-31 15:29:42,121 - train - INFO - Train: 64 [   0/156 (  0%)]  Loss:  2.951661 (2.9517)  Time: 6.688s, 1224.80/s  (6.688s, 1224.80/s)  LR: 2.878e-03  Data: 6.170 (6.170)
2025-05-31 15:30:40,433 - train - INFO - Train: 64 [  50/156 ( 32%)]  Loss:  2.947462 (2.9496)  Time: 0.537s, 15255.03/s  (1.274s, 6427.67/s)  LR: 2.878e-03  Data: 0.000 (0.690)
2025-05-31 15:31:40,903 - train - INFO - Train: 64 [ 100/156 ( 65%)]  Loss:  2.951725 (2.9503)  Time: 2.432s, 3368.45/s  (1.242s, 6594.50/s)  LR: 2.878e-03  Data: 0.457 (0.528)
2025-05-31 15:32:39,280 - train - INFO - Train: 64 [ 150/156 ( 97%)]  Loss:  2.947730 (2.9496)  Time: 0.527s, 15548.61/s  (1.217s, 6728.54/s)  LR: 2.878e-03  Data: 0.000 (0.368)
2025-05-31 15:32:44,003 - train - INFO - Train: 64 [ 155/156 (100%)]  Loss:  2.957979 (2.9513)  Time: 0.523s, 15654.53/s  (1.209s, 6777.26/s)  LR: 2.878e-03  Data: 0.000 (0.356)
2025-05-31 15:32:50,827 - train - INFO - Test: [   0/48]  Time: 6.549 (6.549)  Loss:  1.0781 (1.0781)  Acc@1: 78.8086 (78.8086)  Acc@5: 92.7734 (92.7734)
2025-05-31 15:33:49,540 - train - INFO - Test: [  48/48]  Time: 2.833 (1.332)  Loss:  1.0537 (1.6843)  Acc@1: 79.0094 (63.1580)  Acc@5: 92.6887 (85.0420)
2025-05-31 15:33:55,929 - train - INFO - Train: 65 [   0/156 (  0%)]  Loss:  2.938161 (2.9382)  Time: 5.958s, 1374.94/s  (5.958s, 1374.94/s)  LR: 2.737e-03  Data: 5.366 (5.366)
2025-05-31 15:34:53,908 - train - INFO - Train: 65 [  50/156 ( 32%)]  Loss:  2.971315 (2.9547)  Time: 0.526s, 15561.72/s  (1.254s, 6534.60/s)  LR: 2.737e-03  Data: 0.000 (0.303)
2025-05-31 15:35:54,717 - train - INFO - Train: 65 [ 100/156 ( 65%)]  Loss:  3.008214 (2.9726)  Time: 3.217s, 2546.45/s  (1.235s, 6632.81/s)  LR: 2.737e-03  Data: 0.000 (0.155)
2025-05-31 15:36:52,470 - train - INFO - Train: 65 [ 150/156 ( 97%)]  Loss:  2.982374 (2.9750)  Time: 0.527s, 15537.72/s  (1.209s, 6778.26/s)  LR: 2.737e-03  Data: 0.000 (0.104)
2025-05-31 15:36:57,407 - train - INFO - Train: 65 [ 155/156 (100%)]  Loss:  2.942667 (2.9685)  Time: 0.523s, 15664.79/s  (1.201s, 6818.26/s)  LR: 2.737e-03  Data: 0.000 (0.101)
2025-05-31 15:37:04,275 - train - INFO - Test: [   0/48]  Time: 6.574 (6.574)  Loss:  1.1436 (1.1436)  Acc@1: 77.4414 (77.4414)  Acc@5: 92.7734 (92.7734)
2025-05-31 15:38:02,693 - train - INFO - Test: [  48/48]  Time: 2.639 (1.326)  Loss:  1.0908 (1.7756)  Acc@1: 77.3585 (61.8100)  Acc@5: 92.2170 (84.1400)
2025-05-31 15:38:09,257 - train - INFO - Train: 66 [   0/156 (  0%)]  Loss:  2.926553 (2.9266)  Time: 6.173s, 1327.00/s  (6.173s, 1327.00/s)  LR: 2.599e-03  Data: 5.636 (5.636)
2025-05-31 15:39:08,193 - train - INFO - Train: 66 [  50/156 ( 32%)]  Loss:  2.937478 (2.9320)  Time: 0.530s, 15460.44/s  (1.277s, 6416.93/s)  LR: 2.599e-03  Data: 0.000 (0.567)
2025-05-31 15:40:10,052 - train - INFO - Train: 66 [ 100/156 ( 65%)]  Loss:  2.928691 (2.9309)  Time: 3.141s, 2608.36/s  (1.257s, 6516.66/s)  LR: 2.599e-03  Data: 0.000 (0.330)
2025-05-31 15:41:08,432 - train - INFO - Train: 66 [ 150/156 ( 97%)]  Loss:  2.931070 (2.9309)  Time: 0.532s, 15401.30/s  (1.227s, 6674.03/s)  LR: 2.599e-03  Data: 0.000 (0.221)
2025-05-31 15:41:13,360 - train - INFO - Train: 66 [ 155/156 (100%)]  Loss:  2.944061 (2.9336)  Time: 0.523s, 15656.40/s  (1.220s, 6716.48/s)  LR: 2.599e-03  Data: 0.000 (0.214)
2025-05-31 15:41:20,009 - train - INFO - Test: [   0/48]  Time: 6.355 (6.355)  Loss:  0.8037 (0.8037)  Acc@1: 80.6641 (80.6641)  Acc@5: 93.6523 (93.6523)
2025-05-31 15:42:17,790 - train - INFO - Test: [  48/48]  Time: 2.181 (1.309)  Loss:  0.9072 (1.5590)  Acc@1: 78.3019 (63.1500)  Acc@5: 92.0991 (84.9200)
2025-05-31 15:42:24,763 - train - INFO - Train: 67 [   0/156 (  0%)]  Loss:  2.943690 (2.9437)  Time: 6.594s, 1242.43/s  (6.594s, 1242.43/s)  LR: 2.462e-03  Data: 5.703 (5.703)
2025-05-31 15:43:23,332 - train - INFO - Train: 67 [  50/156 ( 32%)]  Loss:  2.972115 (2.9579)  Time: 0.528s, 15521.59/s  (1.278s, 6411.63/s)  LR: 2.462e-03  Data: 0.000 (0.550)
2025-05-31 15:44:23,577 - train - INFO - Train: 67 [ 100/156 ( 65%)]  Loss:  2.967166 (2.9610)  Time: 2.930s, 2795.88/s  (1.242s, 6597.76/s)  LR: 2.462e-03  Data: 0.000 (0.298)
2025-05-31 15:45:21,177 - train - INFO - Train: 67 [ 150/156 ( 97%)]  Loss:  2.909009 (2.9480)  Time: 0.658s, 12443.96/s  (1.212s, 6759.37/s)  LR: 2.462e-03  Data: 0.000 (0.199)
2025-05-31 15:45:26,600 - train - INFO - Train: 67 [ 155/156 (100%)]  Loss:  2.993959 (2.9572)  Time: 1.076s, 7615.91/s  (1.208s, 6782.23/s)  LR: 2.462e-03  Data: 0.000 (0.193)
2025-05-31 15:45:33,119 - train - INFO - Test: [   0/48]  Time: 6.228 (6.228)  Loss:  1.0977 (1.0977)  Acc@1: 78.2227 (78.2227)  Acc@5: 91.8945 (91.8945)
2025-05-31 15:46:31,285 - train - INFO - Test: [  48/48]  Time: 2.069 (1.314)  Loss:  0.9658 (1.7237)  Acc@1: 79.8349 (62.2660)  Acc@5: 94.1038 (84.5080)
2025-05-31 15:46:38,107 - train - INFO - Train: 68 [   0/156 (  0%)]  Loss:  2.949821 (2.9498)  Time: 6.446s, 1270.80/s  (6.446s, 1270.80/s)  LR: 2.329e-03  Data: 5.499 (5.499)
2025-05-31 15:47:36,420 - train - INFO - Train: 68 [  50/156 ( 32%)]  Loss:  2.937790 (2.9438)  Time: 0.535s, 15300.73/s  (1.270s, 6451.66/s)  LR: 2.329e-03  Data: 0.000 (0.489)
2025-05-31 15:48:36,962 - train - INFO - Train: 68 [ 100/156 ( 65%)]  Loss:  2.982986 (2.9569)  Time: 2.988s, 2741.64/s  (1.241s, 6603.32/s)  LR: 2.329e-03  Data: 0.386 (0.298)
2025-05-31 15:49:35,309 - train - INFO - Train: 68 [ 150/156 ( 97%)]  Loss:  2.908905 (2.9449)  Time: 0.756s, 10836.45/s  (1.216s, 6735.84/s)  LR: 2.329e-03  Data: 0.000 (0.208)
2025-05-31 15:49:40,215 - train - INFO - Train: 68 [ 155/156 (100%)]  Loss:  2.901980 (2.9363)  Time: 0.524s, 15642.66/s  (1.209s, 6777.86/s)  LR: 2.329e-03  Data: 0.000 (0.202)
2025-05-31 15:49:46,865 - train - INFO - Test: [   0/48]  Time: 6.372 (6.372)  Loss:  1.0381 (1.0381)  Acc@1: 78.8086 (78.8086)  Acc@5: 92.4805 (92.4805)
2025-05-31 15:50:45,286 - train - INFO - Test: [  48/48]  Time: 2.790 (1.322)  Loss:  1.0225 (1.6632)  Acc@1: 78.8915 (63.0820)  Acc@5: 93.1604 (85.0020)
2025-05-31 15:50:52,286 - train - INFO - Train: 69 [   0/156 (  0%)]  Loss:  2.969500 (2.9695)  Time: 6.631s, 1235.46/s  (6.631s, 1235.46/s)  LR: 2.197e-03  Data: 5.841 (5.841)
2025-05-31 15:51:49,809 - train - INFO - Train: 69 [  50/156 ( 32%)]  Loss:  2.931412 (2.9505)  Time: 0.528s, 15525.64/s  (1.258s, 6512.43/s)  LR: 2.197e-03  Data: 0.000 (0.637)
2025-05-31 15:52:50,319 - train - INFO - Train: 69 [ 100/156 ( 65%)]  Loss:  2.954417 (2.9518)  Time: 2.796s, 2929.92/s  (1.234s, 6637.12/s)  LR: 2.197e-03  Data: 2.011 (0.638)
2025-05-31 15:53:49,165 - train - INFO - Train: 69 [ 150/156 ( 97%)]  Loss:  2.951644 (2.9517)  Time: 1.050s, 7800.68/s  (1.215s, 6740.88/s)  LR: 2.197e-03  Data: 0.000 (0.554)
2025-05-31 15:53:54,123 - train - INFO - Train: 69 [ 155/156 (100%)]  Loss:  2.939052 (2.9492)  Time: 0.525s, 15616.22/s  (1.208s, 6780.88/s)  LR: 2.197e-03  Data: 0.000 (0.545)
2025-05-31 15:54:00,820 - train - INFO - Test: [   0/48]  Time: 6.402 (6.402)  Loss:  1.0479 (1.0479)  Acc@1: 79.0039 (79.0039)  Acc@5: 92.8711 (92.8711)
2025-05-31 15:54:58,833 - train - INFO - Test: [  48/48]  Time: 2.920 (1.315)  Loss:  1.0244 (1.6322)  Acc@1: 80.0707 (64.0280)  Acc@5: 93.5142 (85.6720)
2025-05-31 15:55:05,786 - train - INFO - Train: 70 [   0/156 (  0%)]  Loss:  2.928458 (2.9285)  Time: 6.417s, 1276.62/s  (6.417s, 1276.62/s)  LR: 2.069e-03  Data: 5.891 (5.891)
2025-05-31 15:56:04,356 - train - INFO - Train: 70 [  50/156 ( 32%)]  Loss:  2.941581 (2.9350)  Time: 0.527s, 15555.76/s  (1.274s, 6429.15/s)  LR: 2.069e-03  Data: 0.000 (0.581)
2025-05-31 15:57:05,381 - train - INFO - Train: 70 [ 100/156 ( 65%)]  Loss:  2.936029 (2.9354)  Time: 3.059s, 2678.28/s  (1.248s, 6566.15/s)  LR: 2.069e-03  Data: 0.160 (0.384)
2025-05-31 15:58:03,527 - train - INFO - Train: 70 [ 150/156 ( 97%)]  Loss:  2.967851 (2.9435)  Time: 0.527s, 15549.28/s  (1.220s, 6717.18/s)  LR: 2.069e-03  Data: 0.000 (0.258)
2025-05-31 15:58:08,333 - train - INFO - Train: 70 [ 155/156 (100%)]  Loss:  2.898651 (2.9345)  Time: 0.522s, 15706.40/s  (1.211s, 6763.16/s)  LR: 2.069e-03  Data: 0.000 (0.250)
2025-05-31 15:58:14,854 - train - INFO - Test: [   0/48]  Time: 6.234 (6.234)  Loss:  0.9336 (0.9336)  Acc@1: 81.1523 (81.1523)  Acc@5: 93.6523 (93.6523)
2025-05-31 15:59:13,165 - train - INFO - Test: [  48/48]  Time: 2.336 (1.317)  Loss:  0.9873 (1.6406)  Acc@1: 79.8349 (63.5760)  Acc@5: 93.0425 (85.3180)
2025-05-31 15:59:19,771 - train - INFO - Train: 71 [   0/156 (  0%)]  Loss:  2.902217 (2.9022)  Time: 6.219s, 1317.31/s  (6.219s, 1317.31/s)  LR: 1.944e-03  Data: 5.063 (5.063)
2025-05-31 16:00:19,187 - train - INFO - Train: 71 [  50/156 ( 32%)]  Loss:  2.946601 (2.9244)  Time: 0.528s, 15525.86/s  (1.287s, 6365.46/s)  LR: 1.944e-03  Data: 0.000 (0.565)
2025-05-31 16:01:19,970 - train - INFO - Train: 71 [ 100/156 ( 65%)]  Loss:  2.935596 (2.9281)  Time: 2.808s, 2916.96/s  (1.252s, 6545.00/s)  LR: 1.944e-03  Data: 0.676 (0.409)
2025-05-31 16:02:18,697 - train - INFO - Train: 71 [ 150/156 ( 97%)]  Loss:  2.931327 (2.9289)  Time: 0.526s, 15572.38/s  (1.226s, 6681.32/s)  LR: 1.944e-03  Data: 0.000 (0.350)
2025-05-31 16:02:23,757 - train - INFO - Train: 71 [ 155/156 (100%)]  Loss:  2.957857 (2.9347)  Time: 0.521s, 15726.27/s  (1.219s, 6718.96/s)  LR: 1.944e-03  Data: 0.000 (0.345)
2025-05-31 16:02:30,604 - train - INFO - Test: [   0/48]  Time: 6.551 (6.551)  Loss:  0.9956 (0.9956)  Acc@1: 79.6875 (79.6875)  Acc@5: 92.7734 (92.7734)
2025-05-31 16:03:28,020 - train - INFO - Test: [  48/48]  Time: 2.130 (1.305)  Loss:  1.0400 (1.6363)  Acc@1: 78.3019 (64.0780)  Acc@5: 92.6887 (85.5060)
2025-05-31 16:03:34,910 - train - INFO - Train: 72 [   0/156 (  0%)]  Loss:  2.923559 (2.9236)  Time: 6.359s, 1288.27/s  (6.359s, 1288.27/s)  LR: 1.821e-03  Data: 5.830 (5.830)
2025-05-31 16:04:32,701 - train - INFO - Train: 72 [  50/156 ( 32%)]  Loss:  2.941177 (2.9324)  Time: 0.529s, 15498.84/s  (1.258s, 6512.81/s)  LR: 1.821e-03  Data: 0.000 (0.513)
2025-05-31 16:05:33,853 - train - INFO - Train: 72 [ 100/156 ( 65%)]  Loss:  2.911956 (2.9256)  Time: 3.438s, 2382.80/s  (1.241s, 6603.26/s)  LR: 1.821e-03  Data: 1.073 (0.437)
2025-05-31 16:06:32,099 - train - INFO - Train: 72 [ 150/156 ( 97%)]  Loss:  2.916682 (2.9233)  Time: 0.526s, 15577.70/s  (1.216s, 6739.45/s)  LR: 1.821e-03  Data: 0.000 (0.328)
2025-05-31 16:06:36,941 - train - INFO - Train: 72 [ 155/156 (100%)]  Loss:  2.899617 (2.9186)  Time: 0.523s, 15659.00/s  (1.208s, 6783.71/s)  LR: 1.821e-03  Data: 0.000 (0.317)
2025-05-31 16:06:43,507 - train - INFO - Test: [   0/48]  Time: 6.272 (6.272)  Loss:  0.9888 (0.9888)  Acc@1: 80.5664 (80.5664)  Acc@5: 92.4805 (92.4805)
2025-05-31 16:07:41,798 - train - INFO - Test: [  48/48]  Time: 2.059 (1.318)  Loss:  0.9692 (1.6368)  Acc@1: 79.0094 (63.5380)  Acc@5: 93.7500 (85.3500)
2025-05-31 16:07:47,987 - train - INFO - Train: 73 [   0/156 (  0%)]  Loss:  2.938884 (2.9389)  Time: 5.794s, 1413.92/s  (5.794s, 1413.92/s)  LR: 1.702e-03  Data: 5.258 (5.258)
2025-05-31 16:08:46,147 - train - INFO - Train: 73 [  50/156 ( 32%)]  Loss:  2.938461 (2.9387)  Time: 0.576s, 14215.30/s  (1.254s, 6532.94/s)  LR: 1.702e-03  Data: 0.000 (0.575)
2025-05-31 16:09:47,413 - train - INFO - Train: 73 [ 100/156 ( 65%)]  Loss:  2.944506 (2.9406)  Time: 3.224s, 2541.05/s  (1.240s, 6607.79/s)  LR: 1.702e-03  Data: 0.000 (0.340)
2025-05-31 16:10:45,316 - train - INFO - Train: 73 [ 150/156 ( 97%)]  Loss:  2.926205 (2.9370)  Time: 0.526s, 15579.34/s  (1.213s, 6755.22/s)  LR: 1.702e-03  Data: 0.000 (0.228)
2025-05-31 16:10:50,270 - train - INFO - Train: 73 [ 155/156 (100%)]  Loss:  2.904295 (2.9305)  Time: 0.522s, 15689.29/s  (1.206s, 6795.12/s)  LR: 1.702e-03  Data: 0.000 (0.221)
2025-05-31 16:10:57,123 - train - INFO - Test: [   0/48]  Time: 6.560 (6.560)  Loss:  1.1260 (1.1260)  Acc@1: 78.4180 (78.4180)  Acc@5: 91.2109 (91.2109)
2025-05-31 16:11:54,218 - train - INFO - Test: [  48/48]  Time: 2.201 (1.299)  Loss:  1.1025 (1.7144)  Acc@1: 77.3585 (62.2100)  Acc@5: 92.3349 (84.4520)
2025-05-31 16:12:01,097 - train - INFO - Train: 74 [   0/156 (  0%)]  Loss:  2.911373 (2.9114)  Time: 6.505s, 1259.41/s  (6.505s, 1259.41/s)  LR: 1.586e-03  Data: 5.986 (5.986)
2025-05-31 16:12:58,994 - train - INFO - Train: 74 [  50/156 ( 32%)]  Loss:  2.924238 (2.9178)  Time: 0.526s, 15561.68/s  (1.263s, 6487.44/s)  LR: 1.586e-03  Data: 0.000 (0.738)
2025-05-31 16:13:58,476 - train - INFO - Train: 74 [ 100/156 ( 65%)]  Loss:  2.923796 (2.9198)  Time: 3.062s, 2675.12/s  (1.227s, 6678.95/s)  LR: 1.586e-03  Data: 2.338 (0.691)
2025-05-31 16:14:56,402 - train - INFO - Train: 74 [ 150/156 ( 97%)]  Loss:  2.945600 (2.9263)  Time: 0.526s, 15564.80/s  (1.204s, 6803.98/s)  LR: 1.586e-03  Data: 0.000 (0.629)
2025-05-31 16:15:01,514 - train - INFO - Train: 74 [ 155/156 (100%)]  Loss:  2.888732 (2.9187)  Time: 0.523s, 15656.44/s  (1.198s, 6837.06/s)  LR: 1.586e-03  Data: 0.000 (0.620)
2025-05-31 16:15:08,329 - train - INFO - Test: [   0/48]  Time: 6.496 (6.496)  Loss:  1.0010 (1.0010)  Acc@1: 80.4688 (80.4688)  Acc@5: 92.8711 (92.8711)
2025-05-31 16:16:05,994 - train - INFO - Test: [  48/48]  Time: 2.143 (1.309)  Loss:  1.0576 (1.6534)  Acc@1: 79.0094 (63.8920)  Acc@5: 92.2170 (85.6160)
2025-05-31 16:16:12,940 - train - INFO - Train: 75 [   0/156 (  0%)]  Loss:  2.935608 (2.9356)  Time: 6.577s, 1245.54/s  (6.577s, 1245.54/s)  LR: 1.473e-03  Data: 6.052 (6.052)
2025-05-31 16:17:11,051 - train - INFO - Train: 75 [  50/156 ( 32%)]  Loss:  2.916507 (2.9261)  Time: 0.527s, 15557.82/s  (1.268s, 6458.75/s)  LR: 1.473e-03  Data: 0.000 (0.671)
2025-05-31 16:18:11,468 - train - INFO - Train: 75 [ 100/156 ( 65%)]  Loss:  2.926758 (2.9263)  Time: 2.546s, 3217.78/s  (1.239s, 6613.71/s)  LR: 1.473e-03  Data: 1.465 (0.568)
2025-05-31 16:19:10,113 - train - INFO - Train: 75 [ 150/156 ( 97%)]  Loss:  2.945371 (2.9311)  Time: 0.528s, 15522.21/s  (1.217s, 6732.04/s)  LR: 1.473e-03  Data: 0.000 (0.466)
2025-05-31 16:19:15,402 - train - INFO - Train: 75 [ 155/156 (100%)]  Loss:  2.901217 (2.9251)  Time: 0.523s, 15649.90/s  (1.212s, 6760.43/s)  LR: 1.473e-03  Data: 0.000 (0.456)
2025-05-31 16:19:21,906 - train - INFO - Test: [   0/48]  Time: 6.207 (6.207)  Loss:  1.2646 (1.2646)  Acc@1: 77.4414 (77.4414)  Acc@5: 91.7969 (91.7969)
2025-05-31 16:20:20,091 - train - INFO - Test: [  48/48]  Time: 3.020 (1.314)  Loss:  1.1084 (1.8063)  Acc@1: 78.4198 (61.9520)  Acc@5: 94.1038 (84.2740)
2025-05-31 16:20:26,834 - train - INFO - Train: 76 [   0/156 (  0%)]  Loss:  2.941097 (2.9411)  Time: 6.379s, 1284.28/s  (6.379s, 1284.28/s)  LR: 1.364e-03  Data: 5.854 (5.854)
2025-05-31 16:21:24,959 - train - INFO - Train: 76 [  50/156 ( 32%)]  Loss:  2.920346 (2.9307)  Time: 0.532s, 15409.05/s  (1.265s, 6477.17/s)  LR: 1.364e-03  Data: 0.000 (0.603)
2025-05-31 16:22:25,955 - train - INFO - Train: 76 [ 100/156 ( 65%)]  Loss:  2.942338 (2.9346)  Time: 3.100s, 2642.91/s  (1.243s, 6592.94/s)  LR: 1.364e-03  Data: 0.654 (0.424)
2025-05-31 16:23:23,743 - train - INFO - Train: 76 [ 150/156 ( 97%)]  Loss:  2.926380 (2.9325)  Time: 0.527s, 15541.48/s  (1.214s, 6749.06/s)  LR: 1.364e-03  Data: 0.000 (0.312)
2025-05-31 16:23:28,867 - train - INFO - Train: 76 [ 155/156 (100%)]  Loss:  2.918832 (2.9298)  Time: 0.524s, 15639.97/s  (1.208s, 6782.96/s)  LR: 1.364e-03  Data: 0.000 (0.302)
2025-05-31 16:23:35,545 - train - INFO - Test: [   0/48]  Time: 6.391 (6.391)  Loss:  0.9966 (0.9966)  Acc@1: 80.2734 (80.2734)  Acc@5: 93.8477 (93.8477)
2025-05-31 16:24:33,564 - train - INFO - Test: [  48/48]  Time: 1.617 (1.314)  Loss:  1.0791 (1.6655)  Acc@1: 78.4198 (63.7560)  Acc@5: 93.0425 (85.3680)
2025-05-31 16:24:40,577 - train - INFO - Train: 77 [   0/156 (  0%)]  Loss:  2.931313 (2.9313)  Time: 6.638s, 1234.12/s  (6.638s, 1234.12/s)  LR: 1.258e-03  Data: 6.108 (6.108)
2025-05-31 16:25:38,719 - train - INFO - Train: 77 [  50/156 ( 32%)]  Loss:  2.898608 (2.9150)  Time: 0.526s, 15562.46/s  (1.270s, 6449.51/s)  LR: 1.258e-03  Data: 0.000 (0.602)
2025-05-31 16:26:38,496 - train - INFO - Train: 77 [ 100/156 ( 65%)]  Loss:  2.913246 (2.9144)  Time: 2.589s, 3163.85/s  (1.233s, 6642.77/s)  LR: 1.258e-03  Data: 1.451 (0.488)
2025-05-31 16:27:36,539 - train - INFO - Train: 77 [ 150/156 ( 97%)]  Loss:  2.924758 (2.9170)  Time: 0.527s, 15537.21/s  (1.209s, 6774.47/s)  LR: 1.258e-03  Data: 0.000 (0.451)
2025-05-31 16:27:41,492 - train - INFO - Train: 77 [ 155/156 (100%)]  Loss:  2.927794 (2.9191)  Time: 0.522s, 15704.24/s  (1.202s, 6813.99/s)  LR: 1.258e-03  Data: 0.000 (0.444)
2025-05-31 16:27:47,924 - train - INFO - Test: [   0/48]  Time: 6.152 (6.152)  Loss:  1.1104 (1.1104)  Acc@1: 77.5391 (77.5391)  Acc@5: 92.2852 (92.2852)
2025-05-31 16:28:45,743 - train - INFO - Test: [  48/48]  Time: 2.338 (1.306)  Loss:  1.1299 (1.6730)  Acc@1: 76.2972 (62.7900)  Acc@5: 90.9198 (84.8440)
2025-05-31 16:28:52,328 - train - INFO - Train: 78 [   0/156 (  0%)]  Loss:  2.952511 (2.9525)  Time: 6.211s, 1319.02/s  (6.211s, 1319.02/s)  LR: 1.156e-03  Data: 5.019 (5.019)
2025-05-31 16:29:50,797 - train - INFO - Train: 78 [  50/156 ( 32%)]  Loss:  2.926760 (2.9396)  Time: 0.526s, 15577.96/s  (1.268s, 6459.56/s)  LR: 1.156e-03  Data: 0.000 (0.340)
2025-05-31 16:30:51,151 - train - INFO - Train: 78 [ 100/156 ( 65%)]  Loss:  2.925220 (2.9348)  Time: 2.373s, 3452.80/s  (1.238s, 6617.50/s)  LR: 1.156e-03  Data: 0.000 (0.172)
2025-05-31 16:31:49,980 - train - INFO - Train: 78 [ 150/156 ( 97%)]  Loss:  2.929684 (2.9335)  Time: 0.526s, 15583.15/s  (1.218s, 6727.97/s)  LR: 1.156e-03  Data: 0.000 (0.115)
2025-05-31 16:31:55,052 - train - INFO - Train: 78 [ 155/156 (100%)]  Loss:  2.930782 (2.9330)  Time: 0.522s, 15703.08/s  (1.211s, 6764.18/s)  LR: 1.156e-03  Data: 0.000 (0.111)
2025-05-31 16:32:01,863 - train - INFO - Test: [   0/48]  Time: 6.517 (6.517)  Loss:  1.0186 (1.0186)  Acc@1: 79.6875 (79.6875)  Acc@5: 92.5781 (92.5781)
2025-05-31 16:33:00,551 - train - INFO - Test: [  48/48]  Time: 2.472 (1.331)  Loss:  1.0381 (1.6238)  Acc@1: 77.5943 (64.1300)  Acc@5: 93.0425 (85.6000)
2025-05-31 16:33:07,776 - train - INFO - Train: 79 [   0/156 (  0%)]  Loss:  2.906768 (2.9068)  Time: 6.691s, 1224.34/s  (6.691s, 1224.34/s)  LR: 1.058e-03  Data: 6.171 (6.171)
2025-05-31 16:34:07,481 - train - INFO - Train: 79 [  50/156 ( 32%)]  Loss:  2.945536 (2.9262)  Time: 0.531s, 15426.72/s  (1.302s, 6292.58/s)  LR: 1.058e-03  Data: 0.000 (0.373)
2025-05-31 16:35:09,511 - train - INFO - Train: 79 [ 100/156 ( 65%)]  Loss:  2.922409 (2.9249)  Time: 3.132s, 2615.28/s  (1.271s, 6442.86/s)  LR: 1.058e-03  Data: 0.000 (0.189)
2025-05-31 16:36:08,271 - train - INFO - Train: 79 [ 150/156 ( 97%)]  Loss:  2.935687 (2.9276)  Time: 0.533s, 15372.85/s  (1.240s, 6608.73/s)  LR: 1.058e-03  Data: 0.000 (0.126)
2025-05-31 16:36:13,291 - train - INFO - Train: 79 [ 155/156 (100%)]  Loss:  2.913262 (2.9247)  Time: 0.528s, 15513.15/s  (1.232s, 6649.39/s)  LR: 1.058e-03  Data: 0.000 (0.122)
2025-05-31 16:36:19,979 - train - INFO - Test: [   0/48]  Time: 6.397 (6.397)  Loss:  1.1436 (1.1436)  Acc@1: 77.4414 (77.4414)  Acc@5: 92.1875 (92.1875)
2025-05-31 16:37:18,883 - train - INFO - Test: [  48/48]  Time: 2.274 (1.333)  Loss:  1.1025 (1.7023)  Acc@1: 78.8915 (62.9000)  Acc@5: 92.9245 (84.8940)
2025-05-31 16:37:26,018 - train - INFO - Train: 80 [   0/156 (  0%)]  Loss:  2.963552 (2.9636)  Time: 6.751s, 1213.39/s  (6.751s, 1213.39/s)  LR: 9.640e-04  Data: 6.222 (6.222)
2025-05-31 16:38:24,626 - train - INFO - Train: 80 [  50/156 ( 32%)]  Loss:  2.913737 (2.9386)  Time: 0.527s, 15542.36/s  (1.281s, 6392.59/s)  LR: 9.640e-04  Data: 0.000 (0.543)
2025-05-31 16:39:25,481 - train - INFO - Train: 80 [ 100/156 ( 65%)]  Loss:  2.926090 (2.9345)  Time: 3.162s, 2590.57/s  (1.250s, 6555.69/s)  LR: 9.640e-04  Data: 0.568 (0.416)
2025-05-31 16:40:23,773 - train - INFO - Train: 80 [ 150/156 ( 97%)]  Loss:  2.908659 (2.9280)  Time: 0.533s, 15371.04/s  (1.222s, 6704.55/s)  LR: 9.640e-04  Data: 0.000 (0.290)
2025-05-31 16:40:28,746 - train - INFO - Train: 80 [ 155/156 (100%)]  Loss:  2.939472 (2.9303)  Time: 0.522s, 15699.46/s  (1.215s, 6744.81/s)  LR: 9.640e-04  Data: 0.000 (0.282)
2025-05-31 16:40:35,810 - train - INFO - Test: [   0/48]  Time: 6.766 (6.766)  Loss:  1.1562 (1.1562)  Acc@1: 77.7344 (77.7344)  Acc@5: 92.1875 (92.1875)
2025-05-31 16:41:33,988 - train - INFO - Test: [  48/48]  Time: 1.831 (1.325)  Loss:  0.9297 (1.6940)  Acc@1: 81.2500 (63.1940)  Acc@5: 94.2217 (85.0480)
2025-05-31 16:41:40,838 - train - INFO - Train: 81 [   0/156 (  0%)]  Loss:  2.910267 (2.9103)  Time: 6.470s, 1266.06/s  (6.470s, 1266.06/s)  LR: 8.737e-04  Data: 5.315 (5.315)
2025-05-31 16:42:39,302 - train - INFO - Train: 81 [  50/156 ( 32%)]  Loss:  2.879054 (2.8947)  Time: 0.528s, 15501.18/s  (1.273s, 6434.17/s)  LR: 8.737e-04  Data: 0.000 (0.261)
2025-05-31 16:43:39,787 - train - INFO - Train: 81 [ 100/156 ( 65%)]  Loss:  2.941969 (2.9104)  Time: 1.819s, 4502.40/s  (1.242s, 6597.12/s)  LR: 8.737e-04  Data: 0.000 (0.132)
2025-05-31 16:44:37,867 - train - INFO - Train: 81 [ 150/156 ( 97%)]  Loss:  2.915597 (2.9117)  Time: 0.527s, 15555.18/s  (1.215s, 6741.27/s)  LR: 8.737e-04  Data: 0.000 (0.088)
2025-05-31 16:44:44,146 - train - INFO - Train: 81 [ 155/156 (100%)]  Loss:  2.931051 (2.9156)  Time: 1.884s, 4349.00/s  (1.217s, 6734.04/s)  LR: 8.737e-04  Data: 0.000 (0.086)
2025-05-31 16:44:50,910 - train - INFO - Test: [   0/48]  Time: 6.462 (6.462)  Loss:  1.0830 (1.0830)  Acc@1: 78.5156 (78.5156)  Acc@5: 93.5547 (93.5547)
2025-05-31 16:45:49,571 - train - INFO - Test: [  48/48]  Time: 2.257 (1.329)  Loss:  1.1992 (1.7162)  Acc@1: 78.0660 (63.2780)  Acc@5: 92.6887 (84.9880)
2025-05-31 16:45:56,496 - train - INFO - Train: 82 [   0/156 (  0%)]  Loss:  2.907126 (2.9071)  Time: 6.549s, 1250.91/s  (6.549s, 1250.91/s)  LR: 7.876e-04  Data: 5.287 (5.287)
2025-05-31 16:46:54,488 - train - INFO - Train: 82 [  50/156 ( 32%)]  Loss:  2.903575 (2.9054)  Time: 0.528s, 15509.70/s  (1.265s, 6473.38/s)  LR: 7.876e-04  Data: 0.000 (0.576)
2025-05-31 16:47:55,493 - train - INFO - Train: 82 [ 100/156 ( 65%)]  Loss:  2.908571 (2.9064)  Time: 3.212s, 2550.38/s  (1.243s, 6590.45/s)  LR: 7.876e-04  Data: 0.789 (0.442)
2025-05-31 16:48:53,486 - train - INFO - Train: 82 [ 150/156 ( 97%)]  Loss:  2.932183 (2.9129)  Time: 0.534s, 15354.54/s  (1.215s, 6739.78/s)  LR: 7.876e-04  Data: 0.000 (0.364)
2025-05-31 16:48:58,324 - train - INFO - Train: 82 [ 155/156 (100%)]  Loss:  2.936975 (2.9177)  Time: 0.523s, 15669.82/s  (1.208s, 6784.16/s)  LR: 7.876e-04  Data: 0.000 (0.357)
2025-05-31 16:49:05,074 - train - INFO - Test: [   0/48]  Time: 6.468 (6.468)  Loss:  1.0850 (1.0850)  Acc@1: 76.3672 (76.3672)  Acc@5: 92.8711 (92.8711)
2025-05-31 16:50:03,606 - train - INFO - Test: [  48/48]  Time: 2.067 (1.327)  Loss:  1.0781 (1.7195)  Acc@1: 77.1226 (61.9360)  Acc@5: 91.5094 (84.3020)
2025-05-31 16:50:10,529 - train - INFO - Train: 83 [   0/156 (  0%)]  Loss:  2.934062 (2.9341)  Time: 6.547s, 1251.20/s  (6.547s, 1251.20/s)  LR: 7.056e-04  Data: 5.889 (5.889)
2025-05-31 16:51:08,841 - train - INFO - Train: 83 [  50/156 ( 32%)]  Loss:  2.944916 (2.9395)  Time: 0.527s, 15554.21/s  (1.272s, 6441.69/s)  LR: 7.056e-04  Data: 0.000 (0.561)
2025-05-31 16:52:09,439 - train - INFO - Train: 83 [ 100/156 ( 65%)]  Loss:  2.952406 (2.9438)  Time: 3.049s, 2687.14/s  (1.242s, 6595.16/s)  LR: 7.056e-04  Data: 0.065 (0.365)
2025-05-31 16:53:07,562 - train - INFO - Train: 83 [ 150/156 ( 97%)]  Loss:  2.953078 (2.9461)  Time: 0.525s, 15612.89/s  (1.216s, 6738.29/s)  LR: 7.056e-04  Data: 0.000 (0.244)
2025-05-31 16:53:12,758 - train - INFO - Train: 83 [ 155/156 (100%)]  Loss:  2.917972 (2.9405)  Time: 0.522s, 15695.70/s  (1.210s, 6769.81/s)  LR: 7.056e-04  Data: 0.000 (0.237)
2025-05-31 16:53:19,751 - train - INFO - Test: [   0/48]  Time: 6.705 (6.705)  Loss:  1.0039 (1.0039)  Acc@1: 79.6875 (79.6875)  Acc@5: 92.2852 (92.2852)
2025-05-31 16:54:17,117 - train - INFO - Test: [  48/48]  Time: 2.260 (1.308)  Loss:  0.9326 (1.5992)  Acc@1: 80.5425 (63.9040)  Acc@5: 93.5142 (85.4560)
2025-05-31 16:54:24,125 - train - INFO - Train: 84 [   0/156 (  0%)]  Loss:  2.975629 (2.9756)  Time: 6.642s, 1233.38/s  (6.642s, 1233.38/s)  LR: 6.278e-04  Data: 6.111 (6.111)
2025-05-31 16:55:22,396 - train - INFO - Train: 84 [  50/156 ( 32%)]  Loss:  2.935310 (2.9555)  Time: 0.528s, 15517.32/s  (1.273s, 6436.31/s)  LR: 6.278e-04  Data: 0.000 (0.747)
2025-05-31 16:56:22,910 - train - INFO - Train: 84 [ 100/156 ( 65%)]  Loss:  2.942423 (2.9511)  Time: 3.132s, 2615.77/s  (1.242s, 6596.73/s)  LR: 6.278e-04  Data: 2.610 (0.713)
2025-05-31 16:57:20,587 - train - INFO - Train: 84 [ 150/156 ( 97%)]  Loss:  2.915700 (2.9423)  Time: 0.533s, 15360.85/s  (1.213s, 6755.84/s)  LR: 6.278e-04  Data: 0.000 (0.682)
2025-05-31 16:57:25,633 - train - INFO - Train: 84 [ 155/156 (100%)]  Loss:  2.905073 (2.9348)  Time: 0.529s, 15484.05/s  (1.206s, 6792.38/s)  LR: 6.278e-04  Data: 0.000 (0.676)
2025-05-31 16:57:32,690 - train - INFO - Test: [   0/48]  Time: 6.765 (6.765)  Loss:  1.0273 (1.0273)  Acc@1: 77.2461 (77.2461)  Acc@5: 92.3828 (92.3828)
2025-05-31 16:58:31,131 - train - INFO - Test: [  48/48]  Time: 2.303 (1.331)  Loss:  1.0303 (1.5899)  Acc@1: 77.9481 (63.9260)  Acc@5: 92.9245 (85.3120)
2025-05-31 16:58:38,120 - train - INFO - Train: 85 [   0/156 (  0%)]  Loss:  2.910620 (2.9106)  Time: 6.617s, 1238.10/s  (6.617s, 1238.10/s)  LR: 5.544e-04  Data: 6.096 (6.096)
2025-05-31 16:59:37,192 - train - INFO - Train: 85 [  50/156 ( 32%)]  Loss:  2.939402 (2.9250)  Time: 0.531s, 15439.23/s  (1.288s, 6360.29/s)  LR: 5.544e-04  Data: 0.000 (0.716)
2025-05-31 17:00:38,604 - train - INFO - Train: 85 [ 100/156 ( 65%)]  Loss:  2.916109 (2.9220)  Time: 1.695s, 4832.73/s  (1.258s, 6509.86/s)  LR: 5.544e-04  Data: 0.753 (0.591)
2025-05-31 17:01:38,641 - train - INFO - Train: 85 [ 150/156 ( 97%)]  Loss:  2.936251 (2.9256)  Time: 0.528s, 15511.88/s  (1.239s, 6610.19/s)  LR: 5.544e-04  Data: 0.000 (0.453)
2025-05-31 17:01:43,630 - train - INFO - Train: 85 [ 155/156 (100%)]  Loss:  2.936872 (2.9279)  Time: 0.658s, 12451.05/s  (1.232s, 6651.81/s)  LR: 5.544e-04  Data: 0.146 (0.446)
2025-05-31 17:01:50,176 - train - INFO - Test: [   0/48]  Time: 6.216 (6.216)  Loss:  0.9849 (0.9849)  Acc@1: 80.2734 (80.2734)  Acc@5: 93.3594 (93.3594)
2025-05-31 17:02:48,473 - train - INFO - Test: [  48/48]  Time: 2.556 (1.317)  Loss:  1.0264 (1.6110)  Acc@1: 80.3066 (64.2800)  Acc@5: 92.9245 (85.8780)
2025-05-31 17:02:55,268 - train - INFO - Train: 86 [   0/156 (  0%)]  Loss:  2.911600 (2.9116)  Time: 6.258s, 1308.95/s  (6.258s, 1308.95/s)  LR: 4.854e-04  Data: 5.462 (5.462)
2025-05-31 17:03:56,130 - train - INFO - Train: 86 [  50/156 ( 32%)]  Loss:  2.883238 (2.8974)  Time: 0.690s, 11880.75/s  (1.316s, 6224.62/s)  LR: 4.854e-04  Data: 0.000 (0.595)
2025-05-31 17:04:57,056 - train - INFO - Train: 86 [ 100/156 ( 65%)]  Loss:  2.938595 (2.9111)  Time: 2.430s, 3371.84/s  (1.268s, 6461.80/s)  LR: 4.854e-04  Data: 1.229 (0.491)
2025-05-31 17:05:56,898 - train - INFO - Train: 86 [ 150/156 ( 97%)]  Loss:  2.926932 (2.9151)  Time: 0.528s, 15503.54/s  (1.244s, 6583.75/s)  LR: 4.854e-04  Data: 0.000 (0.370)
2025-05-31 17:06:02,046 - train - INFO - Train: 86 [ 155/156 (100%)]  Loss:  2.917124 (2.9155)  Time: 0.522s, 15690.88/s  (1.237s, 6620.39/s)  LR: 4.854e-04  Data: 0.000 (0.358)
2025-05-31 17:06:09,204 - train - INFO - Test: [   0/48]  Time: 6.803 (6.803)  Loss:  1.1475 (1.1475)  Acc@1: 77.3438 (77.3438)  Acc@5: 91.8945 (91.8945)
2025-05-31 17:07:08,700 - train - INFO - Test: [  48/48]  Time: 2.788 (1.353)  Loss:  1.1328 (1.7352)  Acc@1: 79.0094 (63.0020)  Acc@5: 92.5707 (84.9040)
2025-05-31 17:07:15,386 - train - INFO - Train: 87 [   0/156 (  0%)]  Loss:  2.900301 (2.9003)  Time: 6.322s, 1295.78/s  (6.322s, 1295.78/s)  LR: 4.208e-04  Data: 5.146 (5.146)
2025-05-31 17:08:14,150 - train - INFO - Train: 87 [  50/156 ( 32%)]  Loss:  2.911315 (2.9058)  Time: 0.528s, 15519.32/s  (1.276s, 6419.16/s)  LR: 4.208e-04  Data: 0.000 (0.273)
2025-05-31 17:09:15,071 - train - INFO - Train: 87 [ 100/156 ( 65%)]  Loss:  2.911449 (2.9077)  Time: 3.130s, 2617.23/s  (1.248s, 6566.34/s)  LR: 4.208e-04  Data: 0.000 (0.138)
2025-05-31 17:10:13,291 - train - INFO - Train: 87 [ 150/156 ( 97%)]  Loss:  2.912782 (2.9090)  Time: 0.525s, 15601.29/s  (1.220s, 6714.63/s)  LR: 4.208e-04  Data: 0.000 (0.093)
2025-05-31 17:10:18,180 - train - INFO - Train: 87 [ 155/156 (100%)]  Loss:  2.937953 (2.9148)  Time: 0.521s, 15716.34/s  (1.212s, 6757.67/s)  LR: 4.208e-04  Data: 0.000 (0.090)
2025-05-31 17:10:25,148 - train - INFO - Test: [   0/48]  Time: 6.682 (6.682)  Loss:  1.1729 (1.1729)  Acc@1: 77.4414 (77.4414)  Acc@5: 91.0156 (91.0156)
2025-05-31 17:11:23,202 - train - INFO - Test: [  48/48]  Time: 2.692 (1.321)  Loss:  1.0479 (1.6426)  Acc@1: 77.8302 (63.1760)  Acc@5: 92.2170 (85.2160)
2025-05-31 17:11:30,348 - train - INFO - Train: 88 [   0/156 (  0%)]  Loss:  2.946723 (2.9467)  Time: 6.779s, 1208.43/s  (6.779s, 1208.43/s)  LR: 3.608e-04  Data: 5.694 (5.694)
2025-05-31 17:12:29,924 - train - INFO - Train: 88 [  50/156 ( 32%)]  Loss:  2.909557 (2.9281)  Time: 0.529s, 15488.17/s  (1.301s, 6296.43/s)  LR: 3.608e-04  Data: 0.000 (0.184)
2025-05-31 17:13:31,249 - train - INFO - Train: 88 [ 100/156 ( 65%)]  Loss:  2.920940 (2.9257)  Time: 2.910s, 2814.91/s  (1.264s, 6480.43/s)  LR: 3.608e-04  Data: 0.000 (0.093)
2025-05-31 17:14:30,876 - train - INFO - Train: 88 [ 150/156 ( 97%)]  Loss:  2.930856 (2.9270)  Time: 0.532s, 15398.85/s  (1.240s, 6604.33/s)  LR: 3.608e-04  Data: 0.000 (0.062)
2025-05-31 17:14:35,697 - train - INFO - Train: 88 [ 155/156 (100%)]  Loss:  2.942008 (2.9300)  Time: 0.530s, 15453.62/s  (1.232s, 6651.83/s)  LR: 3.608e-04  Data: 0.000 (0.060)
2025-05-31 17:14:42,578 - train - INFO - Test: [   0/48]  Time: 6.636 (6.636)  Loss:  0.9912 (0.9912)  Acc@1: 79.7852 (79.7852)  Acc@5: 93.7500 (93.7500)
2025-05-31 17:15:42,655 - train - INFO - Test: [  48/48]  Time: 1.479 (1.361)  Loss:  1.0273 (1.6431)  Acc@1: 79.2453 (63.5960)  Acc@5: 93.6321 (85.2780)
2025-05-31 17:15:50,111 - train - INFO - Train: 89 [   0/156 (  0%)]  Loss:  2.912086 (2.9121)  Time: 7.022s, 1166.67/s  (7.022s, 1166.67/s)  LR: 3.053e-04  Data: 5.721 (5.721)
2025-05-31 17:16:48,601 - train - INFO - Train: 89 [  50/156 ( 32%)]  Loss:  2.906474 (2.9093)  Time: 0.539s, 15186.98/s  (1.285s, 6377.46/s)  LR: 3.053e-04  Data: 0.000 (0.259)
2025-05-31 17:17:50,178 - train - INFO - Train: 89 [ 100/156 ( 65%)]  Loss:  2.927308 (2.9153)  Time: 3.468s, 2362.01/s  (1.258s, 6510.49/s)  LR: 3.053e-04  Data: 0.000 (0.140)
2025-05-31 17:18:49,407 - train - INFO - Train: 89 [ 150/156 ( 97%)]  Loss:  2.896936 (2.9107)  Time: 0.525s, 15593.93/s  (1.234s, 6639.28/s)  LR: 3.053e-04  Data: 0.000 (0.094)
2025-05-31 17:18:54,258 - train - INFO - Train: 89 [ 155/156 (100%)]  Loss:  2.893969 (2.9074)  Time: 0.523s, 15673.05/s  (1.225s, 6685.07/s)  LR: 3.053e-04  Data: 0.000 (0.091)
2025-05-31 17:19:01,239 - train - INFO - Test: [   0/48]  Time: 6.705 (6.705)  Loss:  1.0303 (1.0303)  Acc@1: 80.4688 (80.4688)  Acc@5: 93.8477 (93.8477)
2025-05-31 17:19:59,140 - train - INFO - Test: [  48/48]  Time: 2.405 (1.318)  Loss:  1.0791 (1.7027)  Acc@1: 78.5377 (63.5320)  Acc@5: 93.5142 (85.0820)
2025-05-31 17:20:06,388 - train - INFO - Train: 90 [   0/156 (  0%)]  Loss:  2.900195 (2.9002)  Time: 6.881s, 1190.46/s  (6.881s, 1190.46/s)  LR: 2.545e-04  Data: 6.353 (6.353)
2025-05-31 17:21:07,926 - train - INFO - Train: 90 [  50/156 ( 32%)]  Loss:  2.913068 (2.9066)  Time: 0.528s, 15525.31/s  (1.342s, 6106.42/s)  LR: 2.545e-04  Data: 0.000 (0.342)
2025-05-31 17:22:08,760 - train - INFO - Train: 90 [ 100/156 ( 65%)]  Loss:  2.908476 (2.9072)  Time: 2.279s, 3594.13/s  (1.280s, 6401.47/s)  LR: 2.545e-04  Data: 0.000 (0.173)
2025-05-31 17:23:10,143 - train - INFO - Train: 90 [ 150/156 ( 97%)]  Loss:  2.891474 (2.9033)  Time: 2.726s, 3005.00/s  (1.262s, 6488.90/s)  LR: 2.545e-04  Data: 0.000 (0.116)
2025-05-31 17:23:15,063 - train - INFO - Train: 90 [ 155/156 (100%)]  Loss:  2.911910 (2.9050)  Time: 0.528s, 15515.55/s  (1.254s, 6535.15/s)  LR: 2.545e-04  Data: 0.000 (0.112)
2025-05-31 17:23:21,993 - train - INFO - Test: [   0/48]  Time: 6.618 (6.618)  Loss:  1.0010 (1.0010)  Acc@1: 79.9805 (79.9805)  Acc@5: 93.5547 (93.5547)
2025-05-31 17:24:20,553 - train - INFO - Test: [  48/48]  Time: 2.717 (1.330)  Loss:  1.0303 (1.6615)  Acc@1: 79.7170 (64.0960)  Acc@5: 93.5142 (85.6840)
2025-05-31 17:24:27,383 - train - INFO - Train: 91 [   0/156 (  0%)]  Loss:  2.936776 (2.9368)  Time: 6.456s, 1268.89/s  (6.456s, 1268.89/s)  LR: 2.083e-04  Data: 5.319 (5.319)
2025-05-31 17:25:27,304 - train - INFO - Train: 91 [  50/156 ( 32%)]  Loss:  2.879692 (2.9082)  Time: 0.527s, 15539.55/s  (1.301s, 6294.33/s)  LR: 2.083e-04  Data: 0.000 (0.434)
2025-05-31 17:26:28,286 - train - INFO - Train: 91 [ 100/156 ( 65%)]  Loss:  2.915556 (2.9107)  Time: 3.037s, 2697.24/s  (1.261s, 6496.73/s)  LR: 2.083e-04  Data: 0.000 (0.262)
2025-05-31 17:27:26,502 - train - INFO - Train: 91 [ 150/156 ( 97%)]  Loss:  2.904017 (2.9090)  Time: 0.527s, 15531.15/s  (1.229s, 6665.91/s)  LR: 2.083e-04  Data: 0.000 (0.184)
2025-05-31 17:27:31,422 - train - INFO - Train: 91 [ 155/156 (100%)]  Loss:  2.924268 (2.9121)  Time: 0.526s, 15570.20/s  (1.221s, 6708.76/s)  LR: 2.083e-04  Data: 0.000 (0.182)
2025-05-31 17:27:38,393 - train - INFO - Test: [   0/48]  Time: 6.677 (6.677)  Loss:  1.0557 (1.0557)  Acc@1: 77.8320 (77.8320)  Acc@5: 92.5781 (92.5781)
2025-05-31 17:28:36,826 - train - INFO - Test: [  48/48]  Time: 2.521 (1.329)  Loss:  0.9971 (1.6279)  Acc@1: 79.8349 (63.7780)  Acc@5: 92.9245 (85.4580)
2025-05-31 17:28:43,765 - train - INFO - Train: 92 [   0/156 (  0%)]  Loss:  2.924172 (2.9242)  Time: 6.510s, 1258.37/s  (6.510s, 1258.37/s)  LR: 1.669e-04  Data: 5.940 (5.940)
2025-05-31 17:29:42,704 - train - INFO - Train: 92 [  50/156 ( 32%)]  Loss:  2.899864 (2.9120)  Time: 1.461s, 5608.13/s  (1.283s, 6383.59/s)  LR: 1.669e-04  Data: 0.000 (0.564)
2025-05-31 17:30:43,293 - train - INFO - Train: 92 [ 100/156 ( 65%)]  Loss:  2.925807 (2.9166)  Time: 1.779s, 4605.80/s  (1.248s, 6564.79/s)  LR: 1.669e-04  Data: 0.000 (0.288)
2025-05-31 17:31:42,328 - train - INFO - Train: 92 [ 150/156 ( 97%)]  Loss:  2.913578 (2.9159)  Time: 1.114s, 7353.13/s  (1.226s, 6683.94/s)  LR: 1.669e-04  Data: 0.000 (0.193)
2025-05-31 17:31:47,383 - train - INFO - Train: 92 [ 155/156 (100%)]  Loss:  2.894082 (2.9115)  Time: 0.522s, 15700.77/s  (1.219s, 6721.69/s)  LR: 1.669e-04  Data: 0.000 (0.187)
2025-05-31 17:31:54,114 - train - INFO - Test: [   0/48]  Time: 6.466 (6.466)  Loss:  1.1143 (1.1143)  Acc@1: 78.5156 (78.5156)  Acc@5: 92.5781 (92.5781)
2025-05-31 17:32:52,241 - train - INFO - Test: [  48/48]  Time: 2.661 (1.318)  Loss:  1.0225 (1.6496)  Acc@1: 78.1840 (64.2460)  Acc@5: 93.3962 (85.6140)
2025-05-31 17:32:59,291 - train - INFO - Train: 93 [   0/156 (  0%)]  Loss:  2.900620 (2.9006)  Time: 6.680s, 1226.35/s  (6.680s, 1226.35/s)  LR: 1.303e-04  Data: 6.163 (6.163)
2025-05-31 17:33:57,203 - train - INFO - Train: 93 [  50/156 ( 32%)]  Loss:  2.894534 (2.8976)  Time: 0.533s, 15358.76/s  (1.266s, 6468.34/s)  LR: 1.303e-04  Data: 0.000 (0.708)
2025-05-31 17:34:57,347 - train - INFO - Train: 93 [ 100/156 ( 65%)]  Loss:  2.925847 (2.9070)  Time: 3.156s, 2595.86/s  (1.235s, 6633.27/s)  LR: 1.303e-04  Data: 1.580 (0.588)
2025-05-31 17:35:55,271 - train - INFO - Train: 93 [ 150/156 ( 97%)]  Loss:  2.910934 (2.9080)  Time: 0.527s, 15538.48/s  (1.210s, 6772.24/s)  LR: 1.303e-04  Data: 0.000 (0.446)
2025-05-31 17:36:00,019 - train - INFO - Train: 93 [ 155/156 (100%)]  Loss:  2.916321 (2.9097)  Time: 0.524s, 15640.04/s  (1.201s, 6819.28/s)  LR: 1.303e-04  Data: 0.000 (0.432)
2025-05-31 17:36:06,653 - train - INFO - Test: [   0/48]  Time: 6.346 (6.346)  Loss:  0.9331 (0.9331)  Acc@1: 79.8828 (79.8828)  Acc@5: 93.8477 (93.8477)
2025-05-31 17:37:04,421 - train - INFO - Test: [  48/48]  Time: 2.100 (1.308)  Loss:  0.9458 (1.5612)  Acc@1: 78.7736 (64.5960)  Acc@5: 92.6887 (86.0040)
2025-05-31 17:37:11,423 - train - INFO - Train: 94 [   0/156 (  0%)]  Loss:  2.902799 (2.9028)  Time: 6.464s, 1267.42/s  (6.464s, 1267.42/s)  LR: 9.848e-05  Data: 5.938 (5.938)
2025-05-31 17:38:09,829 - train - INFO - Train: 94 [  50/156 ( 32%)]  Loss:  2.947711 (2.9253)  Time: 0.527s, 15557.83/s  (1.272s, 6440.66/s)  LR: 9.848e-05  Data: 0.000 (0.562)
2025-05-31 17:39:10,429 - train - INFO - Train: 94 [ 100/156 ( 65%)]  Loss:  2.919610 (2.9234)  Time: 1.952s, 4197.26/s  (1.242s, 6594.57/s)  LR: 9.848e-05  Data: 0.001 (0.306)
2025-05-31 17:40:10,032 - train - INFO - Train: 94 [ 150/156 ( 97%)]  Loss:  2.888333 (2.9146)  Time: 0.528s, 15518.34/s  (1.226s, 6683.98/s)  LR: 9.848e-05  Data: 0.000 (0.205)
2025-05-31 17:40:14,802 - train - INFO - Train: 94 [ 155/156 (100%)]  Loss:  2.965605 (2.9248)  Time: 0.522s, 15695.78/s  (1.217s, 6731.85/s)  LR: 9.848e-05  Data: 0.000 (0.198)
2025-05-31 17:40:21,580 - train - INFO - Test: [   0/48]  Time: 6.485 (6.485)  Loss:  0.9966 (0.9966)  Acc@1: 81.1523 (81.1523)  Acc@5: 93.0664 (93.0664)
2025-05-31 17:41:19,679 - train - INFO - Test: [  48/48]  Time: 2.649 (1.318)  Loss:  1.0234 (1.6343)  Acc@1: 79.5991 (64.5320)  Acc@5: 92.9245 (85.6240)
2025-05-31 17:41:26,306 - train - INFO - Train: 95 [   0/156 (  0%)]  Loss:  2.924215 (2.9242)  Time: 6.260s, 1308.55/s  (6.260s, 1308.55/s)  LR: 7.150e-05  Data: 5.299 (5.299)
2025-05-31 17:42:24,975 - train - INFO - Train: 95 [  50/156 ( 32%)]  Loss:  2.905560 (2.9149)  Time: 0.528s, 15523.95/s  (1.273s, 6434.67/s)  LR: 7.150e-05  Data: 0.000 (0.463)
2025-05-31 17:43:25,734 - train - INFO - Train: 95 [ 100/156 ( 65%)]  Loss:  2.883176 (2.9043)  Time: 3.242s, 2526.53/s  (1.244s, 6583.05/s)  LR: 7.150e-05  Data: 0.311 (0.336)
2025-05-31 17:44:23,496 - train - INFO - Train: 95 [ 150/156 ( 97%)]  Loss:  2.911813 (2.9062)  Time: 0.528s, 15506.40/s  (1.215s, 6743.07/s)  LR: 7.150e-05  Data: 0.000 (0.233)
2025-05-31 17:44:28,493 - train - INFO - Train: 95 [ 155/156 (100%)]  Loss:  2.929200 (2.9108)  Time: 0.523s, 15664.87/s  (1.208s, 6781.69/s)  LR: 7.150e-05  Data: 0.000 (0.225)
2025-05-31 17:44:35,144 - train - INFO - Test: [   0/48]  Time: 6.362 (6.362)  Loss:  1.0869 (1.0869)  Acc@1: 77.7344 (77.7344)  Acc@5: 91.2109 (91.2109)
2025-05-31 17:45:33,320 - train - INFO - Test: [  48/48]  Time: 0.926 (1.317)  Loss:  1.0625 (1.6182)  Acc@1: 77.9481 (64.0460)  Acc@5: 92.0991 (85.4280)
2025-05-31 17:45:39,820 - train - INFO - Train: 96 [   0/156 (  0%)]  Loss:  2.931508 (2.9315)  Time: 6.129s, 1336.69/s  (6.129s, 1336.69/s)  LR: 4.939e-05  Data: 5.275 (5.275)
2025-05-31 17:46:38,663 - train - INFO - Train: 96 [  50/156 ( 32%)]  Loss:  2.919773 (2.9256)  Time: 0.526s, 15580.82/s  (1.274s, 6430.65/s)  LR: 4.939e-05  Data: 0.000 (0.619)
2025-05-31 17:47:39,164 - train - INFO - Train: 96 [ 100/156 ( 65%)]  Loss:  2.881368 (2.9109)  Time: 3.107s, 2636.42/s  (1.242s, 6594.40/s)  LR: 4.939e-05  Data: 2.590 (0.643)
2025-05-31 17:48:36,791 - train - INFO - Train: 96 [ 150/156 ( 97%)]  Loss:  2.930123 (2.9157)  Time: 0.783s, 10459.79/s  (1.213s, 6756.03/s)  LR: 4.939e-05  Data: 0.000 (0.617)
2025-05-31 17:48:41,855 - train - INFO - Train: 96 [ 155/156 (100%)]  Loss:  2.913390 (2.9152)  Time: 0.523s, 15653.13/s  (1.206s, 6792.07/s)  LR: 4.939e-05  Data: 0.000 (0.613)
2025-05-31 17:48:48,622 - train - INFO - Test: [   0/48]  Time: 6.474 (6.474)  Loss:  1.0469 (1.0469)  Acc@1: 79.4922 (79.4922)  Acc@5: 94.3359 (94.3359)
2025-05-31 17:49:46,123 - train - INFO - Test: [  48/48]  Time: 2.106 (1.306)  Loss:  1.0850 (1.6950)  Acc@1: 78.4198 (63.3860)  Acc@5: 93.0425 (85.1460)
2025-05-31 17:49:53,088 - train - INFO - Train: 97 [   0/156 (  0%)]  Loss:  2.935488 (2.9355)  Time: 6.579s, 1245.13/s  (6.579s, 1245.13/s)  LR: 3.217e-05  Data: 5.294 (5.294)
2025-05-31 17:50:51,918 - train - INFO - Train: 97 [  50/156 ( 32%)]  Loss:  2.896230 (2.9159)  Time: 0.526s, 15569.66/s  (1.283s, 6387.46/s)  LR: 3.217e-05  Data: 0.001 (0.428)
2025-05-31 17:51:52,093 - train - INFO - Train: 97 [ 100/156 ( 65%)]  Loss:  2.911895 (2.9145)  Time: 2.944s, 2782.41/s  (1.243s, 6588.46/s)  LR: 3.217e-05  Data: 0.930 (0.288)
2025-05-31 17:52:49,398 - train - INFO - Train: 97 [ 150/156 ( 97%)]  Loss:  2.937908 (2.9204)  Time: 0.526s, 15560.04/s  (1.211s, 6763.74/s)  LR: 3.217e-05  Data: 0.000 (0.291)
2025-05-31 17:52:54,451 - train - INFO - Train: 97 [ 155/156 (100%)]  Loss:  2.890216 (2.9143)  Time: 0.521s, 15715.91/s  (1.205s, 6799.85/s)  LR: 3.217e-05  Data: 0.000 (0.288)
2025-05-31 17:53:01,039 - train - INFO - Test: [   0/48]  Time: 6.319 (6.319)  Loss:  0.9810 (0.9810)  Acc@1: 79.1992 (79.1992)  Acc@5: 93.3594 (93.3594)
2025-05-31 17:53:59,550 - train - INFO - Test: [  48/48]  Time: 2.280 (1.323)  Loss:  0.9604 (1.6074)  Acc@1: 78.5377 (63.9600)  Acc@5: 94.1038 (85.6100)
2025-05-31 17:54:06,445 - train - INFO - Train: 98 [   0/156 (  0%)]  Loss:  2.943383 (2.9434)  Time: 6.473s, 1265.55/s  (6.473s, 1265.55/s)  LR: 1.986e-05  Data: 5.085 (5.085)
2025-05-31 17:55:04,388 - train - INFO - Train: 98 [  50/156 ( 32%)]  Loss:  2.960938 (2.9522)  Time: 0.527s, 15550.75/s  (1.263s, 6485.88/s)  LR: 1.986e-05  Data: 0.000 (0.488)
2025-05-31 17:56:05,030 - train - INFO - Train: 98 [ 100/156 ( 65%)]  Loss:  2.886240 (2.9302)  Time: 3.150s, 2600.41/s  (1.238s, 6616.15/s)  LR: 1.986e-05  Data: 0.558 (0.386)
2025-05-31 17:57:03,119 - train - INFO - Train: 98 [ 150/156 ( 97%)]  Loss:  2.920974 (2.9279)  Time: 0.530s, 15467.62/s  (1.213s, 6754.17/s)  LR: 1.986e-05  Data: 0.000 (0.272)
2025-05-31 17:57:08,148 - train - INFO - Train: 98 [ 155/156 (100%)]  Loss:  2.928183 (2.9279)  Time: 0.522s, 15684.84/s  (1.206s, 6791.39/s)  LR: 1.986e-05  Data: 0.000 (0.263)
2025-05-31 17:57:15,062 - train - INFO - Test: [   0/48]  Time: 6.623 (6.623)  Loss:  1.1221 (1.1221)  Acc@1: 79.1016 (79.1016)  Acc@5: 92.1875 (92.1875)
2025-05-31 17:58:13,224 - train - INFO - Test: [  48/48]  Time: 2.253 (1.322)  Loss:  1.0508 (1.6992)  Acc@1: 78.3019 (63.2000)  Acc@5: 93.0425 (84.9860)
2025-05-31 17:58:20,178 - train - INFO - Train: 99 [   0/156 (  0%)]  Loss:  2.929048 (2.9290)  Time: 6.592s, 1242.67/s  (6.592s, 1242.67/s)  LR: 1.246e-05  Data: 5.928 (5.928)
2025-05-31 17:59:18,592 - train - INFO - Train: 99 [  50/156 ( 32%)]  Loss:  2.944098 (2.9366)  Time: 0.527s, 15537.09/s  (1.275s, 6427.19/s)  LR: 1.246e-05  Data: 0.000 (0.440)
2025-05-31 18:00:19,433 - train - INFO - Train: 99 [ 100/156 ( 65%)]  Loss:  2.935004 (2.9360)  Time: 2.975s, 2753.23/s  (1.246s, 6574.70/s)  LR: 1.246e-05  Data: 0.000 (0.222)
2025-05-31 18:01:17,727 - train - INFO - Train: 99 [ 150/156 ( 97%)]  Loss:  2.911035 (2.9298)  Time: 0.526s, 15568.32/s  (1.219s, 6717.76/s)  LR: 1.246e-05  Data: 0.000 (0.149)
2025-05-31 18:01:22,633 - train - INFO - Train: 99 [ 155/156 (100%)]  Loss:  2.901975 (2.9242)  Time: 0.526s, 15587.55/s  (1.212s, 6760.14/s)  LR: 1.246e-05  Data: 0.000 (0.144)
2025-05-31 18:01:29,108 - train - INFO - Test: [   0/48]  Time: 6.187 (6.187)  Loss:  1.0908 (1.0908)  Acc@1: 78.4180 (78.4180)  Acc@5: 92.8711 (92.8711)
2025-05-31 18:02:26,822 - train - INFO - Test: [  48/48]  Time: 1.865 (1.304)  Loss:  1.0781 (1.6418)  Acc@1: 78.6557 (64.3020)  Acc@5: 92.8066 (85.7640)
2025-05-31 18:02:33,377 - train - INFO - Train: 100 [   0/156 (  0%)]  Loss:  2.926173 (2.9262)  Time: 6.179s, 1325.76/s  (6.179s, 1325.76/s)  LR: 1.000e-05  Data: 5.328 (5.328)
2025-05-31 18:03:31,464 - train - INFO - Train: 100 [  50/156 ( 32%)]  Loss:  2.917868 (2.9220)  Time: 0.527s, 15531.33/s  (1.260s, 6501.03/s)  LR: 1.000e-05  Data: 0.000 (0.436)
2025-05-31 18:04:32,128 - train - INFO - Train: 100 [ 100/156 ( 65%)]  Loss:  2.923758 (2.9226)  Time: 2.443s, 3352.57/s  (1.237s, 6622.95/s)  LR: 1.000e-05  Data: 0.166 (0.288)
2025-05-31 18:05:31,322 - train - INFO - Train: 100 [ 150/156 ( 97%)]  Loss:  2.925379 (2.9233)  Time: 0.525s, 15596.33/s  (1.219s, 6718.42/s)  LR: 1.000e-05  Data: 0.000 (0.193)
2025-05-31 18:05:36,196 - train - INFO - Train: 100 [ 155/156 (100%)]  Loss:  2.922971 (2.9232)  Time: 0.523s, 15655.72/s  (1.211s, 6761.92/s)  LR: 1.000e-05  Data: 0.000 (0.187)
2025-05-31 18:05:43,018 - train - INFO - Test: [   0/48]  Time: 6.526 (6.526)  Loss:  1.0391 (1.0391)  Acc@1: 78.8086 (78.8086)  Acc@5: 92.8711 (92.8711)
2025-05-31 18:06:41,072 - train - INFO - Test: [  48/48]  Time: 1.846 (1.318)  Loss:  0.9644 (1.6449)  Acc@1: 79.3632 (63.6980)  Acc@5: 93.0425 (85.3600)
2025-05-31 18:06:47,942 - train - INFO - Train: 101 [   0/156 (  0%)]  Loss:  2.900117 (2.9001)  Time: 6.494s, 1261.47/s  (6.494s, 1261.47/s)  LR: 1.000e-05  Data: 5.974 (5.974)
2025-05-31 18:07:46,905 - train - INFO - Train: 101 [  50/156 ( 32%)]  Loss:  2.922544 (2.9113)  Time: 0.527s, 15544.61/s  (1.283s, 6382.93/s)  LR: 1.000e-05  Data: 0.000 (0.503)
2025-05-31 18:08:48,203 - train - INFO - Train: 101 [ 100/156 ( 65%)]  Loss:  2.932408 (2.9184)  Time: 3.418s, 2396.76/s  (1.255s, 6527.71/s)  LR: 1.000e-05  Data: 0.276 (0.309)
2025-05-31 18:09:46,780 - train - INFO - Train: 101 [ 150/156 ( 97%)]  Loss:  2.921124 (2.9190)  Time: 0.525s, 15596.60/s  (1.227s, 6674.71/s)  LR: 1.000e-05  Data: 0.000 (0.210)
2025-05-31 18:09:51,677 - train - INFO - Train: 101 [ 155/156 (100%)]  Loss:  2.887843 (2.9128)  Time: 0.523s, 15675.38/s  (1.219s, 6718.23/s)  LR: 1.000e-05  Data: 0.000 (0.204)
2025-05-31 18:09:59,435 - train - INFO - Test: [   0/48]  Time: 7.473 (7.473)  Loss:  1.0381 (1.0381)  Acc@1: 80.1758 (80.1758)  Acc@5: 93.0664 (93.0664)
2025-05-31 18:10:58,865 - train - INFO - Test: [  48/48]  Time: 3.551 (1.365)  Loss:  1.2148 (1.6667)  Acc@1: 77.7123 (63.7160)  Acc@5: 91.5094 (85.4180)
2025-05-31 18:11:05,681 - train - INFO - Train: 102 [   0/156 (  0%)]  Loss:  2.880233 (2.8802)  Time: 6.438s, 1272.51/s  (6.438s, 1272.51/s)  LR: 1.000e-05  Data: 5.421 (5.421)
2025-05-31 18:12:04,358 - train - INFO - Train: 102 [  50/156 ( 32%)]  Loss:  2.904266 (2.8922)  Time: 0.528s, 15505.79/s  (1.277s, 6416.40/s)  LR: 1.000e-05  Data: 0.000 (0.421)
2025-05-31 18:13:05,241 - train - INFO - Train: 102 [ 100/156 ( 65%)]  Loss:  2.920648 (2.9017)  Time: 3.134s, 2613.64/s  (1.247s, 6566.86/s)  LR: 1.000e-05  Data: 0.000 (0.233)
2025-05-31 18:14:03,278 - train - INFO - Train: 102 [ 150/156 ( 97%)]  Loss:  2.942505 (2.9119)  Time: 0.525s, 15593.96/s  (1.219s, 6721.70/s)  LR: 1.000e-05  Data: 0.000 (0.156)
2025-05-31 18:14:08,140 - train - INFO - Train: 102 [ 155/156 (100%)]  Loss:  2.923980 (2.9143)  Time: 0.524s, 15634.12/s  (1.211s, 6765.54/s)  LR: 1.000e-05  Data: 0.000 (0.151)
2025-05-31 18:14:15,054 - train - INFO - Test: [   0/48]  Time: 6.623 (6.623)  Loss:  1.0195 (1.0195)  Acc@1: 79.6875 (79.6875)  Acc@5: 92.2852 (92.2852)
2025-05-31 18:15:13,517 - train - INFO - Test: [  48/48]  Time: 2.223 (1.328)  Loss:  1.0068 (1.6207)  Acc@1: 79.7170 (64.1940)  Acc@5: 93.1604 (85.5020)
2025-05-31 18:15:20,420 - train - INFO - Train: 103 [   0/156 (  0%)]  Loss:  2.890475 (2.8905)  Time: 6.536s, 1253.45/s  (6.536s, 1253.45/s)  LR: 1.000e-05  Data: 5.335 (5.335)
2025-05-31 18:16:19,059 - train - INFO - Train: 103 [  50/156 ( 32%)]  Loss:  2.901153 (2.8958)  Time: 0.529s, 15484.30/s  (1.278s, 6410.41/s)  LR: 1.000e-05  Data: 0.000 (0.603)
2025-05-31 18:17:19,845 - train - INFO - Train: 103 [ 100/156 ( 65%)]  Loss:  2.907152 (2.8996)  Time: 3.093s, 2648.27/s  (1.247s, 6568.76/s)  LR: 1.000e-05  Data: 0.624 (0.433)
2025-05-31 18:18:18,710 - train - INFO - Train: 103 [ 150/156 ( 97%)]  Loss:  2.882581 (2.8953)  Time: 2.148s, 3814.60/s  (1.224s, 6692.85/s)  LR: 1.000e-05  Data: 1.632 (0.396)
2025-05-31 18:18:23,698 - train - INFO - Train: 103 [ 155/156 (100%)]  Loss:  2.934347 (2.9031)  Time: 0.522s, 15700.15/s  (1.217s, 6732.84/s)  LR: 1.000e-05  Data: 0.000 (0.394)
2025-05-31 18:18:30,428 - train - INFO - Test: [   0/48]  Time: 6.454 (6.454)  Loss:  1.0518 (1.0518)  Acc@1: 79.4922 (79.4922)  Acc@5: 92.5781 (92.5781)
2025-05-31 18:19:28,650 - train - INFO - Test: [  48/48]  Time: 1.854 (1.320)  Loss:  1.2744 (1.6875)  Acc@1: 77.2406 (63.6100)  Acc@5: 91.8632 (85.3100)
2025-05-31 18:19:35,703 - train - INFO - Train: 104 [   0/156 (  0%)]  Loss:  2.917763 (2.9178)  Time: 6.681s, 1226.09/s  (6.681s, 1226.09/s)  LR: 1.000e-05  Data: 5.604 (5.604)
2025-05-31 18:20:34,017 - train - INFO - Train: 104 [  50/156 ( 32%)]  Loss:  2.885288 (2.9015)  Time: 0.527s, 15552.45/s  (1.274s, 6428.25/s)  LR: 1.000e-05  Data: 0.000 (0.416)
2025-05-31 18:21:34,529 - train - INFO - Train: 104 [ 100/156 ( 65%)]  Loss:  2.901459 (2.9015)  Time: 3.212s, 2550.34/s  (1.243s, 6592.53/s)  LR: 1.000e-05  Data: 0.000 (0.252)
2025-05-31 18:22:32,206 - train - INFO - Train: 104 [ 150/156 ( 97%)]  Loss:  2.937198 (2.9104)  Time: 0.526s, 15566.53/s  (1.213s, 6752.88/s)  LR: 1.000e-05  Data: 0.000 (0.169)
2025-05-31 18:22:37,168 - train - INFO - Train: 104 [ 155/156 (100%)]  Loss:  2.919362 (2.9122)  Time: 0.522s, 15684.82/s  (1.206s, 6792.51/s)  LR: 1.000e-05  Data: 0.000 (0.163)
2025-05-31 18:22:43,699 - train - INFO - Test: [   0/48]  Time: 6.243 (6.243)  Loss:  1.0811 (1.0811)  Acc@1: 80.0781 (80.0781)  Acc@5: 92.8711 (92.8711)
2025-05-31 18:23:41,696 - train - INFO - Test: [  48/48]  Time: 1.972 (1.311)  Loss:  1.1211 (1.7105)  Acc@1: 80.1887 (63.6340)  Acc@5: 92.4528 (85.3660)
2025-05-31 18:23:48,436 - train - INFO - Train: 105 [   0/156 (  0%)]  Loss:  2.914440 (2.9144)  Time: 6.368s, 1286.36/s  (6.368s, 1286.36/s)  LR: 1.000e-05  Data: 5.178 (5.178)
2025-05-31 18:24:47,003 - train - INFO - Train: 105 [  50/156 ( 32%)]  Loss:  2.897137 (2.9058)  Time: 0.527s, 15532.24/s  (1.273s, 6434.05/s)  LR: 1.000e-05  Data: 0.000 (0.462)
2025-05-31 18:25:47,747 - train - INFO - Train: 105 [ 100/156 ( 65%)]  Loss:  2.918510 (2.9100)  Time: 3.224s, 2540.73/s  (1.244s, 6583.54/s)  LR: 1.000e-05  Data: 1.251 (0.359)
2025-05-31 18:26:46,125 - train - INFO - Train: 105 [ 150/156 ( 97%)]  Loss:  2.853950 (2.8960)  Time: 0.529s, 15494.08/s  (1.219s, 6720.91/s)  LR: 1.000e-05  Data: 0.000 (0.284)
2025-05-31 18:26:51,018 - train - INFO - Train: 105 [ 155/156 (100%)]  Loss:  2.941712 (2.9051)  Time: 0.521s, 15711.65/s  (1.211s, 6763.69/s)  LR: 1.000e-05  Data: 0.000 (0.275)
2025-05-31 18:26:57,820 - train - INFO - Test: [   0/48]  Time: 6.505 (6.505)  Loss:  1.1172 (1.1172)  Acc@1: 78.2227 (78.2227)  Acc@5: 91.8945 (91.8945)
2025-05-31 18:27:55,269 - train - INFO - Test: [  48/48]  Time: 2.416 (1.305)  Loss:  1.1338 (1.7373)  Acc@1: 77.3585 (63.0020)  Acc@5: 92.4528 (85.1120)
2025-05-31 18:28:02,340 - train - INFO - Train: 106 [   0/156 (  0%)]  Loss:  2.893809 (2.8938)  Time: 6.695s, 1223.67/s  (6.695s, 1223.67/s)  LR: 1.000e-05  Data: 6.167 (6.167)
2025-05-31 18:29:01,487 - train - INFO - Train: 106 [  50/156 ( 32%)]  Loss:  2.899559 (2.8967)  Time: 0.527s, 15537.80/s  (1.291s, 6345.58/s)  LR: 1.000e-05  Data: 0.000 (0.643)
2025-05-31 18:30:02,323 - train - INFO - Train: 106 [ 100/156 ( 65%)]  Loss:  2.937114 (2.9102)  Time: 2.977s, 2751.97/s  (1.254s, 6531.63/s)  LR: 1.000e-05  Data: 0.000 (0.424)
2025-05-31 18:31:00,481 - train - INFO - Train: 106 [ 150/156 ( 97%)]  Loss:  2.926809 (2.9143)  Time: 0.527s, 15540.62/s  (1.224s, 6692.51/s)  LR: 1.000e-05  Data: 0.000 (0.289)
2025-05-31 18:31:05,504 - train - INFO - Train: 106 [ 155/156 (100%)]  Loss:  2.921003 (2.9157)  Time: 0.522s, 15700.77/s  (1.217s, 6731.24/s)  LR: 1.000e-05  Data: 0.000 (0.280)
2025-05-31 18:31:12,367 - train - INFO - Test: [   0/48]  Time: 6.582 (6.582)  Loss:  1.0586 (1.0586)  Acc@1: 79.5898 (79.5898)  Acc@5: 93.0664 (93.0664)
2025-05-31 18:32:10,334 - train - INFO - Test: [  48/48]  Time: 2.110 (1.317)  Loss:  0.9385 (1.6580)  Acc@1: 81.6038 (63.7320)  Acc@5: 94.8113 (85.4840)
2025-05-31 18:32:17,018 - train - INFO - Train: 107 [   0/156 (  0%)]  Loss:  2.927325 (2.9273)  Time: 6.300s, 1300.40/s  (6.300s, 1300.40/s)  LR: 1.000e-05  Data: 5.767 (5.767)
2025-05-31 18:33:16,448 - train - INFO - Train: 107 [  50/156 ( 32%)]  Loss:  2.941594 (2.9345)  Time: 0.528s, 15524.54/s  (1.289s, 6356.42/s)  LR: 1.000e-05  Data: 0.000 (0.443)
2025-05-31 18:34:15,527 - train - INFO - Train: 107 [ 100/156 ( 65%)]  Loss:  2.932583 (2.9338)  Time: 1.287s, 6364.54/s  (1.236s, 6629.51/s)  LR: 1.000e-05  Data: 0.000 (0.255)
2025-05-31 18:35:15,519 - train - INFO - Train: 107 [ 150/156 ( 97%)]  Loss:  2.891139 (2.9232)  Time: 0.526s, 15568.72/s  (1.224s, 6693.88/s)  LR: 1.000e-05  Data: 0.000 (0.175)
2025-05-31 18:35:21,363 - train - INFO - Train: 107 [ 155/156 (100%)]  Loss:  2.929325 (2.9244)  Time: 0.522s, 15688.96/s  (1.222s, 6703.61/s)  LR: 1.000e-05  Data: 0.000 (0.170)
2025-05-31 18:35:28,272 - train - INFO - Test: [   0/48]  Time: 6.639 (6.639)  Loss:  1.0557 (1.0557)  Acc@1: 78.0273 (78.0273)  Acc@5: 92.3828 (92.3828)
2025-05-31 18:36:26,358 - train - INFO - Test: [  48/48]  Time: 2.152 (1.321)  Loss:  1.0459 (1.6502)  Acc@1: 78.3019 (63.5720)  Acc@5: 92.4528 (85.3820)
2025-05-31 18:36:33,154 - train - INFO - Train: 108 [   0/156 (  0%)]  Loss:  2.941352 (2.9414)  Time: 6.416s, 1276.80/s  (6.416s, 1276.80/s)  LR: 1.000e-05  Data: 5.433 (5.433)
2025-05-31 18:37:31,594 - train - INFO - Train: 108 [  50/156 ( 32%)]  Loss:  2.917095 (2.9292)  Time: 0.527s, 15531.01/s  (1.272s, 6441.98/s)  LR: 1.000e-05  Data: 0.001 (0.602)
2025-05-31 18:38:32,117 - train - INFO - Train: 108 [ 100/156 ( 65%)]  Loss:  2.909180 (2.9225)  Time: 2.652s, 3088.86/s  (1.241s, 6599.28/s)  LR: 1.000e-05  Data: 0.898 (0.502)
2025-05-31 18:39:31,120 - train - INFO - Train: 108 [ 150/156 ( 97%)]  Loss:  2.928096 (2.9239)  Time: 0.526s, 15575.69/s  (1.221s, 6709.03/s)  LR: 1.000e-05  Data: 0.000 (0.383)
2025-05-31 18:39:36,479 - train - INFO - Train: 108 [ 155/156 (100%)]  Loss:  2.923879 (2.9239)  Time: 0.523s, 15674.08/s  (1.216s, 6735.42/s)  LR: 1.000e-05  Data: 0.000 (0.377)
2025-05-31 18:39:43,030 - train - INFO - Test: [   0/48]  Time: 6.258 (6.258)  Loss:  1.0947 (1.0947)  Acc@1: 79.4922 (79.4922)  Acc@5: 92.4805 (92.4805)
2025-05-31 18:40:41,619 - train - INFO - Test: [  48/48]  Time: 1.715 (1.323)  Loss:  1.1328 (1.6572)  Acc@1: 78.4198 (64.2080)  Acc@5: 92.4528 (85.7720)
2025-05-31 18:40:48,653 - train - INFO - Train: 109 [   0/156 (  0%)]  Loss:  2.887675 (2.8877)  Time: 6.608s, 1239.76/s  (6.608s, 1239.76/s)  LR: 1.000e-05  Data: 6.092 (6.092)
2025-05-31 18:41:46,778 - train - INFO - Train: 109 [  50/156 ( 32%)]  Loss:  2.921039 (2.9044)  Time: 0.527s, 15545.32/s  (1.269s, 6454.22/s)  LR: 1.000e-05  Data: 0.000 (0.592)
2025-05-31 18:42:46,807 - train - INFO - Train: 109 [ 100/156 ( 65%)]  Loss:  2.884162 (2.8976)  Time: 1.837s, 4459.42/s  (1.235s, 6631.86/s)  LR: 1.000e-05  Data: 0.000 (0.341)
2025-05-31 18:43:46,953 - train - INFO - Train: 109 [ 150/156 ( 97%)]  Loss:  2.871526 (2.8911)  Time: 0.530s, 15463.52/s  (1.225s, 6689.87/s)  LR: 1.000e-05  Data: 0.000 (0.228)
2025-05-31 18:43:52,171 - train - INFO - Train: 109 [ 155/156 (100%)]  Loss:  2.903368 (2.8936)  Time: 0.521s, 15714.79/s  (1.219s, 6721.72/s)  LR: 1.000e-05  Data: 0.000 (0.221)
2025-05-31 18:43:58,770 - train - INFO - Test: [   0/48]  Time: 6.316 (6.316)  Loss:  1.0156 (1.0156)  Acc@1: 80.5664 (80.5664)  Acc@5: 93.7500 (93.7500)
2025-05-31 18:44:56,486 - train - INFO - Test: [  48/48]  Time: 2.228 (1.307)  Loss:  1.1211 (1.6679)  Acc@1: 77.5943 (64.0800)  Acc@5: 92.4528 (85.5120)
