2025-06-03 13:42:06,036 - train - INFO - Namespace(data_dir='/data/dataset/imagenet', dataset='image_folder', train_split='train', val_split='validation', model='ResNet18', pretrained=False, initial_checkpoint='output/train/20250427-184912-ResNet18-224/best.pth.tar.pth', resume='', no_resume_opt=False, num_classes=1000, gp=None, img_size=224, input_size=None, crop_pct=None, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], interpolation='', batch_size=712, validation_batch_size_multiplier=1, gpu=0, opt='sgd', opt_eps=None, opt_betas=None, momentum=0.9, weight_decay=0.0001, clip_grad=None, clip_mode='norm', sched='cosine', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, lr_cycle_mul=1.0, lr_cycle_limit=1, warmup_lr=0.0001, min_lr=1e-05, epochs=200, epoch_repeats=0.0, start_epoch=None, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, no_aug=False, scale=[0, 1.0], ratio=[0.75, 1.3333333333333333], hflip=0, vflip=0.0, color_jitter=None, aa='rand-m9-mstd0.5-inc1', aug_splits=0, jsd=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.0, cutmix=0.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', mixup_off_epoch=0, smoothing=0.1, train_interpolation='random', bn_tf=False, bn_momentum=None, bn_eps=None, sync_bn=True, dist_bn='', split_bn=False, log_interval=50, recovery_interval=0, checkpoint_hist=1, save_images=False, amp=False, apex_amp=False, native_amp=False, channels_last=False, pin_mem=True, output='', experiment='', eval_metric='top1', tta=0, use_multi_epochs_loader=False, torchscript=False, log_wandb=False, wq_enable=True, wq_mode='LSQ', wq_bitw=32, wq_pos=None, wq_neg=None, wq_per_channel=True, wq_asym=True, aq_enable=True, aq_mode='LSQ', aq_bitw=32, aq_pos=None, aq_neg=None, aq_asym=True, qmodules=['convbn_first;wq:bit:8;aq:bit:8', 'layer1.0.convbn1', 'layer1.0.convbn2', 'layer1.1.convbn1', 'layer1.1.convbn2', 'layer2.0.convbn1', 'layer2.0.convbn2', 'layer2.1.convbn1', 'layer2.1.convbn2', 'layer3.0.convbn1', 'layer3.0.convbn2', 'layer3.1.convbn1', 'layer3.1.convbn2', 'layer4.0.convbn1', 'layer4.0.convbn2', 'layer4.1.convbn1', 'layer4.1.convbn2', 'fc;wq:bit:8;aq:bit:8'], resq_modules=['relu', 'layer1.0.relu2', 'layer1.1.relu2', 'layer2.0.relu2', 'layer3.0.downsample', 'layer2.1.relu2', 'layer3.0.relu2', 'layer3.0.downsample', 'layer3.1.relu2', 'layer4.0.downsample', 'layer4.0.relu2', 'layer4.1.relu2'], resq_enable=True, resq_mode='LSQ', resq_bitw=16, resq_pos=None, resq_neg=None, resq_asym=False, aq_per_channel=False, powerof2=True, world_size=4, local_rank=-1, dist_on_itp=False, dist_url='env://', device='cuda', seed=0, dist_eval=False, use_kd=True, kd_alpha=4, teacher='ResNet18', teacher_checkpoint='output/train/20250427-184912-ResNet18-224/best.pth.tar.pth', log_name='resnet18_imagenet_avgpool_acc15_kd0603', budget=1, bw_list='5, 5, 5, 5, 5, 4, 4, 3, 4, 3, 3, 3, 3, 2, 2, 2', ba_list='4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3', workers=4, multiprocessing_distributed=True, rank=0, distributed=True, dist_backend='nccl', **{'weight-decay': '1e-3'})
2025-06-03 13:42:08,116 - train - INFO - Model ResNet18 created, param count:11689512
2025-06-03 13:42:08,485 - train - INFO - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2025-06-03 13:42:08,486 - train - INFO - Using native Torch AMP. Training in mixed precision.
2025-06-03 13:42:08,486 - train - INFO - Scheduled epochs: 210
2025-06-03 13:42:08,486 - train - INFO - Verifying teacher model
2025-06-03 13:42:08,487 - train - INFO - Verifying initial model in test dataset
2025-06-03 13:42:08,487 - train - INFO - cuda:0
2025-06-03 13:42:08,488 - train - INFO - DistributedDataParallel(
  (module): ResNet(
    (relu): ReLU(
      (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
    )
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (maxpool): AvgPool2d(kernel_size=3, stride=2, padding=1)
    (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (convbn_first): QConvBn2d(
      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
      (quan_w_fn): LsqQuantizer(bit=8, pos=127, neg=-128, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
      (quan_a_fn): LsqQuantizer(bit=8, pos=127, neg=-127, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=True, per_channel=False, apot=False )
      (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=5, pos=15, neg=-16, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=4, pos=7, neg=-8, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=4, pos=7, neg=-8, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=4, pos=7, neg=-8, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=4, pos=15, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (quan_a_fn): LsqQuantizer(bit=16, pos=32767, neg=-32767, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=True, per_channel=False, apot=False )
        )
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=3, pos=3, neg=-4, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=2, pos=1, neg=-2, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (quan_a_fn): LsqQuantizer(bit=16, pos=32767, neg=-32767, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=True, per_channel=False, apot=False )
        )
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn1): QConvBn2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=2, pos=1, neg=-2, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (convbn2): QConvBn2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (quan_w_fn): LsqQuantizer(bit=2, pos=1, neg=-2, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
          (quan_a_fn): LsqQuantizer(bit=3, pos=7, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Sequential()
        (relu2): ReLU(
          (quan_a_fn): LsqQuantizer(bit=16, pos=65535, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
        )
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): QLinear(
      in_features=512, out_features=1000, bias=True
      (quan_w_fn): LsqQuantizer(bit=8, pos=127, neg=-128, norm=(False, 1e-05, 1.0), all_positive=False, symmetric=False, per_channel=True, apot=False )
      (quan_a_fn): LsqQuantizer(bit=8, pos=255, neg=0, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=False, apot=False )
    )
  )
)
2025-06-03 13:42:17,993 - train - INFO - Train: 0 [   0/449 (  0%)]  Loss:  5.317732 (5.3177)  Time: 9.210s,  309.22/s  (9.210s,  309.22/s)  LR: 1.000e-02  Data: 5.217 (5.217)
2025-06-03 13:42:58,542 - train - INFO - Train: 0 [  50/449 ( 11%)]  Loss:  4.025826 (4.6718)  Time: 1.480s, 1924.57/s  (0.975s, 2919.61/s)  LR: 1.000e-02  Data: 0.000 (0.183)
2025-06-03 13:43:39,697 - train - INFO - Train: 0 [ 100/449 ( 22%)]  Loss:  3.888200 (4.4106)  Time: 0.547s, 5210.68/s  (0.900s, 3164.34/s)  LR: 1.000e-02  Data: 0.000 (0.092)
2025-06-03 13:44:20,472 - train - INFO - Train: 0 [ 150/449 ( 33%)]  Loss:  3.723566 (4.2388)  Time: 0.546s, 5216.07/s  (0.872s, 3265.92/s)  LR: 1.000e-02  Data: 0.000 (0.106)
2025-06-03 13:45:01,505 - train - INFO - Train: 0 [ 200/449 ( 45%)]  Loss:  3.664833 (4.1240)  Time: 1.743s, 1633.71/s  (0.859s, 3314.52/s)  LR: 1.000e-02  Data: 1.203 (0.147)
2025-06-03 13:45:51,865 - train - INFO - Train: 0 [ 250/449 ( 56%)]  Loss:  3.720275 (4.0567)  Time: 0.543s, 5246.04/s  (0.889s, 3204.62/s)  LR: 1.000e-02  Data: 0.000 (0.209)
2025-06-03 13:46:34,690 - train - INFO - Train: 0 [ 300/449 ( 67%)]  Loss:  3.809993 (4.0215)  Time: 1.633s, 1743.58/s  (0.883s, 3224.05/s)  LR: 1.000e-02  Data: 1.094 (0.226)
2025-06-03 13:47:15,148 - train - INFO - Train: 0 [ 350/449 ( 78%)]  Loss:  3.576755 (3.9659)  Time: 0.546s, 5216.93/s  (0.873s, 3263.10/s)  LR: 1.000e-02  Data: 0.000 (0.232)
2025-06-03 13:47:57,741 - train - INFO - Train: 0 [ 400/449 ( 89%)]  Loss:  3.531538 (3.9176)  Time: 1.530s, 1861.82/s  (0.870s, 3272.90/s)  LR: 1.000e-02  Data: 0.989 (0.241)
2025-06-03 13:48:36,389 - train - INFO - Train: 0 [ 448/449 (100%)]  Loss:  3.516927 (3.8776)  Time: 1.329s, 2142.41/s  (0.863s, 3299.26/s)  LR: 1.000e-02  Data: 0.793 (0.243)
2025-06-03 13:48:41,263 - train - INFO - Test: [   0/70]  Time: 4.644 (4.644)  Loss:  1.3008 (1.3008)  Acc@1: 70.6812 (70.6812)  Acc@5: 91.2570 (91.2570)
2025-06-03 13:49:22,351 - train - INFO - Test: [  50/70]  Time: 0.119 (0.897)  Loss:  1.9492 (1.8258)  Acc@1: 59.3399 (59.3055)  Acc@5: 80.6180 (82.8280)
2025-06-03 13:49:38,619 - train - INFO - Test: [  70/70]  Time: 0.325 (0.873)  Loss:  2.2949 (1.9042)  Acc@1: 47.9688 (57.8320)  Acc@5: 75.0000 (81.4335)
2025-06-03 13:49:43,842 - train - INFO - Train: 1 [   0/449 (  0%)]  Loss:  3.502715 (3.5027)  Time: 4.916s,  579.39/s  (4.916s,  579.39/s)  LR: 9.999e-03  Data: 4.094 (4.094)
2025-06-03 13:50:22,574 - train - INFO - Train: 1 [  50/449 ( 11%)]  Loss:  3.538989 (3.5209)  Time: 0.553s, 5149.55/s  (0.856s, 3327.82/s)  LR: 9.999e-03  Data: 0.000 (0.269)
2025-06-03 13:51:01,280 - train - INFO - Train: 1 [ 100/449 ( 22%)]  Loss:  3.455896 (3.4992)  Time: 0.978s, 2911.33/s  (0.815s, 3493.15/s)  LR: 9.999e-03  Data: 0.431 (0.216)
2025-06-03 13:51:39,493 - train - INFO - Train: 1 [ 150/449 ( 33%)]  Loss:  3.446210 (3.4860)  Time: 0.553s, 5145.78/s  (0.798s, 3567.15/s)  LR: 9.999e-03  Data: 0.000 (0.196)
2025-06-03 13:52:18,567 - train - INFO - Train: 1 [ 200/449 ( 45%)]  Loss:  3.490914 (3.4869)  Time: 1.051s, 2709.36/s  (0.794s, 3586.07/s)  LR: 9.999e-03  Data: 0.512 (0.191)
2025-06-03 13:52:56,526 - train - INFO - Train: 1 [ 250/449 ( 56%)]  Loss:  3.466924 (3.4836)  Time: 0.548s, 5197.84/s  (0.787s, 3617.87/s)  LR: 9.999e-03  Data: 0.000 (0.188)
2025-06-03 13:53:34,818 - train - INFO - Train: 1 [ 300/449 ( 67%)]  Loss:  3.449448 (3.4787)  Time: 0.547s, 5206.60/s  (0.784s, 3634.28/s)  LR: 9.999e-03  Data: 0.000 (0.175)
2025-06-03 13:54:13,873 - train - INFO - Train: 1 [ 350/449 ( 78%)]  Loss:  3.488179 (3.4799)  Time: 0.547s, 5205.73/s  (0.783s, 3635.98/s)  LR: 9.999e-03  Data: 0.000 (0.151)
2025-06-03 13:54:51,794 - train - INFO - Train: 1 [ 400/449 ( 89%)]  Loss:  3.408586 (3.4720)  Time: 0.545s, 5223.64/s  (0.780s, 3650.45/s)  LR: 9.999e-03  Data: 0.000 (0.132)
2025-06-03 13:55:28,554 - train - INFO - Train: 1 [ 448/449 (100%)]  Loss:  3.427349 (3.4675)  Time: 0.545s, 5226.17/s  (0.779s, 3657.65/s)  LR: 9.999e-03  Data: 0.000 (0.118)
2025-06-03 13:55:33,211 - train - INFO - Test: [   0/70]  Time: 4.458 (4.458)  Loss:  1.5840 (1.5840)  Acc@1: 68.6096 (68.6096)  Acc@5: 87.0084 (87.0084)
2025-06-03 13:56:14,299 - train - INFO - Test: [  50/70]  Time: 0.120 (0.893)  Loss:  2.3086 (2.0289)  Acc@1: 52.2823 (56.3546)  Acc@5: 75.0702 (80.4342)
2025-06-03 13:56:30,367 - train - INFO - Test: [  70/70]  Time: 0.034 (0.868)  Loss:  2.5898 (2.1103)  Acc@1: 38.9062 (54.7885)  Acc@5: 74.8438 (78.9555)
2025-06-03 13:56:35,914 - train - INFO - Train: 2 [   0/449 (  0%)]  Loss:  3.469703 (3.4697)  Time: 5.257s,  541.73/s  (5.257s,  541.73/s)  LR: 9.998e-03  Data: 4.702 (4.702)
2025-06-03 13:57:16,077 - train - INFO - Train: 2 [  50/449 ( 11%)]  Loss:  3.445731 (3.4577)  Time: 0.548s, 5196.42/s  (0.891s, 3198.03/s)  LR: 9.998e-03  Data: 0.000 (0.345)
2025-06-03 13:57:54,942 - train - INFO - Train: 2 [ 100/449 ( 22%)]  Loss:  3.462843 (3.4594)  Time: 1.529s, 1862.92/s  (0.834s, 3412.94/s)  LR: 9.998e-03  Data: 0.990 (0.289)
2025-06-03 13:58:32,845 - train - INFO - Train: 2 [ 150/449 ( 33%)]  Loss:  3.357165 (3.4339)  Time: 0.547s, 5209.53/s  (0.809s, 3519.70/s)  LR: 9.998e-03  Data: 0.000 (0.264)
2025-06-03 13:59:11,394 - train - INFO - Train: 2 [ 200/449 ( 45%)]  Loss:  3.422931 (3.4317)  Time: 0.572s, 4977.03/s  (0.800s, 3561.52/s)  LR: 9.998e-03  Data: 0.030 (0.237)
2025-06-03 13:59:50,805 - train - INFO - Train: 2 [ 250/449 ( 56%)]  Loss:  3.685791 (3.4740)  Time: 1.073s, 2655.23/s  (0.797s, 3571.72/s)  LR: 9.998e-03  Data: 0.000 (0.190)
2025-06-03 14:00:29,470 - train - INFO - Train: 2 [ 300/449 ( 67%)]  Loss:  3.509403 (3.4791)  Time: 1.008s, 2826.00/s  (0.793s, 3589.73/s)  LR: 9.998e-03  Data: 0.000 (0.159)
2025-06-03 14:01:08,399 - train - INFO - Train: 2 [ 350/449 ( 78%)]  Loss:  3.360735 (3.4643)  Time: 1.170s, 2433.66/s  (0.791s, 3599.30/s)  LR: 9.998e-03  Data: 0.000 (0.136)
2025-06-03 14:01:47,963 - train - INFO - Train: 2 [ 400/449 ( 89%)]  Loss:  3.355611 (3.4522)  Time: 1.005s, 2834.50/s  (0.791s, 3599.32/s)  LR: 9.998e-03  Data: 0.000 (0.119)
2025-06-03 14:02:25,417 - train - INFO - Train: 2 [ 448/449 (100%)]  Loss:  3.381917 (3.4452)  Time: 1.066s, 2672.69/s  (0.790s, 3604.67/s)  LR: 9.998e-03  Data: 0.000 (0.106)
2025-06-03 14:02:29,906 - train - INFO - Test: [   0/70]  Time: 4.334 (4.334)  Loss:  1.0889 (1.0889)  Acc@1: 76.4045 (76.4045)  Acc@5: 93.7500 (93.7500)
2025-06-03 14:03:10,659 - train - INFO - Test: [  50/70]  Time: 0.119 (0.884)  Loss:  1.7734 (1.7258)  Acc@1: 62.2191 (62.1324)  Acc@5: 83.3216 (84.6056)
2025-06-03 14:03:26,735 - train - INFO - Test: [  70/70]  Time: 0.034 (0.861)  Loss:  2.1992 (1.7978)  Acc@1: 50.0000 (60.7235)  Acc@5: 77.8125 (83.3420)
2025-06-03 14:03:31,584 - train - INFO - Train: 3 [   0/449 (  0%)]  Loss:  3.395861 (3.3959)  Time: 4.484s,  635.15/s  (4.484s,  635.15/s)  LR: 9.994e-03  Data: 3.676 (3.676)
2025-06-03 14:04:11,190 - train - INFO - Train: 3 [  50/449 ( 11%)]  Loss:  3.343192 (3.3695)  Time: 0.552s, 5158.04/s  (0.864s, 3294.48/s)  LR: 9.994e-03  Data: 0.000 (0.287)
2025-06-03 14:04:51,375 - train - INFO - Train: 3 [ 100/449 ( 22%)]  Loss:  3.401122 (3.3801)  Time: 1.759s, 1619.51/s  (0.834s, 3413.32/s)  LR: 9.994e-03  Data: 0.000 (0.188)
2025-06-03 14:05:30,093 - train - INFO - Train: 3 [ 150/449 ( 33%)]  Loss:  3.362341 (3.3756)  Time: 0.553s, 5149.59/s  (0.814s, 3496.64/s)  LR: 9.994e-03  Data: 0.000 (0.126)
2025-06-03 14:06:09,888 - train - INFO - Train: 3 [ 200/449 ( 45%)]  Loss:  3.416091 (3.3837)  Time: 1.602s, 1778.19/s  (0.810s, 3516.64/s)  LR: 9.994e-03  Data: 0.000 (0.095)
2025-06-03 14:06:48,759 - train - INFO - Train: 3 [ 250/449 ( 56%)]  Loss:  3.494232 (3.4021)  Time: 0.548s, 5193.35/s  (0.803s, 3544.95/s)  LR: 9.994e-03  Data: 0.000 (0.076)
2025-06-03 14:07:28,307 - train - INFO - Train: 3 [ 300/449 ( 67%)]  Loss:  3.350187 (3.3947)  Time: 1.482s, 1922.20/s  (0.801s, 3554.11/s)  LR: 9.994e-03  Data: 0.000 (0.063)
2025-06-03 14:08:06,879 - train - INFO - Train: 3 [ 350/449 ( 78%)]  Loss:  3.312228 (3.3844)  Time: 0.552s, 5159.39/s  (0.797s, 3573.11/s)  LR: 9.994e-03  Data: 0.000 (0.054)
2025-06-03 14:08:46,398 - train - INFO - Train: 3 [ 400/449 ( 89%)]  Loss:  3.333299 (3.3787)  Time: 1.414s, 2013.67/s  (0.796s, 3576.86/s)  LR: 9.994e-03  Data: 0.000 (0.048)
2025-06-03 14:09:23,958 - train - INFO - Train: 3 [ 448/449 (100%)]  Loss:  3.378201 (3.3787)  Time: 1.320s, 2157.37/s  (0.795s, 3583.48/s)  LR: 9.994e-03  Data: 0.000 (0.043)
2025-06-03 14:09:28,535 - train - INFO - Test: [   0/70]  Time: 4.424 (4.424)  Loss:  1.1289 (1.1289)  Acc@1: 75.1053 (75.1053)  Acc@5: 92.4860 (92.4860)
2025-06-03 14:10:09,454 - train - INFO - Test: [  50/70]  Time: 0.119 (0.889)  Loss:  1.8037 (1.7144)  Acc@1: 61.1657 (62.4931)  Acc@5: 82.7247 (84.9113)
2025-06-03 14:10:25,818 - train - INFO - Test: [  70/70]  Time: 0.034 (0.869)  Loss:  2.3301 (1.7928)  Acc@1: 47.1875 (60.9355)  Acc@5: 75.7812 (83.5540)
2025-06-03 14:10:30,778 - train - INFO - Train: 4 [   0/449 (  0%)]  Loss:  3.315633 (3.3156)  Time: 4.530s,  628.73/s  (4.530s,  628.73/s)  LR: 9.990e-03  Data: 3.970 (3.970)
2025-06-03 14:11:09,915 - train - INFO - Train: 4 [  50/449 ( 11%)]  Loss:  3.426180 (3.3709)  Time: 0.550s, 5174.37/s  (0.856s, 3326.44/s)  LR: 9.990e-03  Data: 0.000 (0.242)
2025-06-03 14:11:49,701 - train - INFO - Train: 4 [ 100/449 ( 22%)]  Loss:  3.339746 (3.3605)  Time: 1.603s, 1776.90/s  (0.826s, 3446.93/s)  LR: 9.990e-03  Data: 0.000 (0.149)
2025-06-03 14:12:28,246 - train - INFO - Train: 4 [ 150/449 ( 33%)]  Loss:  3.313166 (3.3487)  Time: 0.546s, 5212.96/s  (0.808s, 3525.16/s)  LR: 9.990e-03  Data: 0.000 (0.099)
2025-06-03 14:13:08,114 - train - INFO - Train: 4 [ 200/449 ( 45%)]  Loss:  3.354479 (3.3498)  Time: 1.434s, 1986.72/s  (0.805s, 3536.66/s)  LR: 9.990e-03  Data: 0.000 (0.075)
2025-06-03 14:13:46,515 - train - INFO - Train: 4 [ 250/449 ( 56%)]  Loss:  3.372069 (3.3535)  Time: 0.548s, 5194.47/s  (0.798s, 3569.59/s)  LR: 9.990e-03  Data: 0.000 (0.060)
2025-06-03 14:14:25,615 - train - INFO - Train: 4 [ 300/449 ( 67%)]  Loss:  3.319101 (3.3486)  Time: 1.543s, 1845.63/s  (0.795s, 3581.43/s)  LR: 9.990e-03  Data: 0.000 (0.050)
2025-06-03 14:15:03,448 - train - INFO - Train: 4 [ 350/449 ( 78%)]  Loss:  3.346823 (3.3484)  Time: 0.547s, 5205.18/s  (0.790s, 3606.34/s)  LR: 9.990e-03  Data: 0.000 (0.043)
2025-06-03 14:15:42,570 - train - INFO - Train: 4 [ 400/449 ( 89%)]  Loss:  3.300731 (3.3431)  Time: 1.377s, 2067.69/s  (0.789s, 3610.51/s)  LR: 9.990e-03  Data: 0.000 (0.038)
2025-06-03 14:16:19,386 - train - INFO - Train: 4 [ 448/449 (100%)]  Loss:  3.472188 (3.3560)  Time: 1.168s, 2438.87/s  (0.786s, 3621.23/s)  LR: 9.990e-03  Data: 0.000 (0.034)
2025-06-03 14:16:24,138 - train - INFO - Test: [   0/70]  Time: 4.594 (4.594)  Loss:  1.4824 (1.4824)  Acc@1: 68.0478 (68.0478)  Acc@5: 89.1854 (89.1854)
2025-06-03 14:17:04,864 - train - INFO - Test: [  50/70]  Time: 0.120 (0.889)  Loss:  2.0508 (1.9584)  Acc@1: 56.5660 (57.1072)  Acc@5: 79.0028 (81.3271)
2025-06-03 14:17:21,055 - train - INFO - Test: [  70/70]  Time: 0.034 (0.866)  Loss:  2.7852 (2.0237)  Acc@1: 38.9062 (55.9845)  Acc@5: 69.5312 (80.1535)
2025-06-03 14:17:25,752 - train - INFO - Train: 5 [   0/449 (  0%)]  Loss:  3.483702 (3.4837)  Time: 4.442s,  641.22/s  (4.442s,  641.22/s)  LR: 9.985e-03  Data: 3.887 (3.887)
2025-06-03 14:18:04,047 - train - INFO - Train: 5 [  50/449 ( 11%)]  Loss:  3.393158 (3.4384)  Time: 0.557s, 5113.76/s  (0.838s, 3398.95/s)  LR: 9.985e-03  Data: 0.000 (0.247)
2025-06-03 14:18:43,310 - train - INFO - Train: 5 [ 100/449 ( 22%)]  Loss:  3.319379 (3.3987)  Time: 0.832s, 3421.87/s  (0.812s, 3508.10/s)  LR: 9.985e-03  Data: 0.050 (0.214)
2025-06-03 14:19:22,148 - train - INFO - Train: 5 [ 150/449 ( 33%)]  Loss:  3.282606 (3.3697)  Time: 0.547s, 5202.09/s  (0.800s, 3559.07/s)  LR: 9.985e-03  Data: 0.000 (0.145)
2025-06-03 14:20:01,238 - train - INFO - Train: 5 [ 200/449 ( 45%)]  Loss:  3.346071 (3.3650)  Time: 0.547s, 5209.10/s  (0.796s, 3579.60/s)  LR: 9.985e-03  Data: 0.000 (0.109)
2025-06-03 14:20:40,598 - train - INFO - Train: 5 [ 250/449 ( 56%)]  Loss:  3.333358 (3.3597)  Time: 0.547s, 5208.08/s  (0.794s, 3587.17/s)  LR: 9.985e-03  Data: 0.000 (0.088)
2025-06-03 14:21:19,522 - train - INFO - Train: 5 [ 300/449 ( 67%)]  Loss:  3.337605 (3.3566)  Time: 0.548s, 5197.95/s  (0.791s, 3598.82/s)  LR: 9.985e-03  Data: 0.000 (0.073)
2025-06-03 14:21:59,829 - train - INFO - Train: 5 [ 350/449 ( 78%)]  Loss:  3.296031 (3.3490)  Time: 0.547s, 5209.30/s  (0.793s, 3589.30/s)  LR: 9.985e-03  Data: 0.000 (0.063)
2025-06-03 14:22:38,638 - train - INFO - Train: 5 [ 400/449 ( 89%)]  Loss:  3.299845 (3.3435)  Time: 0.547s, 5208.45/s  (0.791s, 3599.10/s)  LR: 9.985e-03  Data: 0.000 (0.055)
2025-06-03 14:23:15,941 - train - INFO - Train: 5 [ 448/449 (100%)]  Loss:  3.250315 (3.3342)  Time: 0.544s, 5238.90/s  (0.790s, 3606.02/s)  LR: 9.985e-03  Data: 0.000 (0.049)
2025-06-03 14:23:20,460 - train - INFO - Test: [   0/70]  Time: 4.364 (4.364)  Loss:  1.2012 (1.2012)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.8891 (91.8891)
2025-06-03 14:24:01,860 - train - INFO - Test: [  50/70]  Time: 0.119 (0.897)  Loss:  1.7578 (1.6851)  Acc@1: 64.2205 (63.5148)  Acc@5: 82.4438 (85.3554)
2025-06-03 14:24:18,031 - train - INFO - Test: [  70/70]  Time: 0.034 (0.872)  Loss:  2.4531 (1.7625)  Acc@1: 44.3750 (62.1785)  Acc@5: 74.6875 (84.2215)
2025-06-03 14:24:23,005 - train - INFO - Train: 6 [   0/449 (  0%)]  Loss:  3.270931 (3.2709)  Time: 4.596s,  619.69/s  (4.596s,  619.69/s)  LR: 9.978e-03  Data: 4.049 (4.049)
2025-06-03 14:25:02,129 - train - INFO - Train: 6 [  50/449 ( 11%)]  Loss:  3.371435 (3.3212)  Time: 0.554s, 5141.27/s  (0.857s, 3322.30/s)  LR: 9.978e-03  Data: 0.000 (0.309)
2025-06-03 14:25:42,118 - train - INFO - Train: 6 [ 100/449 ( 22%)]  Loss:  3.293382 (3.3119)  Time: 1.553s, 1833.39/s  (0.829s, 3436.40/s)  LR: 9.978e-03  Data: 1.006 (0.279)
2025-06-03 14:26:21,024 - train - INFO - Train: 6 [ 150/449 ( 33%)]  Loss:  3.254145 (3.2975)  Time: 0.554s, 5141.70/s  (0.812s, 3507.41/s)  LR: 9.978e-03  Data: 0.000 (0.262)
2025-06-03 14:27:00,284 - train - INFO - Train: 6 [ 200/449 ( 45%)]  Loss:  3.425481 (3.3231)  Time: 1.521s, 1872.93/s  (0.805s, 3536.46/s)  LR: 9.978e-03  Data: 0.972 (0.255)
2025-06-03 14:27:39,725 - train - INFO - Train: 6 [ 250/449 ( 56%)]  Loss:  3.363149 (3.3298)  Time: 0.553s, 5147.05/s  (0.802s, 3550.99/s)  LR: 9.978e-03  Data: 0.000 (0.251)
2025-06-03 14:28:19,612 - train - INFO - Train: 6 [ 300/449 ( 67%)]  Loss:  3.336008 (3.3306)  Time: 1.528s, 1863.49/s  (0.801s, 3554.15/s)  LR: 9.978e-03  Data: 0.980 (0.250)
2025-06-03 14:28:58,685 - train - INFO - Train: 6 [ 350/449 ( 78%)]  Loss:  3.284163 (3.3248)  Time: 0.553s, 5149.85/s  (0.798s, 3566.76/s)  LR: 9.978e-03  Data: 0.000 (0.247)
2025-06-03 14:29:38,031 - train - INFO - Train: 6 [ 400/449 ( 89%)]  Loss:  3.314494 (3.3237)  Time: 1.478s, 1927.04/s  (0.797s, 3573.23/s)  LR: 9.978e-03  Data: 0.931 (0.246)
2025-06-03 14:30:15,508 - train - INFO - Train: 6 [ 448/449 (100%)]  Loss:  3.304226 (3.3217)  Time: 1.541s, 1848.72/s  (0.795s, 3581.05/s)  LR: 9.978e-03  Data: 0.995 (0.244)
2025-06-03 14:30:20,090 - train - INFO - Test: [   0/70]  Time: 4.418 (4.418)  Loss:  1.1641 (1.1641)  Acc@1: 74.8244 (74.8244)  Acc@5: 92.2402 (92.2402)
2025-06-03 14:31:01,294 - train - INFO - Test: [  50/70]  Time: 0.120 (0.895)  Loss:  1.9580 (1.7010)  Acc@1: 57.4438 (62.4477)  Acc@5: 79.5646 (84.5630)
2025-06-03 14:31:17,596 - train - INFO - Test: [  70/70]  Time: 0.034 (0.872)  Loss:  2.3379 (1.7791)  Acc@1: 49.6875 (60.8560)  Acc@5: 75.7812 (83.2590)
2025-06-03 14:31:22,757 - train - INFO - Train: 7 [   0/449 (  0%)]  Loss:  3.315329 (3.3153)  Time: 4.923s,  578.47/s  (4.923s,  578.47/s)  LR: 9.970e-03  Data: 3.646 (3.646)
2025-06-03 14:32:01,479 - train - INFO - Train: 7 [  50/449 ( 11%)]  Loss:  3.250836 (3.2831)  Time: 0.547s, 5207.74/s  (0.856s, 3327.95/s)  LR: 9.970e-03  Data: 0.000 (0.202)
2025-06-03 14:32:40,876 - train - INFO - Train: 7 [ 100/449 ( 22%)]  Loss:  3.245543 (3.2706)  Time: 1.133s, 2513.16/s  (0.822s, 3463.97/s)  LR: 9.970e-03  Data: 0.038 (0.121)
2025-06-03 14:33:20,075 - train - INFO - Train: 7 [ 150/449 ( 33%)]  Loss:  3.270798 (3.2706)  Time: 0.902s, 3156.61/s  (0.810s, 3518.13/s)  LR: 9.970e-03  Data: 0.000 (0.081)
2025-06-03 14:33:59,855 - train - INFO - Train: 7 [ 200/449 ( 45%)]  Loss:  3.320603 (3.2806)  Time: 0.552s, 5160.00/s  (0.806s, 3533.29/s)  LR: 9.970e-03  Data: 0.000 (0.061)
2025-06-03 14:34:38,242 - train - INFO - Train: 7 [ 250/449 ( 56%)]  Loss:  3.349653 (3.2921)  Time: 0.546s, 5219.21/s  (0.798s, 3567.13/s)  LR: 9.970e-03  Data: 0.000 (0.049)
2025-06-03 14:35:17,442 - train - INFO - Train: 7 [ 300/449 ( 67%)]  Loss:  3.261176 (3.2877)  Time: 0.748s, 3805.66/s  (0.796s, 3577.85/s)  LR: 9.970e-03  Data: 0.000 (0.041)
2025-06-03 14:35:56,881 - train - INFO - Train: 7 [ 350/449 ( 78%)]  Loss:  3.262804 (3.2846)  Time: 1.443s, 1974.07/s  (0.795s, 3582.50/s)  LR: 9.970e-03  Data: 0.000 (0.035)
2025-06-03 14:36:35,054 - train - INFO - Train: 7 [ 400/449 ( 89%)]  Loss:  3.212762 (3.2766)  Time: 0.552s, 5160.13/s  (0.791s, 3600.33/s)  LR: 9.970e-03  Data: 0.000 (0.031)
2025-06-03 14:37:12,150 - train - INFO - Train: 7 [ 448/449 (100%)]  Loss:  3.291943 (3.2781)  Time: 0.551s, 5170.30/s  (0.789s, 3609.22/s)  LR: 9.970e-03  Data: 0.000 (0.027)
2025-06-03 14:37:16,750 - train - INFO - Test: [   0/70]  Time: 4.438 (4.438)  Loss:  1.2207 (1.2207)  Acc@1: 72.8581 (72.8581)  Acc@5: 91.5379 (91.5379)
2025-06-03 14:37:57,570 - train - INFO - Test: [  50/70]  Time: 0.119 (0.887)  Loss:  1.8770 (1.7031)  Acc@1: 60.1826 (62.2274)  Acc@5: 80.7935 (84.6587)
2025-06-03 14:38:13,762 - train - INFO - Test: [  70/70]  Time: 0.034 (0.865)  Loss:  2.3945 (1.7787)  Acc@1: 48.5938 (60.6805)  Acc@5: 72.9688 (83.3535)
2025-06-03 14:38:18,702 - train - INFO - Train: 8 [   0/449 (  0%)]  Loss:  3.352872 (3.3529)  Time: 4.698s,  606.21/s  (4.698s,  606.21/s)  LR: 9.961e-03  Data: 3.736 (3.736)
2025-06-03 14:38:57,417 - train - INFO - Train: 8 [  50/449 ( 11%)]  Loss:  3.251322 (3.3021)  Time: 0.547s, 5210.75/s  (0.851s, 3345.79/s)  LR: 9.961e-03  Data: 0.000 (0.088)
2025-06-03 14:39:36,971 - train - INFO - Train: 8 [ 100/449 ( 22%)]  Loss:  3.250745 (3.2850)  Time: 1.528s, 1864.19/s  (0.821s, 3467.06/s)  LR: 9.961e-03  Data: 0.000 (0.044)
2025-06-03 14:40:15,341 - train - INFO - Train: 8 [ 150/449 ( 33%)]  Loss:  3.275402 (3.2826)  Time: 0.546s, 5212.26/s  (0.804s, 3544.31/s)  LR: 9.961e-03  Data: 0.000 (0.030)
2025-06-03 14:40:54,364 - train - INFO - Train: 8 [ 200/449 ( 45%)]  Loss:  3.260558 (3.2782)  Time: 1.372s, 2076.49/s  (0.798s, 3569.83/s)  LR: 9.961e-03  Data: 0.000 (0.022)
2025-06-03 14:41:32,256 - train - INFO - Train: 8 [ 250/449 ( 56%)]  Loss:  3.312313 (3.2839)  Time: 0.548s, 5193.88/s  (0.790s, 3605.83/s)  LR: 9.961e-03  Data: 0.000 (0.018)
2025-06-03 14:42:11,433 - train - INFO - Train: 8 [ 300/449 ( 67%)]  Loss:  3.298564 (3.2860)  Time: 1.118s, 2547.35/s  (0.789s, 3610.62/s)  LR: 9.961e-03  Data: 0.000 (0.015)
2025-06-03 14:42:49,986 - train - INFO - Train: 8 [ 350/449 ( 78%)]  Loss:  3.304076 (3.2882)  Time: 1.187s, 2398.76/s  (0.786s, 3622.23/s)  LR: 9.961e-03  Data: 0.000 (0.013)
2025-06-03 14:43:28,283 - train - INFO - Train: 8 [ 400/449 ( 89%)]  Loss:  3.221770 (3.2808)  Time: 0.547s, 5208.92/s  (0.784s, 3633.94/s)  LR: 9.961e-03  Data: 0.000 (0.011)
2025-06-03 14:44:04,797 - train - INFO - Train: 8 [ 448/449 (100%)]  Loss:  3.287779 (3.2815)  Time: 0.544s, 5235.74/s  (0.781s, 3645.40/s)  LR: 9.961e-03  Data: 0.000 (0.010)
2025-06-03 14:44:09,286 - train - INFO - Test: [   0/70]  Time: 4.330 (4.330)  Loss:  1.1846 (1.1846)  Acc@1: 75.7374 (75.7374)  Acc@5: 92.0295 (92.0295)
2025-06-03 14:44:50,113 - train - INFO - Test: [  50/70]  Time: 0.119 (0.885)  Loss:  1.7695 (1.6525)  Acc@1: 62.6404 (63.7909)  Acc@5: 82.1980 (85.6094)
2025-06-03 14:45:06,140 - train - INFO - Test: [  70/70]  Time: 0.034 (0.862)  Loss:  2.0039 (1.7199)  Acc@1: 46.0938 (62.2265)  Acc@5: 80.6250 (84.3610)
2025-06-03 14:45:11,176 - train - INFO - Train: 9 [   0/449 (  0%)]  Loss:  3.279894 (3.2799)  Time: 4.682s,  608.23/s  (4.682s,  608.23/s)  LR: 9.950e-03  Data: 3.573 (3.573)
2025-06-03 14:45:49,602 - train - INFO - Train: 9 [  50/449 ( 11%)]  Loss:  3.263647 (3.2718)  Time: 0.547s, 5210.71/s  (0.845s, 3369.45/s)  LR: 9.950e-03  Data: 0.000 (0.162)
2025-06-03 14:46:29,317 - train - INFO - Train: 9 [ 100/449 ( 22%)]  Loss:  3.282342 (3.2753)  Time: 1.543s, 1845.24/s  (0.820s, 3473.10/s)  LR: 9.950e-03  Data: 0.000 (0.121)
2025-06-03 14:47:07,946 - train - INFO - Train: 9 [ 150/449 ( 33%)]  Loss:  3.285541 (3.2779)  Time: 0.553s, 5146.60/s  (0.804s, 3540.99/s)  LR: 9.950e-03  Data: 0.000 (0.081)
2025-06-03 14:47:47,555 - train - INFO - Train: 9 [ 200/449 ( 45%)]  Loss:  3.294442 (3.2812)  Time: 1.639s, 1737.50/s  (0.801s, 3554.32/s)  LR: 9.950e-03  Data: 0.000 (0.061)
2025-06-03 14:48:25,710 - train - INFO - Train: 9 [ 250/449 ( 56%)]  Loss:  3.268896 (3.2791)  Time: 0.553s, 5145.92/s  (0.794s, 3588.41/s)  LR: 9.950e-03  Data: 0.000 (0.049)
2025-06-03 14:49:04,938 - train - INFO - Train: 9 [ 300/449 ( 67%)]  Loss:  3.287225 (3.2803)  Time: 1.390s, 2048.38/s  (0.792s, 3595.27/s)  LR: 9.950e-03  Data: 0.000 (0.041)
2025-06-03 14:49:43,183 - train - INFO - Train: 9 [ 350/449 ( 78%)]  Loss:  3.227226 (3.2737)  Time: 0.547s, 5208.19/s  (0.788s, 3612.99/s)  LR: 9.950e-03  Data: 0.000 (0.035)
2025-06-03 14:50:22,586 - train - INFO - Train: 9 [ 400/449 ( 89%)]  Loss:  3.275078 (3.2738)  Time: 1.412s, 2016.86/s  (0.788s, 3613.13/s)  LR: 9.950e-03  Data: 0.000 (0.031)
2025-06-03 14:50:59,500 - train - INFO - Train: 9 [ 448/449 (100%)]  Loss:  3.216496 (3.2681)  Time: 1.357s, 2098.08/s  (0.786s, 3622.58/s)  LR: 9.950e-03  Data: 0.000 (0.027)
2025-06-03 14:51:04,056 - train - INFO - Test: [   0/70]  Time: 4.391 (4.391)  Loss:  1.1523 (1.1523)  Acc@1: 74.0169 (74.0169)  Acc@5: 91.6784 (91.6784)
2025-06-03 14:51:44,927 - train - INFO - Test: [  50/70]  Time: 0.119 (0.887)  Loss:  1.7363 (1.6802)  Acc@1: 62.6756 (62.2494)  Acc@5: 82.0927 (84.4617)
2025-06-03 14:52:01,171 - train - INFO - Test: [  70/70]  Time: 0.034 (0.866)  Loss:  2.2891 (1.7567)  Acc@1: 44.5312 (60.8710)  Acc@5: 77.6562 (83.3360)
2025-06-03 14:52:06,333 - train - INFO - Train: 10 [   0/449 (  0%)]  Loss:  3.297581 (3.2976)  Time: 4.908s,  580.28/s  (4.908s,  580.28/s)  LR: 9.939e-03  Data: 4.173 (4.173)
2025-06-03 14:52:45,372 - train - INFO - Train: 10 [  50/449 ( 11%)]  Loss:  3.279809 (3.2887)  Time: 0.551s, 5167.93/s  (0.862s, 3305.13/s)  LR: 9.939e-03  Data: 0.000 (0.278)
2025-06-03 14:53:24,241 - train - INFO - Train: 10 [ 100/449 ( 22%)]  Loss:  3.318872 (3.2988)  Time: 0.857s, 3323.30/s  (0.820s, 3473.39/s)  LR: 9.939e-03  Data: 0.063 (0.171)
2025-06-03 14:54:03,466 - train - INFO - Train: 10 [ 150/449 ( 33%)]  Loss:  3.282248 (3.2946)  Time: 0.547s, 5205.28/s  (0.808s, 3523.87/s)  LR: 9.939e-03  Data: 0.000 (0.117)
2025-06-03 14:54:42,099 - train - INFO - Train: 10 [ 200/449 ( 45%)]  Loss:  3.313900 (3.2985)  Time: 0.555s, 5135.79/s  (0.799s, 3562.88/s)  LR: 9.939e-03  Data: 0.000 (0.088)
2025-06-03 14:55:21,787 - train - INFO - Train: 10 [ 250/449 ( 56%)]  Loss:  3.296177 (3.2981)  Time: 0.547s, 5203.66/s  (0.798s, 3567.86/s)  LR: 9.939e-03  Data: 0.000 (0.070)
2025-06-03 14:56:00,331 - train - INFO - Train: 10 [ 300/449 ( 67%)]  Loss:  3.234794 (3.2891)  Time: 0.547s, 5205.48/s  (0.794s, 3588.31/s)  LR: 9.939e-03  Data: 0.000 (0.059)
2025-06-03 14:56:39,608 - train - INFO - Train: 10 [ 350/449 ( 78%)]  Loss:  3.239573 (3.2829)  Time: 0.547s, 5209.58/s  (0.793s, 3593.58/s)  LR: 9.939e-03  Data: 0.000 (0.050)
2025-06-03 14:57:17,564 - train - INFO - Train: 10 [ 400/449 ( 89%)]  Loss:  3.294269 (3.2841)  Time: 0.553s, 5151.44/s  (0.788s, 3612.59/s)  LR: 9.939e-03  Data: 0.000 (0.044)
2025-06-03 14:57:54,964 - train - INFO - Train: 10 [ 448/449 (100%)]  Loss:  3.234561 (3.2792)  Time: 0.753s, 3783.80/s  (0.787s, 3617.10/s)  LR: 9.939e-03  Data: 0.000 (0.040)
2025-06-03 14:57:59,481 - train - INFO - Test: [   0/70]  Time: 4.324 (4.324)  Loss:  1.0215 (1.0215)  Acc@1: 77.8441 (77.8441)  Acc@5: 93.3287 (93.3287)
2025-06-03 14:58:40,371 - train - INFO - Test: [  50/70]  Time: 0.119 (0.887)  Loss:  1.8477 (1.6778)  Acc@1: 60.6039 (62.3864)  Acc@5: 81.2500 (84.6559)
2025-06-03 14:58:56,500 - train - INFO - Test: [  70/70]  Time: 0.034 (0.864)  Loss:  1.9717 (1.7594)  Acc@1: 52.9688 (60.8995)  Acc@5: 82.0312 (83.2810)
2025-06-03 14:59:01,470 - train - INFO - Train: 11 [   0/449 (  0%)]  Loss:  3.276274 (3.2763)  Time: 4.682s,  608.34/s  (4.682s,  608.34/s)  LR: 9.926e-03  Data: 4.135 (4.135)
2025-06-03 14:59:40,074 - train - INFO - Train: 11 [  50/449 ( 11%)]  Loss:  3.181917 (3.2291)  Time: 0.553s, 5150.23/s  (0.849s, 3355.66/s)  LR: 9.926e-03  Data: 0.000 (0.297)
2025-06-03 15:00:19,454 - train - INFO - Train: 11 [ 100/449 ( 22%)]  Loss:  3.193638 (3.2173)  Time: 1.437s, 1981.85/s  (0.818s, 3479.73/s)  LR: 9.926e-03  Data: 0.407 (0.242)
2025-06-03 15:00:57,821 - train - INFO - Train: 11 [ 150/449 ( 33%)]  Loss:  3.204253 (3.2140)  Time: 0.546s, 5212.08/s  (0.802s, 3553.29/s)  LR: 9.926e-03  Data: 0.000 (0.188)
2025-06-03 15:01:37,333 - train - INFO - Train: 11 [ 200/449 ( 45%)]  Loss:  3.236531 (3.2185)  Time: 1.282s, 2221.51/s  (0.799s, 3565.78/s)  LR: 9.926e-03  Data: 0.742 (0.202)
2025-06-03 15:02:16,071 - train - INFO - Train: 11 [ 250/449 ( 56%)]  Loss:  3.202583 (3.2159)  Time: 0.547s, 5210.67/s  (0.794s, 3587.22/s)  LR: 9.926e-03  Data: 0.000 (0.208)
2025-06-03 15:02:55,836 - train - INFO - Train: 11 [ 300/449 ( 67%)]  Loss:  3.265859 (3.2230)  Time: 1.061s, 2685.39/s  (0.794s, 3586.20/s)  LR: 9.926e-03  Data: 0.521 (0.215)
2025-06-03 15:03:35,409 - train - INFO - Train: 11 [ 350/449 ( 78%)]  Loss:  3.290319 (3.2314)  Time: 0.553s, 5150.35/s  (0.794s, 3587.95/s)  LR: 9.926e-03  Data: 0.000 (0.219)
2025-06-03 15:04:14,870 - train - INFO - Train: 11 [ 400/449 ( 89%)]  Loss:  3.243281 (3.2327)  Time: 1.115s, 2555.08/s  (0.793s, 3590.54/s)  LR: 9.926e-03  Data: 0.575 (0.222)
2025-06-03 15:04:52,221 - train - INFO - Train: 11 [ 448/449 (100%)]  Loss:  3.237267 (3.2332)  Time: 1.122s, 2538.12/s  (0.792s, 3597.84/s)  LR: 9.926e-03  Data: 0.578 (0.223)
2025-06-03 15:04:56,819 - train - INFO - Test: [   0/70]  Time: 4.447 (4.447)  Loss:  1.0615 (1.0615)  Acc@1: 76.2289 (76.2289)  Acc@5: 92.9424 (92.9424)
2025-06-03 15:05:38,878 - train - INFO - Test: [  50/70]  Time: 0.120 (0.912)  Loss:  1.7998 (1.7445)  Acc@1: 62.0787 (61.4274)  Acc@5: 82.9354 (84.2043)
2025-06-03 15:05:55,120 - train - INFO - Test: [  70/70]  Time: 0.035 (0.884)  Loss:  2.3086 (1.8099)  Acc@1: 45.6250 (60.1655)  Acc@5: 77.0312 (83.0215)
2025-06-03 15:06:00,092 - train - INFO - Train: 12 [   0/449 (  0%)]  Loss:  3.224248 (3.2242)  Time: 4.728s,  602.43/s  (4.728s,  602.43/s)  LR: 9.912e-03  Data: 3.945 (3.945)
2025-06-03 15:06:39,339 - train - INFO - Train: 12 [  50/449 ( 11%)]  Loss:  3.307037 (3.2656)  Time: 0.547s, 5202.33/s  (0.862s, 3303.01/s)  LR: 9.912e-03  Data: 0.000 (0.233)
2025-06-03 15:07:19,523 - train - INFO - Train: 12 [ 100/449 ( 22%)]  Loss:  3.197244 (3.2428)  Time: 1.459s, 1952.61/s  (0.833s, 3417.98/s)  LR: 9.912e-03  Data: 0.000 (0.124)
2025-06-03 15:07:58,143 - train - INFO - Train: 12 [ 150/449 ( 33%)]  Loss:  3.256464 (3.2462)  Time: 0.551s, 5173.21/s  (0.813s, 3502.73/s)  LR: 9.912e-03  Data: 0.000 (0.083)
2025-06-03 15:08:37,646 - train - INFO - Train: 12 [ 200/449 ( 45%)]  Loss:  3.244235 (3.2458)  Time: 1.238s, 2300.65/s  (0.807s, 3527.61/s)  LR: 9.912e-03  Data: 0.000 (0.062)
2025-06-03 15:09:16,604 - train - INFO - Train: 12 [ 250/449 ( 56%)]  Loss:  3.320744 (3.2583)  Time: 0.551s, 5170.34/s  (0.802s, 3552.32/s)  LR: 9.912e-03  Data: 0.000 (0.050)
2025-06-03 15:09:55,792 - train - INFO - Train: 12 [ 300/449 ( 67%)]  Loss:  3.265631 (3.2594)  Time: 1.481s, 1922.66/s  (0.799s, 3565.62/s)  LR: 9.912e-03  Data: 0.000 (0.042)
2025-06-03 15:10:35,177 - train - INFO - Train: 12 [ 350/449 ( 78%)]  Loss:  3.224601 (3.2550)  Time: 0.549s, 5186.56/s  (0.797s, 3572.66/s)  LR: 9.912e-03  Data: 0.000 (0.036)
2025-06-03 15:11:14,529 - train - INFO - Train: 12 [ 400/449 ( 89%)]  Loss:  3.265926 (3.2562)  Time: 1.505s, 1892.12/s  (0.796s, 3578.36/s)  LR: 9.912e-03  Data: 0.000 (0.031)
2025-06-03 15:11:51,946 - train - INFO - Train: 12 [ 448/449 (100%)]  Loss:  3.261740 (3.2568)  Time: 1.412s, 2017.64/s  (0.794s, 3586.27/s)  LR: 9.912e-03  Data: 0.000 (0.028)
2025-06-03 15:11:56,711 - train - INFO - Test: [   0/70]  Time: 4.597 (4.597)  Loss:  1.6260 (1.6260)  Acc@1: 69.3469 (69.3469)  Acc@5: 88.0618 (88.0618)
2025-06-03 15:12:46,188 - train - INFO - Test: [  50/70]  Time: 0.119 (1.060)  Loss:  2.0215 (1.9409)  Acc@1: 58.3216 (59.4741)  Acc@5: 80.6882 (82.5788)
2025-06-03 15:13:05,050 - train - INFO - Test: [  70/70]  Time: 0.034 (1.027)  Loss:  2.7734 (2.0001)  Acc@1: 45.0000 (58.2170)  Acc@5: 69.6875 (81.5265)
2025-06-03 15:13:11,302 - train - INFO - Train: 13 [   0/449 (  0%)]  Loss:  3.302736 (3.3027)  Time: 6.011s,  473.83/s  (6.011s,  473.83/s)  LR: 9.896e-03  Data: 5.334 (5.334)
2025-06-03 15:14:01,021 - train - INFO - Train: 13 [  50/449 ( 11%)]  Loss:  3.265098 (3.2839)  Time: 0.657s, 4334.27/s  (1.093s, 2606.72/s)  LR: 9.896e-03  Data: 0.001 (0.504)
2025-06-03 15:14:50,548 - train - INFO - Train: 13 [ 100/449 ( 22%)]  Loss:  3.245855 (3.2712)  Time: 1.640s, 1736.93/s  (1.042s, 2733.07/s)  LR: 9.896e-03  Data: 1.100 (0.462)
2025-06-03 15:15:38,866 - train - INFO - Train: 13 [ 150/449 ( 33%)]  Loss:  3.220993 (3.2587)  Time: 1.387s, 2053.95/s  (1.017s, 2800.46/s)  LR: 9.896e-03  Data: 0.847 (0.442)
2025-06-03 15:16:28,772 - train - INFO - Train: 13 [ 200/449 ( 45%)]  Loss:  3.246930 (3.2563)  Time: 0.546s, 5214.18/s  (1.012s, 2813.47/s)  LR: 9.896e-03  Data: 0.000 (0.435)
2025-06-03 15:17:14,781 - train - INFO - Train: 13 [ 250/449 ( 56%)]  Loss:  3.262400 (3.2573)  Time: 0.979s, 2908.46/s  (0.994s, 2865.41/s)  LR: 9.896e-03  Data: 0.271 (0.406)
2025-06-03 15:17:59,844 - train - INFO - Train: 13 [ 300/449 ( 67%)]  Loss:  3.220857 (3.2521)  Time: 0.579s, 4914.83/s  (0.979s, 2910.49/s)  LR: 9.896e-03  Data: 0.000 (0.350)
2025-06-03 15:18:44,978 - train - INFO - Train: 13 [ 350/449 ( 78%)]  Loss:  3.238905 (3.2505)  Time: 0.741s, 3844.72/s  (0.968s, 2943.00/s)  LR: 9.896e-03  Data: 0.000 (0.300)
2025-06-03 15:19:33,431 - train - INFO - Train: 13 [ 400/449 ( 89%)]  Loss:  3.223227 (3.2474)  Time: 0.621s, 4585.35/s  (0.968s, 2942.51/s)  LR: 9.896e-03  Data: 0.000 (0.263)
2025-06-03 15:20:17,254 - train - INFO - Train: 13 [ 448/449 (100%)]  Loss:  3.265848 (3.2493)  Time: 1.003s, 2838.42/s  (0.962s, 2960.48/s)  LR: 9.896e-03  Data: 0.000 (0.235)
2025-06-03 15:20:21,907 - train - INFO - Test: [   0/70]  Time: 4.491 (4.491)  Loss:  1.3105 (1.3105)  Acc@1: 72.4017 (72.4017)  Acc@5: 90.3441 (90.3441)
2025-06-03 15:21:08,103 - train - INFO - Test: [  50/70]  Time: 0.120 (0.994)  Loss:  1.8779 (1.8110)  Acc@1: 60.1124 (60.4752)  Acc@5: 82.4438 (83.2576)
2025-06-03 15:21:24,529 - train - INFO - Test: [  70/70]  Time: 0.034 (0.945)  Loss:  1.9434 (1.9002)  Acc@1: 53.5938 (58.9230)  Acc@5: 82.5000 (81.8515)
2025-06-03 15:21:29,492 - train - INFO - Train: 14 [   0/449 (  0%)]  Loss:  3.275837 (3.2758)  Time: 4.664s,  610.57/s  (4.664s,  610.57/s)  LR: 9.880e-03  Data: 4.035 (4.035)
2025-06-03 15:22:08,895 - train - INFO - Train: 14 [  50/449 ( 11%)]  Loss:  3.170040 (3.2229)  Time: 0.545s, 5223.27/s  (0.864s, 3296.22/s)  LR: 9.880e-03  Data: 0.000 (0.301)
2025-06-03 15:22:49,047 - train - INFO - Train: 14 [ 100/449 ( 22%)]  Loss:  3.238945 (3.2283)  Time: 1.393s, 2044.32/s  (0.834s, 3415.60/s)  LR: 9.880e-03  Data: 0.846 (0.277)
2025-06-03 15:23:28,154 - train - INFO - Train: 14 [ 150/449 ( 33%)]  Loss:  3.292562 (3.2443)  Time: 0.553s, 5148.26/s  (0.817s, 3487.18/s)  LR: 9.880e-03  Data: 0.000 (0.262)
2025-06-03 15:24:07,785 - train - INFO - Train: 14 [ 200/449 ( 45%)]  Loss:  3.240874 (3.2437)  Time: 1.156s, 2463.20/s  (0.811s, 3512.99/s)  LR: 9.880e-03  Data: 0.609 (0.257)
2025-06-03 15:24:46,804 - train - INFO - Train: 14 [ 250/449 ( 56%)]  Loss:  3.649434 (3.3113)  Time: 0.555s, 5127.79/s  (0.805s, 3539.38/s)  LR: 9.880e-03  Data: 0.000 (0.252)
2025-06-03 15:25:25,653 - train - INFO - Train: 14 [ 300/449 ( 67%)]  Loss:  3.214271 (3.2974)  Time: 0.932s, 3054.64/s  (0.800s, 3559.74/s)  LR: 9.880e-03  Data: 0.385 (0.247)
2025-06-03 15:26:04,783 - train - INFO - Train: 14 [ 350/449 ( 78%)]  Loss:  3.269714 (3.2940)  Time: 0.546s, 5213.20/s  (0.798s, 3570.85/s)  LR: 9.880e-03  Data: 0.000 (0.246)
2025-06-03 15:26:43,175 - train - INFO - Train: 14 [ 400/449 ( 89%)]  Loss:  3.220286 (3.2858)  Time: 0.951s, 2994.71/s  (0.794s, 3587.54/s)  LR: 9.880e-03  Data: 0.411 (0.243)
2025-06-03 15:27:20,252 - train - INFO - Train: 14 [ 448/449 (100%)]  Loss:  3.291008 (3.2863)  Time: 0.741s, 3845.63/s  (0.792s, 3597.93/s)  LR: 9.880e-03  Data: 0.202 (0.242)
2025-06-03 15:27:24,986 - train - INFO - Test: [   0/70]  Time: 4.573 (4.573)  Loss:  1.0234 (1.0234)  Acc@1: 78.4410 (78.4410)  Acc@5: 93.0478 (93.0478)
2025-06-03 15:28:05,884 - train - INFO - Test: [  50/70]  Time: 0.119 (0.892)  Loss:  1.6533 (1.6337)  Acc@1: 65.7654 (63.4088)  Acc@5: 85.6039 (85.4022)
2025-06-03 15:28:22,007 - train - INFO - Test: [  70/70]  Time: 0.034 (0.868)  Loss:  2.1660 (1.7125)  Acc@1: 49.8438 (62.0175)  Acc@5: 77.6562 (84.1565)
2025-06-03 15:28:26,763 - train - INFO - Train: 15 [   0/449 (  0%)]  Loss:  3.221939 (3.2219)  Time: 4.516s,  630.60/s  (4.516s,  630.60/s)  LR: 9.862e-03  Data: 3.661 (3.661)
2025-06-03 15:29:07,000 - train - INFO - Train: 15 [  50/449 ( 11%)]  Loss:  3.290282 (3.2561)  Time: 0.548s, 5198.78/s  (0.877s, 3245.72/s)  LR: 9.862e-03  Data: 0.000 (0.093)
2025-06-03 15:29:47,307 - train - INFO - Train: 15 [ 100/449 ( 22%)]  Loss:  3.283759 (3.2653)  Time: 1.377s, 2068.21/s  (0.842s, 3381.81/s)  LR: 9.862e-03  Data: 0.000 (0.047)
2025-06-03 15:30:25,720 - train - INFO - Train: 15 [ 150/449 ( 33%)]  Loss:  3.283697 (3.2699)  Time: 0.548s, 5197.51/s  (0.818s, 3483.03/s)  LR: 9.862e-03  Data: 0.000 (0.031)
2025-06-03 15:31:05,836 - train - INFO - Train: 15 [ 200/449 ( 45%)]  Loss:  3.217102 (3.2594)  Time: 1.410s, 2019.71/s  (0.814s, 3499.40/s)  LR: 9.862e-03  Data: 0.000 (0.024)
2025-06-03 15:31:44,179 - train - INFO - Train: 15 [ 250/449 ( 56%)]  Loss:  3.214137 (3.2518)  Time: 0.553s, 5151.16/s  (0.804s, 3540.14/s)  LR: 9.862e-03  Data: 0.000 (0.019)
2025-06-03 15:32:23,645 - train - INFO - Train: 15 [ 300/449 ( 67%)]  Loss:  3.181007 (3.2417)  Time: 1.468s, 1940.13/s  (0.802s, 3551.31/s)  LR: 9.862e-03  Data: 0.000 (0.016)
2025-06-03 15:33:02,792 - train - INFO - Train: 15 [ 350/449 ( 78%)]  Loss:  3.240066 (3.2415)  Time: 0.546s, 5213.87/s  (0.799s, 3563.36/s)  LR: 9.862e-03  Data: 0.000 (0.014)
2025-06-03 15:33:42,458 - train - INFO - Train: 15 [ 400/449 ( 89%)]  Loss:  3.310189 (3.2491)  Time: 1.541s, 1847.86/s  (0.799s, 3566.68/s)  LR: 9.862e-03  Data: 0.000 (0.012)
2025-06-03 15:34:19,897 - train - INFO - Train: 15 [ 448/449 (100%)]  Loss:  3.272559 (3.2515)  Time: 1.339s, 2126.37/s  (0.797s, 3575.56/s)  LR: 9.862e-03  Data: 0.000 (0.011)
2025-06-03 15:34:24,779 - train - INFO - Test: [   0/70]  Time: 4.724 (4.724)  Loss:  1.1836 (1.1836)  Acc@1: 73.6306 (73.6306)  Acc@5: 92.3455 (92.3455)
2025-06-03 15:35:05,851 - train - INFO - Test: [  50/70]  Time: 0.120 (0.898)  Loss:  1.6133 (1.7425)  Acc@1: 64.6067 (62.0559)  Acc@5: 84.9368 (84.4549)
2025-06-03 15:35:22,184 - train - INFO - Test: [  70/70]  Time: 0.034 (0.875)  Loss:  2.5469 (1.8281)  Acc@1: 40.6250 (60.3065)  Acc@5: 72.5000 (82.8810)
2025-06-03 15:35:27,234 - train - INFO - Train: 16 [   0/449 (  0%)]  Loss:  3.291573 (3.2916)  Time: 4.809s,  592.20/s  (4.809s,  592.20/s)  LR: 9.843e-03  Data: 4.258 (4.258)
2025-06-03 15:36:05,920 - train - INFO - Train: 16 [  50/449 ( 11%)]  Loss:  3.237749 (3.2647)  Time: 0.546s, 5214.74/s  (0.853s, 3339.56/s)  LR: 9.843e-03  Data: 0.000 (0.244)
2025-06-03 15:36:45,700 - train - INFO - Train: 16 [ 100/449 ( 22%)]  Loss:  3.230611 (3.2533)  Time: 1.447s, 1968.30/s  (0.824s, 3454.28/s)  LR: 9.843e-03  Data: 0.304 (0.160)
2025-06-03 15:37:24,266 - train - INFO - Train: 16 [ 150/449 ( 33%)]  Loss:  3.201592 (3.2404)  Time: 0.547s, 5205.84/s  (0.807s, 3529.69/s)  LR: 9.843e-03  Data: 0.000 (0.108)
2025-06-03 15:38:04,001 - train - INFO - Train: 16 [ 200/449 ( 45%)]  Loss:  3.260189 (3.2443)  Time: 1.522s, 1870.68/s  (0.804s, 3542.99/s)  LR: 9.843e-03  Data: 0.000 (0.081)
2025-06-03 15:38:42,488 - train - INFO - Train: 16 [ 250/449 ( 56%)]  Loss:  3.252639 (3.2457)  Time: 0.548s, 5195.14/s  (0.797s, 3573.21/s)  LR: 9.843e-03  Data: 0.000 (0.065)
2025-06-03 15:39:22,251 - train - INFO - Train: 16 [ 300/449 ( 67%)]  Loss:  3.255651 (3.2471)  Time: 0.929s, 3064.29/s  (0.797s, 3574.56/s)  LR: 9.843e-03  Data: 0.000 (0.054)
2025-06-03 15:40:01,565 - train - INFO - Train: 16 [ 350/449 ( 78%)]  Loss:  3.256712 (3.2483)  Time: 0.548s, 5200.47/s  (0.795s, 3581.27/s)  LR: 9.843e-03  Data: 0.000 (0.047)
2025-06-03 15:40:41,683 - train - INFO - Train: 16 [ 400/449 ( 89%)]  Loss:  3.257006 (3.2493)  Time: 1.624s, 1753.70/s  (0.796s, 3577.29/s)  LR: 9.843e-03  Data: 0.000 (0.041)
2025-06-03 15:41:19,652 - train - INFO - Train: 16 [ 448/449 (100%)]  Loss:  3.278351 (3.2522)  Time: 1.187s, 2398.37/s  (0.796s, 3579.77/s)  LR: 9.843e-03  Data: 0.000 (0.037)
2025-06-03 15:41:24,392 - train - INFO - Test: [   0/70]  Time: 4.585 (4.585)  Loss:  1.0957 (1.0957)  Acc@1: 78.1250 (78.1250)  Acc@5: 92.7669 (92.7669)
2025-06-03 15:42:06,243 - train - INFO - Test: [  50/70]  Time: 0.119 (0.911)  Loss:  1.9385 (1.7196)  Acc@1: 57.9003 (62.4952)  Acc@5: 81.3905 (84.7550)
2025-06-03 15:42:22,878 - train - INFO - Test: [  70/70]  Time: 0.034 (0.888)  Loss:  2.5078 (1.8082)  Acc@1: 42.8125 (60.7600)  Acc@5: 76.0938 (83.2760)
2025-06-03 15:42:27,541 - train - INFO - Train: 17 [   0/449 (  0%)]  Loss:  3.220622 (3.2206)  Time: 4.414s,  645.18/s  (4.414s,  645.18/s)  LR: 9.823e-03  Data: 3.854 (3.854)
2025-06-03 15:43:07,357 - train - INFO - Train: 17 [  50/449 ( 11%)]  Loss:  3.334247 (3.2774)  Time: 0.553s, 5147.26/s  (0.867s, 3283.95/s)  LR: 9.823e-03  Data: 0.000 (0.318)
2025-06-03 15:43:47,386 - train - INFO - Train: 17 [ 100/449 ( 22%)]  Loss:  3.251775 (3.2689)  Time: 1.426s, 1997.64/s  (0.834s, 3413.91/s)  LR: 9.823e-03  Data: 0.878 (0.284)
2025-06-03 15:44:26,934 - train - INFO - Train: 17 [ 150/449 ( 33%)]  Loss:  3.244722 (3.2628)  Time: 0.553s, 5148.80/s  (0.820s, 3473.63/s)  LR: 9.823e-03  Data: 0.000 (0.269)
2025-06-03 15:45:07,334 - train - INFO - Train: 17 [ 200/449 ( 45%)]  Loss:  3.268558 (3.2640)  Time: 1.468s, 1940.05/s  (0.817s, 3486.22/s)  LR: 9.823e-03  Data: 0.922 (0.266)
2025-06-03 15:45:46,241 - train - INFO - Train: 17 [ 250/449 ( 56%)]  Loss:  3.219288 (3.2565)  Time: 0.551s, 5170.77/s  (0.809s, 3519.54/s)  LR: 9.823e-03  Data: 0.000 (0.259)
2025-06-03 15:46:26,193 - train - INFO - Train: 17 [ 300/449 ( 67%)]  Loss:  3.274183 (3.2591)  Time: 1.368s, 2082.54/s  (0.807s, 3526.95/s)  LR: 9.823e-03  Data: 0.830 (0.257)
2025-06-03 15:47:06,645 - train - INFO - Train: 17 [ 350/449 ( 78%)]  Loss:  3.255123 (3.2586)  Time: 0.554s, 5138.77/s  (0.808s, 3526.00/s)  LR: 9.823e-03  Data: 0.000 (0.257)
2025-06-03 15:47:46,026 - train - INFO - Train: 17 [ 400/449 ( 89%)]  Loss:  3.256634 (3.2584)  Time: 1.422s, 2003.09/s  (0.805s, 3536.97/s)  LR: 9.823e-03  Data: 0.875 (0.255)
2025-06-03 15:48:24,064 - train - INFO - Train: 17 [ 448/449 (100%)]  Loss:  3.289658 (3.2615)  Time: 1.228s, 2319.16/s  (0.804s, 3542.99/s)  LR: 9.823e-03  Data: 0.689 (0.254)
2025-06-03 15:48:28,499 - train - INFO - Test: [   0/70]  Time: 4.270 (4.270)  Loss:  1.1699 (1.1699)  Acc@1: 76.0534 (76.0534)  Acc@5: 93.7500 (93.7500)
2025-06-03 15:49:09,524 - train - INFO - Test: [  50/70]  Time: 0.119 (0.888)  Loss:  1.8008 (1.8835)  Acc@1: 65.4846 (61.3668)  Acc@5: 83.4972 (84.0693)
2025-06-03 15:49:26,102 - train - INFO - Test: [  70/70]  Time: 0.034 (0.871)  Loss:  2.0645 (1.9309)  Acc@1: 50.6250 (60.1120)  Acc@5: 80.7812 (82.9250)
2025-06-03 15:49:30,790 - train - INFO - Train: 18 [   0/449 (  0%)]  Loss:  3.273087 (3.2731)  Time: 4.435s,  642.13/s  (4.435s,  642.13/s)  LR: 9.802e-03  Data: 3.878 (3.878)
2025-06-03 15:50:11,033 - train - INFO - Train: 18 [  50/449 ( 11%)]  Loss:  3.283596 (3.2783)  Time: 0.548s, 5195.82/s  (0.876s, 3251.04/s)  LR: 9.802e-03  Data: 0.000 (0.215)
2025-06-03 15:50:52,047 - train - INFO - Train: 18 [ 100/449 ( 22%)]  Loss:  3.208024 (3.2549)  Time: 1.581s, 1801.29/s  (0.848s, 3356.82/s)  LR: 9.802e-03  Data: 0.000 (0.111)
